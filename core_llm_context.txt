=====================================
CORE REPOSITORY CONTEXT
Generated on: Wed Jun 18 17:36:40 PDT 2025
=====================================


=====================================
FILE: ./AGENTS.md
=====================================

# AGENTS.md - AI Agent Guide for ModelAudit

This file provides comprehensive guidance for AI agents working with the ModelAudit codebase - a security scanner for AI/ML model files.

## üéØ Project Overview

ModelAudit is a Python security scanner that detects malicious code, backdoors, and security risks in ML model files. It supports multiple formats (PyTorch, TensorFlow, Keras, SafeTensors, Pickle, ZIP) and provides both CLI and programmatic interfaces.

**Key Security Focus:**

- Dangerous code execution patterns
- Suspicious opcodes in pickle files
- Malicious configurations in model files
- Blacklisted model names and patterns
- Weight distribution anomalies

## üìÅ Project Structure

```
modelaudit/
‚îú‚îÄ‚îÄ modelaudit/                 # Main package
‚îÇ   ‚îú‚îÄ‚îÄ scanners/              # Core scanner implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py           # Abstract base scanner class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pickle_scanner.py  # Pickle security scanner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pytorch_zip_scanner.py  # PyTorch model scanner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tf_savedmodel_scanner.py  # TensorFlow scanner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ keras_h5_scanner.py     # Keras H5 scanner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ safetensors_scanner.py  # SafeTensors scanner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ manifest_scanner.py     # Config/manifest scanner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ weight_distribution_scanner.py  # Weight anomaly detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ zip_scanner.py          # Generic ZIP scanner
‚îÇ   ‚îú‚îÄ‚îÄ utils/                 # Utility modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ filetype.py       # File format detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ name_policies/         # Model name blacklist policies
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                # Command-line interface
‚îÇ   ‚îú‚îÄ‚îÄ core.py               # Core scanning logic
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ tests/                     # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ test_*_scanner.py     # Scanner-specific tests
‚îÇ   ‚îú‚îÄ‚îÄ test_integration.py   # Integration tests
‚îÇ   ‚îú‚îÄ‚îÄ test_cli.py          # CLI tests
‚îÇ   ‚îî‚îÄ‚îÄ conftest.py          # pytest configuration
‚îú‚îÄ‚îÄ pyproject.toml           # Poetry configuration
‚îú‚îÄ‚îÄ README.md                # Project documentation
‚îî‚îÄ‚îÄ CLAUDE.md               # Claude-specific guidance
```

## üîß Development Conventions

### Code Style & Standards

**Python Version:** 3.9+ (supports 3.9, 3.10, 3.11, 3.12)

**Code Quality Tools:**

- **Ruff**: Ultra-fast linter and formatter (replaces Black, isort, flake8)
- **MyPy**: Static type checking
- **pytest**: Testing framework with coverage

**Formatting Standards:**

```bash
# ALWAYS run these before committing:
poetry run ruff format .         # Format code
poetry run ruff check .          # Lint code
poetry run mypy modelaudit/      # Type check
poetry run pytest               # Run tests
```

### Naming Conventions

- **Classes**: PascalCase (e.g., `PickleScanner`, `BaseScanner`)
- **Functions/Variables**: snake_case (e.g., `scan_model`, `file_path`)
- **Constants**: UPPER_SNAKE_CASE (e.g., `SUSPICIOUS_GLOBALS`, `DANGEROUS_OPCODES`)
- **Private methods**: Leading underscore (e.g., `_check_path`, `_create_result`)

### Type Hints

Always use type hints for function parameters and return values:

```python
def scan(self, path: str) -> ScanResult:
    """Scan a model file."""
    pass

def can_handle(cls, path: str) -> bool:
    """Check if scanner can handle this file."""
    return True
```

## üõ°Ô∏è Scanner Architecture

### Creating New Scanners

All scanners inherit from `BaseScanner` in `modelaudit/scanners/base.py`:

```python
from .base import BaseScanner, IssueSeverity, ScanResult

class MyScanner(BaseScanner):
    name = "my_scanner"  # Unique identifier
    description = "Scans my format for security issues"
    supported_extensions = [".myformat"]

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Return True if this scanner can handle the file."""
        # Implementation here
        pass

    def scan(self, path: str) -> ScanResult:
        """Scan the file and return results."""
        result = self._create_result()
        # Scanning logic here
        result.finish(success=not result.has_errors)
        return result
```

### Scanner Registration

Add new scanners to `SCANNER_REGISTRY` in `modelaudit/scanners/__init__.py`:

```python
SCANNER_REGISTRY = [
    PickleScanner,
    PyTorchZipScanner,
    # ... existing scanners
    MyScanner,  # Add here
]
```

### Issue Reporting

Use the `ScanResult` and `Issue` classes for consistent reporting:

```python
# Report security issues
result.add_issue(
    "Detected malicious code execution",
    severity=IssueSeverity.ERROR,
    location=path,
    details={"pattern": "os.system", "position": 123}
)

# Severity levels: ERROR, WARNING, INFO, DEBUG
```

## üß™ Testing Guidelines

### Test Structure

- **One test file per scanner**: `tests/test_{scanner_name}.py`
- **Integration tests**: `tests/test_integration.py`
- **CLI tests**: `tests/test_cli.py`

### Test Patterns

```python
from pathlib import Path
import pytest
from modelaudit.scanners.my_scanner import MyScanner

def test_my_scanner_safe_file(tmp_path: Path) -> None:
    """Test scanner with safe file."""
    # Create test file
    test_file = tmp_path / "safe.myformat"
    test_file.write_bytes(b"safe content")

    # Run scanner
    scanner = MyScanner()
    result = scanner.scan(str(test_file))

    # Assert results
    assert result.success is True
    assert not result.has_errors

def test_my_scanner_malicious_file(tmp_path: Path) -> None:
    """Test scanner with malicious file."""
    # Create malicious test file
    malicious_file = tmp_path / "malicious.myformat"
    malicious_file.write_bytes(b"malicious content")

    # Run scanner
    scanner = MyScanner()
    result = scanner.scan(str(malicious_file))

    # Assert malicious content detected
    assert result.has_errors
    assert any("malicious" in issue.message.lower() for issue in result.issues)
```

### Running Tests

```bash
# Run all tests
poetry run pytest

# Run specific test file
poetry run pytest tests/test_my_scanner.py -v

# Run with coverage
poetry run pytest --cov=modelaudit

# Run tests for specific Python versions
poetry install --extras all  # Install all dependencies first
poetry run pytest
```

## üì¶ Dependencies & Installation

### Development Setup

```bash
# Clone and setup
git clone https://github.com/promptfoo/modelaudit.git
cd modelaudit

# Install with Poetry (recommended)
poetry install --extras all  # All optional dependencies
poetry install --extras "tensorflow pytorch h5"  # Specific extras

# Or with pip
pip install -e .[all]
```

### Optional Dependencies

The project uses optional dependencies for specific scanners:

- `tensorflow`: TensorFlow SavedModel scanning
- `h5py`: Keras H5 model scanning
- `torch`: PyTorch model scanning
- `pyyaml`: YAML manifest scanning
- `safetensors`: SafeTensors model scanning
- `onnx`: ONNX model scanning

Always test that scanners gracefully handle missing optional dependencies.

## üîç Security Detection Patterns

### Common Suspicious Patterns

```python
# Dangerous imports to detect
SUSPICIOUS_GLOBALS = {
    "os": "*",
    "subprocess": "*",
    "eval": "*",
    "exec": "*",
    "__import__": "*"
}

# Dangerous pickle opcodes
DANGEROUS_OPCODES = [
    "REDUCE", "INST", "OBJ", "NEWOBJ", "STACK_GLOBAL"
]

# Suspicious string patterns
SUSPICIOUS_PATTERNS = [
    r"os\.system",
    r"subprocess\.",
    r"eval\(",
    r"exec\(",
    r"__import__"
]
```

### ML Context Detection

The codebase includes smart detection to reduce false positives in ML contexts:

```python
# ML-safe patterns that shouldn't trigger alerts
ML_SAFE_GLOBALS = {
    "torch": ["*"],
    "numpy": ["*"],
    "transformers": ["*"],
    "sklearn": ["*"]
}
```

## üöÄ CLI Usage

### Basic Commands

```bash
# Scan single file
modelaudit scan model.pkl

# Scan directory
modelaudit scan ./models/

# Export to JSON
modelaudit scan model.pkl --format json --output results.json

# With custom blacklist
modelaudit scan model.pkl --blacklist "unsafe_model"

# Verbose output
modelaudit scan model.pkl --verbose
```

### Exit Codes

- **0**: No security issues found
- **1**: Security issues detected (scan succeeded)
- **2**: Scan errors occurred (file not found, etc.)

## üéØ AI Agent Guidelines

### When Modifying Scanners

1. **Always preserve security focus** - Don't weaken detection capabilities
2. **Test with both safe and malicious samples**
3. **Consider ML context** - Avoid false positives in legitimate ML usage
4. **Follow the scanner pattern** - Use `BaseScanner` interface
5. **Add comprehensive tests** - Include edge cases and error conditions

### When Adding Features

1. **Check optional dependencies** - Handle gracefully when missing
2. **Update `SCANNER_REGISTRY`** if adding new scanners
3. **Follow existing code patterns** and style
4. **Add appropriate documentation**
5. **Consider CI/CD impact** - Ensure tests pass across Python versions

### When Debugging Issues

1. **Check file format detection** in `utils/filetype.py`
2. **Verify scanner selection** logic in core scanning
3. **Review issue severity levels** and reporting
4. **Test with various file formats** and edge cases
5. **Use verbose mode** for detailed logging

## üìã Pull Request Guidelines

When contributing code:

1. **Follow conventional commits**: `feat:`, `fix:`, `docs:`, etc.
2. **All tests must pass** across Python 3.9-3.12
3. **Code must be formatted** with Ruff
4. **Type checking must pass** with MyPy
5. **Keep PRs focused** on single concerns
6. **Include tests** for new functionality
7. **Update documentation** as needed

### Pre-commit Checklist

```bash
# Run before every commit
poetry run ruff format .
poetry run ruff check .
poetry run mypy modelaudit/
poetry run pytest
```

## üîó Key Files for AI Agents

- **`modelaudit/scanners/base.py`**: Scanner interface and base classes
- **`modelaudit/core.py`**: Main scanning orchestration logic
- **`modelaudit/cli.py`**: Command-line interface implementation
- **`pyproject.toml`**: Dependencies and project configuration
- **`tests/conftest.py`**: Test configuration and fixtures

Understanding these files is crucial for effective contributions to the ModelAudit codebase.


=====================================
FILE: ./CLAUDE.md
=====================================

# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

ModelAudit is a security scanner for AI/ML model files that detects potential security risks before deployment. It scans for malicious code, suspicious operations, unsafe configurations, and blacklisted model names.

## Key Commands

```bash
# Setup
./setup-poetry.sh              # Interactive setup script
poetry install --extras all    # Install all dependencies

# Running the scanner
poetry run modelaudit scan model.pkl
poetry run modelaudit scan --format json --output results.json model.pkl

# Testing
poetry run pytest                          # Run all tests
poetry run pytest tests/test_pickle_scanner.py  # Run specific test file
poetry run pytest -k "test_pickle"         # Run tests matching pattern

# Linting and Formatting (run ALL before committing)
poetry run ruff format .                   # Format code with Ruff (alternative formatter)
poetry run mypy modelaudit/                # Type checking

# Quick format check
poetry run ruff format --check .           # Check if Ruff formatting is needed

# Recommended order before committing:
# 2. poetry run ruff format .
# 5. poetry run mypy modelaudit/
# 6. poetry run pytest
```

## Architecture

### Scanner System

- All scanners inherit from `BaseScanner` in `modelaudit/scanners/base.py`
- Scanners implement `can_handle(file_path)` and `scan(file_path, timeout)` methods
- Scanner registration happens via `SCANNER_REGISTRY` in `modelaudit/scanners/__init__.py`
- Each scanner returns a `ScanResult` containing `Issue` objects

### Core Components

- `cli.py`: Click-based CLI interface
- `core.py`: Main scanning logic and file traversal
- `risk_scoring.py`: Normalizes issues to 0.0-1.0 risk scores
- `scanners/`: Format-specific scanner implementations
- `utils/filetype.py`: File type detection utilities

### Adding New Scanners

1. Create scanner class inheriting from `BaseScanner`
2. Implement `can_handle()` and `scan()` methods
3. Register in `SCANNER_REGISTRY`
4. Add tests in `tests/test_<scanner_name>.py`

### Security Detection Focus

- Dangerous imports (os, sys, subprocess, eval, exec)
- Pickle opcodes (REDUCE, INST, OBJ, NEWOBJ, STACK_GLOBAL)
- Encoded payloads (base64, hex)
- Unsafe Lambda layers (Keras/TensorFlow)
- Executable files in archives
- Blacklisted model names
- Weight distribution anomalies (outlier neurons, dissimilar weight vectors)

## Exit Codes

- 0: No security issues found
- 1: Security issues detected
- 2: Scan errors occurred


=====================================
FILE: ./CONTRIBUTING.md
=====================================

# Contributing to ModelAudit

Thank you for your interest in contributing to ModelAudit! This guide will help you get started with development and contributing to the project.

## üõ†Ô∏è Development Setup

### Prerequisites

- Python 3.9 or higher
- Poetry (recommended) or pip
- Git

### Setup

```bash
# Clone repository
git clone https://github.com/promptfoo/modelaudit.git
cd modelaudit

# Install with Poetry (recommended)
poetry sync --with dev --extras "all"

# Or with pip
pip install -e .[all]
```

### Testing with Development Version

**Install and test your local development version:**

```bash
# Option 1: Install in development mode with pip
pip install -e .[all]

# Then test the CLI directly
modelaudit scan test_model.pkl

# Option 2: Use Poetry (recommended)
poetry sync --with dev --extras "all"

# Test with Poetry run (no shell activation needed)
poetry run modelaudit scan test_model.pkl

# Test with Python import
poetry run python -c "from modelaudit.core import scan_file; print(scan_file('test_model.pkl'))"
```

**Create test models for development:**

```bash
# Create a simple test pickle file
python -c "import pickle; pickle.dump({'test': 'data'}, open('test_model.pkl', 'wb'))"

# Test scanning it
modelaudit scan test_model.pkl
```

### Running Tests

```bash
# Run all tests
poetry run pytest

# Run with coverage
poetry run pytest --cov=modelaudit

# Run specific test categories
poetry run pytest tests/test_pickle_scanner.py -v
poetry run pytest tests/test_integration.py -v

# Run tests with all optional dependencies
poetry sync --with dev --extras "all"
poetry run pytest
```

### Development Workflow

```bash
# Run linting and formatting with Ruff
poetry run ruff check .          # Check entire codebase (including tests)
poetry run ruff check --fix .    # Automatically fix lint issues
poetry run ruff format .         # Format code

# Type checking
poetry run mypy modelaudit/

# Build package
poetry build

# The generated distribution contains only the `modelaudit` code and metadata.
# Unnecessary files like tests and Docker configurations are excluded via
# `MANIFEST.in`.

# Publish (maintainers only)
poetry publish
```

**Code Quality Tools:**

This project uses modern Python tooling for maintaining code quality:

- **[Ruff](https://docs.astral.sh/ruff/)**: Ultra-fast Python linter and formatter (replaces Black, isort, flake8)
- **[MyPy](https://mypy.readthedocs.io/)**: Static type checker
- **[Biome](https://biomejs.dev/)**: Fast formatter for JSON and YAML files

**File Formatting with Biome:**

```bash
# Format JSON and YAML files
npx @biomejs/biome format --write .

# Check formatting (for CI)
npx @biomejs/biome ci .
```

## ü§ù Contributing Guidelines

### Getting Started

```bash
# Create feature branch
git checkout -b feature/your-feature-name

# Make your changes...
git add .
git commit -m "feat: description"
git push origin feature/your-feature-name
```

**Pull Request Guidelines:**

- Create PR against `main` branch
- Follow Conventional Commits format (`feat:`, `fix:`, `docs:`, etc.)
- All PRs are squash-merged with a conventional commit message
- Keep changes small and focused
- Add tests for new functionality
- Update documentation as needed

### Commit Message Format

We use [Conventional Commits](https://www.conventionalcommits.org/) format:

- `feat:` - New features
- `fix:` - Bug fixes
- `docs:` - Documentation updates
- `test:` - Adding or updating tests
- `refactor:` - Code refactoring
- `perf:` - Performance improvements
- `chore:` - Maintenance tasks

### Project Structure

```
modelaudit/
‚îú‚îÄ‚îÄ modelaudit/
‚îÇ   ‚îú‚îÄ‚îÄ scanners/          # Model format scanners
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py                    # Base scanner class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pickle_scanner.py          # Pickle/joblib security scanner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tf_savedmodel_scanner.py   # TensorFlow SavedModel scanner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ keras_h5_scanner.py        # Keras H5 model scanner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pytorch_zip_scanner.py     # PyTorch ZIP format scanner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pytorch_binary_scanner.py  # PyTorch binary format scanner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ safetensors_scanner.py     # SafeTensors format scanner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ weight_distribution_scanner.py # Weight analysis scanner
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ zip_scanner.py             # ZIP archive scanner
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ manifest_scanner.py        # Config/manifest scanner
‚îÇ   ‚îú‚îÄ‚îÄ utils/             # Utility modules
‚îÇ   ‚îú‚îÄ‚îÄ auth/              # Authentication modules
‚îÇ   ‚îú‚îÄ‚îÄ name_policies/     # Name policy modules
‚îÇ   ‚îú‚îÄ‚îÄ cli.py            # Command-line interface
‚îÇ   ‚îî‚îÄ‚îÄ core.py           # Core scanning logic
‚îú‚îÄ‚îÄ tests/                # Test suite
‚îú‚îÄ‚îÄ .github/              # GitHub Actions workflows
‚îî‚îÄ‚îÄ README.md             # User documentation
```

### Adding New Scanners

When adding a new scanner for a model format:

1. Create a new scanner file in `modelaudit/scanners/`
2. Implement the scanner class following existing patterns
3. Add appropriate tests in `tests/`
4. Update documentation
5. Add any new dependencies to `pyproject.toml`

### Code Style

- Follow PEP 8 style guidelines
- Use type hints where appropriate
- Write descriptive docstrings
- Keep functions focused and small
- Add comments for complex logic

### Testing

- Write tests for all new functionality
- Ensure tests pass locally before submitting PR
- Include both unit tests and integration tests
- Test with different model formats and edge cases

## üìã Development Tasks

### Common Development Tasks

```bash
# Run full test suite with coverage
poetry run pytest --cov=modelaudit --cov-report=html

# Check for type errors
poetry run mypy modelaudit/

# Format and lint code
poetry run ruff format .
poetry run ruff check --fix .

# Build documentation (if applicable)
# Add documentation build commands here

# Create test models for specific formats
python -c "import torch; torch.save({'model': 'data'}, 'test.pt')"
python -c "import pickle; pickle.dump({'test': 'malicious'}, open('malicious.pkl', 'wb'))"
```

### Release Process (Maintainers)

1. Update version in `pyproject.toml`
2. Create release PR
3. After merge, create GitHub release
4. Poetry will automatically publish to PyPI

## üêõ Reporting Issues

When reporting issues:

- Use the GitHub issue templates
- Include ModelAudit version and Python version
- Provide minimal reproduction steps
- Include error messages and stack traces
- Mention the model format and size if applicable

## üí° Feature Requests

For feature requests:

- Check existing issues first
- Describe the use case clearly
- Explain why it would benefit users
- Consider proposing an implementation approach

## üìû Getting Help

- GitHub Issues: For bugs and feature requests
- GitHub Discussions: For questions and general discussion
- Email: For security issues or private matters

Thank you for contributing to ModelAudit! üöÄ


=====================================
FILE: ./Dockerfile
=====================================

FROM python:3.9-slim

WORKDIR /app

# Install Poetry
RUN pip install poetry==1.5.1

# Copy Poetry configuration files
COPY pyproject.toml poetry.lock* ./

# Configure Poetry to not create a virtual environment inside the container
RUN poetry config virtualenvs.create false

# Install dependencies (base only by default)
RUN poetry install --no-dev --no-interaction

# Copy project code
COPY . .

# Set entrypoint
ENTRYPOINT ["modelaudit"]
CMD ["--help"] 


=====================================
FILE: ./Dockerfile.full
=====================================

FROM python:3.9-slim

WORKDIR /app

# Install Poetry
RUN pip install poetry==1.5.1

# Copy Poetry configuration files
COPY pyproject.toml poetry.lock* ./

# Configure Poetry to not create a virtual environment inside the container
RUN poetry config virtualenvs.create false

# Install all dependencies
RUN poetry install --no-dev --no-interaction --extras "all"

# Copy project code
COPY . .

# Set entrypoint
ENTRYPOINT ["modelaudit"]
CMD ["--help"] 


=====================================
FILE: ./Dockerfile.tensorflow
=====================================

FROM python:3.9-slim

WORKDIR /app

# Install Poetry
RUN pip install poetry==1.5.1

# Copy Poetry configuration files
COPY pyproject.toml poetry.lock* ./

# Configure Poetry to not create a virtual environment inside the container
RUN poetry config virtualenvs.create false

# Install dependencies with TensorFlow extras
RUN poetry install --no-dev --no-interaction --extras "tensorflow"

# Copy project code
COPY . .

# Set entrypoint
ENTRYPOINT ["modelaudit"]
CMD ["--help"] 


=====================================
FILE: ./MANIFEST.in
=====================================

include LICENSE
include README.md
exclude tests*
exclude Dockerfile*
exclude docker-compose.yml
exclude setup-poetry.sh
exclude AGENTS.md
exclude CLAUDE.md
exclude llm_context.txt


=====================================
FILE: ./README.md
=====================================

# ModelAudit

A security scanner for AI models. Quickly check your AIML models for potential security risks before deployment.

[![PyPI version](https://badge.fury.io/py/modelaudit.svg)](https://pypi.org/project/modelaudit/)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)

<img width="989" alt="image" src="https://www.promptfoo.dev/img/docs/modelaudit/modelaudit-result.png" />

## Table of Contents

- [ModelAudit](#modelaudit)
  - [Table of Contents](#table-of-contents)
  - [üîç What It Does](#-what-it-does)
  - [üöÄ Quick Start](#-quick-start)
    - [Installation](#installation)
    - [Basic Usage](#basic-usage)
  - [‚ú® Features](#-features)
    - [Core Capabilities](#core-capabilities)
    - [Reporting \& Integration](#reporting--integration)
    - [Security Detection](#security-detection)
  - [üõ°Ô∏è Supported Model Formats](#Ô∏è-supported-model-formats)
    - [Weight Analysis](#weight-analysis)
  - [‚öôÔ∏è Advanced Usage](#Ô∏è-advanced-usage)
    - [Command Line Options](#command-line-options)
    - [Exit Codes](#exit-codes)
  - [üìã JSON Output Format](#-json-output-format)
  - [üîÑ CI/CD Integration](#-cicd-integration)
    - [Basic Integration](#basic-integration)
    - [Platform Examples](#platform-examples)
  - [üîß Troubleshooting](#-troubleshooting)
    - [Common Issues](#common-issues)
  - [‚ö†Ô∏è Limitations](#Ô∏è-limitations)
  - [üìù License](#-license)

## üîç What It Does

ModelAudit scans ML model files for:

- **Malicious code execution** (e.g., `os.system` calls in pickled models)
- **Suspicious TensorFlow operations** (PyFunc, file I/O operations)
- **Potentially unsafe Keras Lambda layers** with arbitrary code execution
- **Dangerous pickle opcodes** (REDUCE, INST, OBJ, STACK_GLOBAL)
- **Custom ONNX operators** and external data integrity issues
- **Encoded payloads** and suspicious string patterns
- **Risky configurations** in model architectures
- **Suspicious patterns** in model manifests and configuration files
- **Models with blacklisted names** or content patterns
- **Malicious content in ZIP archives** including nested archives and zip bombs
- **Container-delivered models** in OCI/Docker layers and manifest files
- **GGUF/GGML file integrity** and tensor alignment validation
- **Anomalous weight patterns** that may indicate trojaned models (statistical analysis)
- **Joblib serialization vulnerabilities** (compression bombs, embedded pickle content)
- **NumPy array integrity issues** (malformed headers, dangerous dtypes)

## üöÄ Quick Start

### Installation

ModelAudit is available on [PyPI](https://pypi.org/project/modelaudit/) and requires **Python 3.9 or higher**.

**Basic installation:**

```bash
pip install modelaudit
```

**With optional dependencies for specific model formats:**

```bash
# For TensorFlow SavedModel scanning
pip install modelaudit[tensorflow]

# For Keras H5 model scanning
pip install modelaudit[h5]

# For PyTorch model scanning
pip install modelaudit[pytorch]

# For ONNX model scanning
pip install modelaudit[onnx]

# For YAML manifest scanning
pip install modelaudit[yaml]

# For SafeTensors model scanning
pip install modelaudit[safetensors]

# For Joblib model scanning
pip install modelaudit[joblib]

# Install all optional dependencies
pip install modelaudit[all]
```

### Basic Usage

```bash
# Scan a single model
modelaudit scan model.pkl

# Scan an ONNX model
modelaudit scan model.onnx

# Scan multiple models
modelaudit scan model1.pkl model2.h5 model3.pt llama-model.gguf model4.joblib model5.npy

# Scan a directory
modelaudit scan ./models/

# Export results to JSON
modelaudit scan model.pkl --format json --output results.json
```

**Example output:**

```bash
$ modelaudit scan suspicious_model.pkl

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ModelAudit Security Scanner
Scanning for potential security issues in ML model files
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Paths to scan: suspicious_model.pkl
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

‚úì Scanning suspicious_model.pkl

Active Scanner: pickle
Scan completed in 0.02 seconds
Files scanned: 1
Scanned 156 bytes
Issues found: 2 critical, 1 warnings

1. suspicious_model.pkl (pos 28): [CRITICAL] Suspicious module reference found: posix.system
2. suspicious_model.pkl (pos 52): [WARNING] Found REDUCE opcode - potential __reduce__ method execution

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚úó Scan completed with findings
```

## ‚ú® Features

### Core Capabilities

- **Multiple Format Support**: PyTorch (.pt, .pth, .bin), TensorFlow (SavedModel, .pb), Keras (.h5, .hdf5, .keras), SafeTensors (.safetensors), GGUF/GGML (.gguf, .ggml), Pickle (.pkl, .pickle, .ckpt), Joblib (.joblib), NumPy (.npy, .npz), ZIP archives (.zip), Manifests (.json, .yaml, .xml, etc.)
- **Automatic Format Detection**: Identifies model formats automatically
- **Deep Security Analysis**: Examines model internals, not just metadata
- **Recursive Archive Scanning**: Scans contents of ZIP files and nested archives
- **Batch Processing**: Scan multiple files and directories efficiently
- **Configurable Scanning**: Set timeouts, file size limits, custom blacklists

### Reporting & Integration

- **Multiple Output Formats**: Human-readable text and machine-readable JSON
- **Detailed Reporting**: Scan duration, files processed, bytes scanned, issue severity
- **Severity Levels**: CRITICAL, WARNING, INFO, DEBUG for flexible filtering
- **CI/CD Integration**: Clear exit codes for automated pipeline integration

### Security Detection

- **Code Execution**: Detects embedded Python code, eval/exec calls, system commands
- **Pickle Security**: Analyzes dangerous opcodes, suspicious imports, encoded payloads
- **Model Integrity**: Checks for unexpected files, suspicious configurations
- **Archive Security**: Automatic Zip-Slip protection against directory traversal, zip bombs, malicious nested files
- **Pattern Matching**: Custom blacklist patterns for organizational policies

## üõ°Ô∏è Supported Model Formats

ModelAudit provides specialized security scanners for different model formats:

| Format             | File Extensions                                                                                          | What We Check                                                   |
| ------------------ | -------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| **Pickle**         | `.pkl`, `.pickle`, `.bin`, `.pt`, `.pth`, `.ckpt`                                                        | Malicious code execution, dangerous opcodes, suspicious imports |
| **PyTorch Zip**    | `.pt`, `.pth`                                                                                            | Embedded pickle analysis, suspicious files, custom patterns     |
| **PyTorch Binary** | `.bin`                                                                                                   | Binary tensor data analysis, embedded content                   |
| **TensorFlow**     | SavedModel dirs, `.pb`                                                                                   | Suspicious operations, file I/O, Python execution               |
| **Keras**          | `.h5`, `.hdf5`, `.keras`                                                                                 | Lambda layers, custom objects, dangerous configurations         |
| **ONNX**           | `.onnx`                                                                                                  | Custom operators, external data validation, tensor integrity    |
| **SafeTensors**    | `.safetensors`                                                                                           | Metadata integrity, tensor validation                           |
| **GGUF/GGML**      | `.gguf`, `.ggml`                                                                                         | Header validation, tensor integrity, metadata security checks   |
| **Joblib**         | `.joblib`                                                                                                | Compression bomb detection, embedded pickle analysis            |
| **NumPy**          | `.npy`, `.npz`                                                                                           | Array integrity, dangerous dtypes, dimension validation         |
| **ZIP Archives**   | `.zip`                                                                                                   | Recursive content scanning, zip bombs, directory traversal      |
| **Manifests**      | `.json`, `.yaml`, `.yml`, `.xml`, `.toml`, `.ini`, `.cfg`, `.config`, `.manifest`, `.model`, `.metadata` | Suspicious keys, credential exposure, blacklisted patterns      |

### Weight Analysis

ModelAudit can detect anomalous weight patterns that may indicate trojaned models using statistical analysis. This feature is disabled by default for large language models to avoid false positives.

## ‚öôÔ∏è Advanced Usage

### Command Line Options

```bash
# Set maximum file size to scan (1GB limit)
modelaudit scan model.pkl --max-file-size 1073741824

# Add custom blacklist patterns
modelaudit scan model.pkl --blacklist "unsafe_model" --blacklist "malicious_net"

# Set scan timeout (5 minutes)
modelaudit scan large_model.pkl --timeout 300

# Verbose output for debugging
modelaudit scan model.pkl --verbose
```

### Exit Codes

ModelAudit uses different exit codes to indicate scan results:

- **0**: Success - No security issues found
- **1**: Security issues found (scan completed successfully)
- **2**: Errors occurred during scanning (e.g., file not found, scan failures)

## üìã JSON Output Format

When using `--format json`, ModelAudit outputs structured results:

```json
{
  "scanner_names": ["pickle"],
  "start_time": 1750168822.481906,
  "bytes_scanned": 74,
  "issues": [
    {
      "message": "Found REDUCE opcode - potential __reduce__ method execution",
      "severity": "warning",
      "location": "evil.pickle (pos 71)",
      "details": {
        "position": 71,
        "opcode": "REDUCE",
        "ml_context_confidence": 0.0
      },
      "timestamp": 1750168822.482304
    },
    {
      "message": "Suspicious module reference found: posix.system",
      "severity": "critical",
      "location": "evil.pickle (pos 28)",
      "details": {
        "module": "posix",
        "function": "system",
        "position": 28,
        "opcode": "STACK_GLOBAL",
        "ml_context_confidence": 0.0
      },
      "timestamp": 1750168822.482378
    }
  ],
  "has_errors": false,
  "files_scanned": 1,
  "duration": 0.0005328655242919922
}
```

Each issue includes a `message`, `severity` level (`critical`, `warning`, `info`, `debug`), `location`, and scanner-specific `details`.

## üîÑ CI/CD Integration

ModelAudit is designed to integrate seamlessly into CI/CD pipelines with clear exit codes:

- **Exit Code 0**: No security issues found
- **Exit Code 1**: Security issues found (fails the build)
- **Exit Code 2**: Scan errors occurred (fails the build)

### Basic Integration

```bash
# Install ModelAudit
pip install modelaudit[all]

# Scan models and fail build if issues found
modelaudit scan models/ --format json --output scan-results.json

# Optional: Upload scan-results.json as build artifact
```

### Platform Examples

**GitHub Actions:**

```yaml
- name: Scan models
  run: |
    pip install modelaudit[all]
    modelaudit scan models/ --format json --output results.json
```

**GitLab CI:**

```yaml
model-security-scan:
  script:
    - pip install modelaudit[all]
    - modelaudit scan models/ --format json --output results.json
  artifacts:
    paths: [results.json]
```

**Jenkins:**

```groovy
sh 'pip install modelaudit[all]'
sh 'modelaudit scan models/ --format json --output results.json'
```

## üîß Troubleshooting

### Common Issues

**Installation Problems:**

```bash
# If you get dependency conflicts
pip install --upgrade pip setuptools wheel
pip install modelaudit[all] --no-cache-dir

# If optional dependencies fail, install base package first
pip install modelaudit
pip install tensorflow h5py torch pyyaml safetensors onnx joblib  # Add what you need
```

**Large Models:**

```bash
# Increase file size limit and timeout for large models
modelaudit scan large_model.pt --max-file-size 5000000000 --timeout 600
```

**Debug Mode:**

```bash
# Enable verbose output for troubleshooting
modelaudit scan model.pkl --verbose
```

**Getting Help:**

- Use `--verbose` for detailed output
- Use `--format json` to see all details
- Check file permissions and format support
- Report issues on the [promptfoo GitHub repository](https://github.com/promptfoo/promptfoo/issues)

## ‚ö†Ô∏è Limitations

ModelAudit is designed to find **obvious security risks** in model files, including direct code execution attempts, known dangerous patterns, malicious archive structures, and suspicious configurations.

**What it cannot detect:**

- Advanced adversarial attacks or subtle weight manipulation
- Heavily encoded/encrypted malicious payloads
- Runtime behavior that only triggers under specific conditions
- Model poisoning through careful data manipulation

**Recommendations:**

- Use ModelAudit as one layer of your security strategy
- Review flagged issues manually - not all warnings indicate malicious intent
- Combine with other security practices like sandboxed execution and runtime monitoring
- Implement automated scanning in CI/CD pipelines

## üìù License

This project is licensed under the [MIT License](https://opensource.org/licenses/MIT) - see the [LICENSE](LICENSE) file for details.


=====================================
FILE: ./TODO.md
=====================================

# ModelAudit Security Audit - TODO List

## üö® CRITICAL SECURITY ISSUES (Fix Immediately)

### 1. Security Vulnerabilities in Core Scanning Logic
- [x] **CRITICAL**: Fix potential code execution in pickle scanner unsafe operations
- [x] **CRITICAL**: Add proper path traversal protection in file path handling
- [x] **CRITICAL**: Implement resource exhaustion limits for large model scanning
- [x] **CRITICAL**: Secure deserialization handling in scanners themselves

### 2. Input Validation & Sanitization
- [ ] **HIGH**: Implement comprehensive file type validation using magic numbers
- [ ] **HIGH**: Add consistent size limits across all scanners
- [ ] **HIGH**: Fix path injection vulnerabilities in user-provided paths
- [ ] **HIGH**: Validate configuration parameters properly

### 3. Error Handling & Resource Management
- [ ] **HIGH**: Fix unhandled exceptions in scanner modules
- [ ] **HIGH**: Prevent silent failures from propagating
- [ ] **HIGH**: Implement proper resource cleanup (file handles, memory)
- [ ] **HIGH**: Add file locking mechanisms for concurrent access

### 4. Threading & Concurrency Issues
- [ ] **MEDIUM**: Make core scanning logic thread-safe
- [ ] **MEDIUM**: Implement proper synchronization for shared resources
- [ ] **MEDIUM**: Add concurrent scanning support with proper coordination

## üêõ BUGS & CODE QUALITY ISSUES

### 5. Architecture & Design Issues
- [ ] **MEDIUM**: Standardize scanner interface and patterns
- [ ] **MEDIUM**: Implement consistent error handling across scanners
- [ ] **MEDIUM**: Reduce code duplication in scanning logic
- [ ] **LOW**: Break down large functions (>100 lines) in pickle_scanner.py

### 6. Performance Issues
- [ ] **MEDIUM**: Implement caching for repeated file scans
- [ ] **MEDIUM**: Add streaming analysis for large models
- [ ] **MEDIUM**: Implement parallel processing for directory scanning
- [ ] **LOW**: Optimize memory usage patterns

### 7. Configuration Management
- [ ] **MEDIUM**: Replace hardcoded values with configurable options
- [ ] **MEDIUM**: Add configuration validation and defaults
- [ ] **LOW**: Create configuration schema documentation

## üìö DOCUMENTATION & USABILITY

### 8. Documentation Problems
- [ ] **HIGH**: Create comprehensive examples in examples/ directory
- [ ] **HIGH**: Add proper API documentation with docstrings
- [ ] **MEDIUM**: Fix inconsistent severity level documentation
- [ ] **MEDIUM**: Create installation troubleshooting guide
- [ ] **LOW**: Add type hints to all public functions

### 9. User Experience
- [ ] **MEDIUM**: Create interactive web dashboard
- [ ] **MEDIUM**: Improve CLI interface with better output formatting
- [ ] **MEDIUM**: Add guided remediation suggestions
- [ ] **LOW**: Create IDE plugins for popular editors

## üöÄ FEATURE ENHANCEMENTS

### 10. Advanced Threat Detection
- [ ] **HIGH**: Implement federated learning attack detection
- [ ] **HIGH**: Add model inversion attack scanning
- [ ] **MEDIUM**: Create membership inference vulnerability tests
- [ ] **MEDIUM**: Add adversarial robustness testing
- [ ] **MEDIUM**: Implement advanced backdoor detection

### 11. Enterprise Features
- [ ] **HIGH**: Create configurable policy engine
- [ ] **HIGH**: Add comprehensive audit trails
- [ ] **MEDIUM**: Implement role-based access controls
- [ ] **MEDIUM**: Create approval workflow system
- [ ] **LOW**: Add SIEM integration capabilities

### 12. Integration & Automation
- [ ] **HIGH**: Create native CI/CD platform plugins
- [ ] **HIGH**: Add MLOps platform integrations
- [ ] **MEDIUM**: Build RESTful API for programmatic access
- [ ] **MEDIUM**: Create pre-commit hooks
- [ ] **LOW**: Add webhook support for notifications

### 13. AI-Powered Analysis
- [ ] **MEDIUM**: Implement behavioral pattern analysis
- [ ] **MEDIUM**: Add ML-based anomaly detection
- [ ] **MEDIUM**: Create comprehensive risk scoring
- [ ] **LOW**: Build false positive reduction system

## üéØ PRODUCT STRATEGY

### 14. Market & Community
- [ ] **HIGH**: Establish bug bounty program
- [ ] **HIGH**: Create security advisory publication process
- [ ] **MEDIUM**: Build academic partnerships
- [ ] **MEDIUM**: Contribute to industry standards
- [ ] **LOW**: Expand conference presence

### 15. Performance & Scalability
- [ ] **MEDIUM**: Implement distributed scanning architecture
- [ ] **MEDIUM**: Add cloud storage integrations
- [ ] **MEDIUM**: Create optimized container images
- [ ] **LOW**: Add GPU acceleration support

### 16. Business Model Evolution
- [ ] **LOW**: Design open core feature separation
- [ ] **LOW**: Plan SaaS platform architecture
- [ ] **LOW**: Create professional services offerings
- [ ] **LOW**: Develop training & certification programs

## üìä TESTING & QUALITY ASSURANCE

### 17. Test Coverage & Quality
- [ ] **HIGH**: Add comprehensive security vulnerability tests
- [ ] **HIGH**: Create integration tests for all scanners
- [ ] **MEDIUM**: Add performance benchmarking tests
- [ ] **MEDIUM**: Implement fuzzing tests for input validation
- [ ] **LOW**: Add property-based testing

### 18. CI/CD Improvements
- [ ] **MEDIUM**: Add security scanning in CI pipeline
- [ ] **MEDIUM**: Implement automated performance regression tests
- [ ] **MEDIUM**: Add dependency vulnerability scanning
- [ ] **LOW**: Create multi-platform testing matrix

## üìà METRICS & MONITORING

### 19. Success Metrics
- [ ] **MEDIUM**: Implement usage analytics (privacy-respecting)
- [ ] **MEDIUM**: Track vulnerability detection effectiveness
- [ ] **LOW**: Monitor performance improvements
- [ ] **LOW**: Measure community engagement

### 20. Monitoring & Alerting
- [ ] **MEDIUM**: Add runtime error monitoring
- [ ] **MEDIUM**: Create performance monitoring dashboard
- [ ] **LOW**: Implement security alert system
- [ ] **LOW**: Add health check endpoints

---

## Priority Levels
- **CRITICAL**: Security vulnerabilities that could lead to code execution
- **HIGH**: Issues affecting security, functionality, or user experience
- **MEDIUM**: Improvements that enhance capabilities or maintainability
- **LOW**: Nice-to-have features and optimizations

## Current Focus
Starting with item #1: Security Vulnerabilities in Core Scanning Logic 

=====================================
FILE: ./biome.json
=====================================

[BINARY FILE - CONTENT SKIPPED]


=====================================
FILE: ./docker-compose.yml
=====================================

version: "3"

services:
  modelaudit-base:
    build: .
    volumes:
      - ./models:/models
    command: scan --help

  modelaudit-tensorflow:
    build:
      context: .
      dockerfile: Dockerfile.tensorflow
    volumes:
      - ./models:/models
    command: scan --help

  modelaudit-full:
    build:
      context: .
      dockerfile: Dockerfile.full
    volumes:
      - ./models:/models
    command: scan --help


=====================================
FILE: ./llm_context.txt
=====================================

## Overview
This project, "modelaudit," scans ML model files for suspicious or malicious content. It also supports blacklisting certain model names or families (e.g. "llama"). The overall purpose is to enhance security around model files that might contain malicious code, especially in Python-based serialization formats (Pickle, PyTorch .pt, Keras .h5, TensorFlow SavedModel, etc.).

## Purpose
1. **Security / Compliance**: Detect potential code-injection attacks (e.g. `os.system` in a pickle) before models are used internally.
2. **Pre-Deployment Checks**: Integrate with CI/CD so that any new model artifact gets scanned automatically.
3. **Name-Based Policies**: For example, disallow certain model families by name ("llama," "alpaca," etc.).
4. **Extendability**: The design is modular, so new scanners or rules can be added as the ML ecosystem evolves.

## Project Structure
- **pyproject.toml**: Defines build system and dependencies (optional extras for TensorFlow, PyTorch, h5py, dev tooling).
- **README.md**: Intro to usage, installation, purpose.
- **setup.cfg**: Additional packaging configuration.
- **modelaudit/** (main code folder):
  - **__init__.py**: Version info.
  - **cli.py**: Provides the command-line interface (`modelaudit scan <path>`).
  - **core.py**: High-level logic to decide how to scan a path. It uses `detect_file_format` from `utils/filetype.py` and calls the appropriate scanner.
  - **name_policies/**:
    - **blacklist.py**: Example of a simple text-based blacklist for model names.
  - **scanners/**:
    - **base.py**: Shared `ScanResult` class that collects issues.
    - **pickle_scanner.py**: Scans for suspicious code references in pickles.
    - **pytorch_zip_scanner.py**: Scans PyTorch's newer zip-based `.pt` or `.pth` files by extracting embedded `.pkl`s.
    - **keras_h5_scanner.py**: Checks Keras .h5 files for `Lambda` layers or other suspicious config.
    - **tf_savedmodel_scanner.py**: Looks for suspicious ops in TensorFlow SavedModel (e.g. `ReadFile` or `WriteFile`).
  - **utils/**:
    - **filetype.py**: More robust format detection by checking magic numbers, file extensions, or presence of special files like `saved_model.pb`. Also has logic to gather Hugging Face shards.
- **tests/**: Contains a simple `test_basic.py`.

## Shortcomings / Potential Improvements
1. **Partial / Chunk-Based Scanning**: Large files might need streaming to avoid big memory usage.
2. **Time Limits / Resource Constraints**: Production usage likely needs a timeout or safety checks if the file is huge or corrupted.
3. **Advanced Name Inference**: Currently minimal; if you want to parse a Hugging Face‚Äìstyle `config.json` or `model_index.json` to detect the real model name, you'd have to implement that.
4. **Safetensors / ONNX**: Provided placeholders, but the scanning logic is minimal or missing for those. Real usage might need further coverage.
5. **Caching / Hash Checking**: Large organizations might want caching of scan results based on file checksums.
6. **Policy Customization**: A simple blacklist is included, but advanced or dynamic rule sets (with version checks, in-code DSL, etc.) would require custom expansions.
7. **Parallel / Distributed Scanning**: For big MLOps pipelines, you might want concurrency or a microservice approach.
8. **Dependency Management**: Some scanners need optional packages (TensorFlow, h5py). If these are not installed, scanning is skipped or yields an error message.

## Enhanced Scanner Architecture for Production Readiness
Based on the review of the scanners, here are improvements to make the system more robust and production-ready:

1. **Standardized Scanner Interface**: - DONE!
   - Create an abstract base class with common methods all scanners must implement
   - Define consistent input/output contracts for all scanners
   - Implement capability detection (can_handle method) for each scanner

2. **Enhanced Error Handling and Reporting**: - DONE!
   - Classify issues by severity (INFO, WARNING, ERROR)
   - Return structured results (JSON) with clear distinctions between errors and findings
   - Add graceful error recovery to continue scanning when possible

3. **Performance Improvements**:
   - Process large files in chunks to avoid OOM errors
   - Add configurable timeouts for each scanner to prevent hanging
   - Implement progress reporting for tracking large file scans

4. **Extensibility**:
   - Create a plugin system for loading custom scanners
   - Support external rule configuration files
   - Enable version-specific scanning rules

5. **Production Features**:
   - Implement file hash-based caching to avoid rescanning unchanged files
   - Support scanning multiple files concurrently
   - Add metrics collection and telemetry
   - Provide comprehensive logging with configurable levels

6. **Security Enhancements**:
   - Run high-risk operations in a sandboxed environment
   - Expand detection rule sets for more comprehensive coverage
   - Support model signature verification
   - Implement resource limits during scanning

7. **Testing Improvements**:
   - Create a comprehensive test corpus with known-good and known-bad samples
   - Add fuzz testing to ensure scanners don't crash on malformed inputs
   - Measure and optimize scanner performance

8. **CLI Improvements**:
   - Add progress bars for large files
   - Support multiple output formats (text, JSON, HTML report)
   - Implement batch operations for scanning entire directories

## Key Points for an LLM Developer
- **Modularity**: Each scanner is an isolated module. New ones can be added for additional formats (like `.npz`, safetensors, or others).
- **Context**: "modelaudit" is a base project that you can build on to handle security scanning in your ML pipeline. 
- **Usage**: `modelaudit scan /path/to/model` is the main CLI entry. 
- **Dependencies**: The optional extras `[tensorflow,h5,pytorch]` let you scan specific frameworks.

The main idea is to keep scanning logic easy to modify or expand, ensuring that all major ML formats used by your organization can be checked for malicious content before use in production.


=====================================
FILE: ./modelaudit/.pytest_cache/README.md
=====================================

# pytest cache directory

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


=====================================
FILE: ./modelaudit/__init__.py
=====================================

"""ModelAudit package initialization.

This package uses the modern single-source version approach recommended by the
Python Packaging Authority (PyPA) as of 2025. The version is defined once in
pyproject.toml and accessed at runtime via importlib.metadata.
"""

from importlib.metadata import PackageNotFoundError, version

try:
    __version__ = version("modelaudit")
except PackageNotFoundError:
    # Package is not installed or in development mode
    __version__ = "unknown"


=====================================
FILE: ./modelaudit/cli.py
=====================================

import json
import logging
import os
import sys
import time
from typing import Any

import click
from yaspin import yaspin
from yaspin.spinners import Spinners

from . import __version__
from .core import determine_exit_code, scan_model_directory_or_file

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger("modelaudit")


@click.group()
@click.version_option(__version__)
def cli():
    """Static scanner for ML models"""
    pass


@cli.command("scan")
@click.argument("paths", nargs=-1, type=click.Path(exists=True), required=True)
@click.option(
    "--blacklist",
    "-b",
    multiple=True,
    help="Additional blacklist patterns to check against model names",
)
@click.option(
    "--format",
    "-f",
    type=click.Choice(["text", "json"]),
    default="text",
    help="Output format [default: text]",
)
@click.option(
    "--output",
    "-o",
    type=click.Path(),
    help="Output file path (prints to stdout if not specified)",
)
@click.option(
    "--timeout",
    "-t",
    type=int,
    default=300,
    help="Scan timeout in seconds [default: 300]",
)
@click.option("--verbose", "-v", is_flag=True, help="Enable verbose output")
@click.option(
    "--max-file-size",
    type=int,
    default=0,
    help="Maximum file size to scan in bytes [default: unlimited]",
)
def scan_command(paths, blacklist, format, output, timeout, verbose, max_file_size):
    """Scan files or directories for malicious content.

    \b
    Usage:
        modelaudit scan /path/to/model1 /path/to/model2 ...

    You can specify additional blacklist patterns with ``--blacklist`` or ``-b``:

        modelaudit scan /path/to/model1 /path/to/model2 -b llama -b alpaca

    \b
    Advanced options:
        --format, -f       Output format (text or json)
        --output, -o       Write results to a file instead of stdout
        --timeout, -t      Set scan timeout in seconds
        --verbose, -v      Show detailed information during scanning
        --max-file-size    Maximum file size to scan in bytes

    \b
    Exit codes:
        0 - Success, no security issues found
        1 - Security issues found (scan completed successfully)
        2 - Errors occurred during scanning
    """
    # Print a nice header if not in JSON mode and not writing to a file
    if format == "text" and not output:
        header = [
            "‚îÄ" * 80,
            click.style("ModelAudit Security Scanner", fg="blue", bold=True),
            click.style(
                "Scanning for potential security issues in ML model files",
                fg="cyan",
            ),
            "‚îÄ" * 80,
        ]
        click.echo("\n".join(header))
        click.echo(f"Paths to scan: {click.style(', '.join(paths), fg='green')}")
        if blacklist:
            click.echo(
                f"Additional blacklist patterns: "
                f"{click.style(', '.join(blacklist), fg='yellow')}",
            )
        click.echo("‚îÄ" * 80)
        click.echo("")

    # Set logging level based on verbosity
    if verbose:
        logger.setLevel(logging.DEBUG)

    # Aggregated results
    aggregated_results = {
        "scanner_names": [],  # Track all scanner names used
        "start_time": time.time(),
        "bytes_scanned": 0,
        "issues": [],
        "has_errors": False,
        "files_scanned": 0,
    }

    # Scan each path
    for path in paths:
        # Early exit for common non-model file extensions
        if os.path.isfile(path):
            _, ext = os.path.splitext(path)
            ext = ext.lower()
            if ext in (
                ".md",
                ".txt",
                ".py",
                ".js",
                ".html",
                ".css",
                ".json",
                ".yaml",
                ".yml",
            ):
                if verbose:
                    logger.info(f"Skipping non-model file: {path}")
                click.echo(f"Skipping non-model file: {path}")
                continue

        # Show progress indicator if in text mode and not writing to a file
        spinner = None
        if format == "text" and not output:
            spinner_text = f"Scanning {click.style(path, fg='cyan')}"
            spinner = yaspin(Spinners.dots, text=spinner_text)
            spinner.start()

        # Perform the scan with the specified options
        try:
            # Define progress callback if using spinner
            progress_callback = None
            if spinner:

                def update_progress(message, percentage):
                    spinner.text = f"{message} ({percentage:.1f}%)"

                progress_callback = update_progress

            # Run the scan with progress reporting
            results = scan_model_directory_or_file(
                path,
                blacklist_patterns=list(blacklist) if blacklist else None,
                timeout=timeout,
                max_file_size=max_file_size,
                progress_callback=progress_callback,
            )

            # Aggregate results
            aggregated_results["bytes_scanned"] += results.get("bytes_scanned", 0)
            aggregated_results["issues"].extend(results.get("issues", []))
            aggregated_results["files_scanned"] += results.get(
                "files_scanned",
                1,
            )  # Count each file scanned
            if results.get("has_errors", False):
                aggregated_results["has_errors"] = True

            # Track scanner names
            for scanner in results.get("scanners", []):
                if (
                    scanner
                    and scanner not in aggregated_results["scanner_names"]
                    and scanner != "unknown"
                ):
                    aggregated_results["scanner_names"].append(scanner)

            # Show completion status if in text mode and not writing to a file
            if spinner:
                if results.get("issues", []):
                    # Filter out DEBUG severity issues when not in verbose mode
                    visible_issues = [
                        issue
                        for issue in results.get("issues", [])
                        if verbose
                        or not isinstance(issue, dict)
                        or issue.get("severity") != "debug"
                    ]
                    issue_count = len(visible_issues)
                    spinner.text = f"Scanned {click.style(path, fg='cyan')}"
                    if issue_count > 0:
                        spinner.ok(
                            click.style(
                                f"‚úì Found {issue_count} issues!",
                                fg="yellow",
                                bold=True,
                            ),
                        )
                    else:
                        spinner.ok(click.style("‚úì", fg="green", bold=True))
                else:
                    spinner.text = f"Scanned {click.style(path, fg='cyan')}"
                    spinner.ok(click.style("‚úì", fg="green", bold=True))

        except Exception as e:
            # Show error if in text mode and not writing to a file
            if spinner:
                spinner.text = f"Error scanning {click.style(path, fg='cyan')}"
                spinner.fail(click.style("‚úó", fg="red", bold=True))

            logger.error(f"Error during scan of {path}: {str(e)}", exc_info=verbose)
            click.echo(f"Error scanning {path}: {str(e)}", err=True)
            aggregated_results["has_errors"] = True

    # Calculate total duration
    aggregated_results["duration"] = time.time() - aggregated_results["start_time"]

    # Format the output
    if format == "json":
        output_data = aggregated_results
        output_text = json.dumps(output_data, indent=2)
    else:
        # Text format
        output_text = format_text_output(aggregated_results, verbose)

    # Send output to the specified destination
    if output:
        with open(output, "w") as f:
            f.write(output_text)
        click.echo(f"Results written to {output}")
    else:
        # Add a separator line between debug output and scan results
        if format == "text":
            click.echo("\n" + "‚îÄ" * 80)
        click.echo(output_text)

    # Exit with appropriate error code based on scan results
    exit_code = determine_exit_code(aggregated_results)
    sys.exit(exit_code)


def format_text_output(results: dict[str, Any], verbose: bool = False) -> str:
    """Format scan results as human-readable text with colors"""
    output_lines = []

    # Add summary information with styling
    if "scanner_names" in results and results["scanner_names"]:
        scanner_names = results["scanner_names"]
        if len(scanner_names) == 1:
            output_lines.append(
                click.style(
                    f"Active Scanner: {scanner_names[0]}", fg="blue", bold=True
                ),
            )
        else:
            output_lines.append(
                click.style(
                    f"Active Scanners: {', '.join(scanner_names)}",
                    fg="blue",
                    bold=True,
                ),
            )
    if "duration" in results:
        duration = results["duration"]
        if duration < 0.01:
            # For very fast scans, show more precision
            output_lines.append(
                click.style(
                    f"Scan completed in {duration:.3f} seconds",
                    fg="cyan",
                ),
            )
        else:
            output_lines.append(
                click.style(
                    f"Scan completed in {duration:.2f} seconds",
                    fg="cyan",
                ),
            )
    if "files_scanned" in results:
        output_lines.append(
            click.style(f"Files scanned: {results['files_scanned']}", fg="cyan"),
        )
    if "bytes_scanned" in results:
        # Format bytes in a more readable way
        bytes_scanned = results["bytes_scanned"]
        if bytes_scanned >= 1024 * 1024 * 1024:
            size_str = f"{bytes_scanned / (1024 * 1024 * 1024):.2f} GB"
        elif bytes_scanned >= 1024 * 1024:
            size_str = f"{bytes_scanned / (1024 * 1024):.2f} MB"
        elif bytes_scanned >= 1024:
            size_str = f"{bytes_scanned / 1024:.2f} KB"
        else:
            size_str = f"{bytes_scanned} bytes"
        output_lines.append(click.style(f"Scanned {size_str}", fg="cyan"))

    # Add issue details with color-coded severity
    issues = results.get("issues", [])
    # Filter out DEBUG severity issues when not in verbose mode
    visible_issues = [
        issue
        for issue in issues
        if verbose or not isinstance(issue, dict) or issue.get("severity") != "debug"
    ]

    if visible_issues:
        # Count issues by severity (excluding DEBUG when not in verbose mode)
        error_count = sum(
            1
            for issue in visible_issues
            if isinstance(issue, dict) and issue.get("severity") == "critical"
        )
        warning_count = sum(
            1
            for issue in visible_issues
            if isinstance(issue, dict) and issue.get("severity") == "warning"
        )
        info_count = sum(
            1
            for issue in visible_issues
            if isinstance(issue, dict) and issue.get("severity") == "info"
        )
        debug_count = sum(
            1
            for issue in issues
            if isinstance(issue, dict) and issue.get("severity") == "debug"
        )

        # Only show debug count in verbose mode
        issue_summary = []
        if error_count:
            issue_summary.append(
                click.style(f"{error_count} critical", fg="red", bold=True),
            )
        if warning_count:
            issue_summary.append(click.style(f"{warning_count} warnings", fg="yellow"))
        if info_count:
            issue_summary.append(click.style(f"{info_count} info", fg="blue"))
        if verbose and debug_count:
            issue_summary.append(click.style(f"{debug_count} debug", fg="cyan"))

        if issue_summary:
            output_lines.append(
                click.style("Issues found: ", fg="white") + ", ".join(issue_summary),
            )

        # Only display visible issues
        for i, issue in enumerate(visible_issues, 1):
            severity = issue.get("severity", "warning").lower()

            # Skip debug issues if verbose is not enabled
            if severity == "debug" and not verbose:
                continue

            message = issue.get("message", "Unknown issue")
            location = issue.get("location", "")

            # Color-code based on severity
            if severity == "critical":
                severity_style = click.style("[CRITICAL]", fg="red", bold=True)
            elif severity == "warning":
                severity_style = click.style("[WARNING]", fg="yellow")
            elif severity == "info":
                severity_style = click.style("[INFO]", fg="blue")
            elif severity == "debug":
                severity_style = click.style("[DEBUG]", fg="bright_black")

            # Format the issue line
            issue_num = click.style(f"{i}.", fg="white", bold=True)
            if location:
                location_str = click.style(f"{location}", fg="cyan", bold=True)
                output_lines.append(
                    f"{issue_num} {location_str}: {severity_style} {message}",
                )
            else:
                output_lines.append(f"{issue_num} {severity_style} {message}")

            # Add "Why" explanation if available
            why = issue.get("why")
            if why:
                # Indent the explanation and style it
                why_label = click.style("   Why:", fg="magenta", bold=True)
                why_text = click.style(f" {why}", fg="bright_white")
                output_lines.append(f"{why_label}{why_text}")

            # Add a small separator between issues for readability
            if i < len(visible_issues):
                output_lines.append("")
    else:
        output_lines.append(
            "\n" + click.style("‚úì No issues found", fg="green", bold=True),
        )

    # Add a footer
    output_lines.append("‚îÄ" * 80)
    if visible_issues:
        if any(
            isinstance(issue, dict) and issue.get("severity") == "critical"
            for issue in visible_issues
        ):
            status = click.style("‚úó Scan completed with findings", fg="red", bold=True)
        else:
            status = click.style(
                "‚ö† Scan completed with warnings",
                fg="yellow",
                bold=True,
            )
    else:
        status = click.style("‚úì Scan completed successfully", fg="green", bold=True)
    output_lines.append(status)

    return "\n".join(output_lines)


def main():
    cli()


=====================================
FILE: ./modelaudit/core.py
=====================================

import logging
import os
import time
from pathlib import Path
from typing import Any, Callable, Optional, cast

from modelaudit.scanners import SCANNER_REGISTRY
from modelaudit.scanners.base import IssueSeverity, ScanResult
from modelaudit.utils.filetype import detect_file_format

logger = logging.getLogger("modelaudit.core")


def scan_model_directory_or_file(
    path: str,
    blacklist_patterns: Optional[list[str]] = None,
    timeout: int = 300,
    max_file_size: int = 0,
    progress_callback: Optional[Callable[[str, float], None]] = None,
    **kwargs,
) -> dict[str, Any]:
    """
    Scan a model file or directory for malicious content.

    Args:
        path: Path to the model file or directory
        blacklist_patterns: Additional blacklist patterns to check against model names
        timeout: Scan timeout in seconds
        max_file_size: Maximum file size to scan in bytes
        progress_callback: Optional callback function to report progress
                          (message, percentage)
        **kwargs: Additional arguments to pass to scanners

    Returns:
        Dictionary with scan results
    """
    # Start timer for timeout
    start_time = time.time()

    # Initialize results with proper type hints
    results: dict[str, Any] = {
        "start_time": start_time,
        "path": path,
        "bytes_scanned": 0,
        "issues": [],
        "success": True,
        "files_scanned": 0,
        "scanners": [],  # Track the scanners used
    }

    # Configure scan options
    config = {
        "blacklist_patterns": blacklist_patterns,
        "max_file_size": max_file_size,
        "timeout": timeout,
        **kwargs,
    }

    try:
        # Check if path exists
        if not os.path.exists(path):
            raise FileNotFoundError(f"Path does not exist: {path}")

        # Check if path is readable
        if not os.access(path, os.R_OK):
            raise PermissionError(f"Path is not readable: {path}")

        # Check if path is a directory
        if os.path.isdir(path):
            if progress_callback:
                progress_callback(f"Scanning directory: {path}", 0.0)

            # Scan all files in the directory
            total_files = sum(1 for _ in Path(path).rglob("*") if _.is_file())
            processed_files = 0

            for root, _, files in os.walk(path):
                for file in files:
                    file_path = os.path.join(root, file)

                    # Check timeout
                    if time.time() - start_time > timeout:
                        raise TimeoutError(f"Scan timeout after {timeout} seconds")

                    # Update progress
                    if progress_callback and total_files > 0:
                        processed_files += 1
                        progress_callback(
                            f"Scanning file {processed_files}/{total_files}: {file}",
                            processed_files / total_files * 100,
                        )

                    # Scan the file
                    try:
                        file_result = scan_file(file_path, config)
                        # Use cast to help mypy understand the types
                        results["bytes_scanned"] = (
                            cast(int, results["bytes_scanned"])
                            + file_result.bytes_scanned
                        )
                        results["files_scanned"] = (
                            cast(int, results["files_scanned"]) + 1
                        )  # Increment file count

                        # Track scanner name
                        scanner_name = file_result.scanner_name
                        scanners_list = cast(list[str], results["scanners"])
                        if scanner_name and scanner_name not in scanners_list:
                            scanners_list.append(scanner_name)

                        # Add issues from file scan
                        issues_list = cast(list[dict[str, Any]], results["issues"])
                        for issue in file_result.issues:
                            issues_list.append(issue.to_dict())
                    except Exception as e:
                        logger.warning(f"Error scanning file {file_path}: {str(e)}")
                        # Add as an issue
                        issues_list = cast(list[dict[str, Any]], results["issues"])
                        issues_list.append(
                            {
                                "message": f"Error scanning file: {str(e)}",
                                "severity": IssueSeverity.WARNING.value,
                                "location": file_path,
                                "details": {"exception_type": type(e).__name__},
                            },
                        )
        else:
            # Scan a single file
            if progress_callback:
                progress_callback(f"Scanning file: {path}", 0.0)

            # Get file size for progress reporting
            file_size = os.path.getsize(path)
            results["files_scanned"] = 1  # Single file scan

            # Create a wrapper for the file to report progress
            if progress_callback is not None and file_size > 0:
                import builtins
                from typing import IO

                original_builtins_open = builtins.open

                def progress_open(
                    file_path: str,
                    mode: str = "r",
                    *args,
                    **kwargs,
                ) -> IO[Any]:
                    file = original_builtins_open(file_path, mode, *args, **kwargs)
                    file_pos = 0

                    # Override read method to report progress
                    original_read = file.read

                    def progress_read(size: int = -1) -> Any:
                        nonlocal file_pos
                        data = original_read(size)
                        if isinstance(data, (str, bytes)):
                            file_pos += len(data)
                        if progress_callback is not None:
                            progress_callback(
                                f"Reading file: {os.path.basename(file_path)}",
                                min(file_pos / file_size * 100, 100),
                            )
                        return data

                    file.read = progress_read  # type: ignore[method-assign]
                    return file

                # Monkey patch open temporarily
                builtins.open = progress_open  # type: ignore

                try:
                    file_result = scan_file(path, config)
                finally:
                    # Restore original open
                    builtins.open = original_builtins_open
            else:
                file_result = scan_file(path, config)

            results["bytes_scanned"] = (
                cast(int, results["bytes_scanned"]) + file_result.bytes_scanned
            )

            # Track scanner name
            scanner_name = file_result.scanner_name
            scanners_list = cast(list[str], results["scanners"])
            if scanner_name and scanner_name not in scanners_list:
                scanners_list.append(scanner_name)

            # Add issues from file scan
            issues_list = cast(list[dict[str, Any]], results["issues"])
            for issue in file_result.issues:
                issues_list.append(issue.to_dict())

            if progress_callback:
                progress_callback(f"Completed scanning: {path}", 100.0)

    except Exception as e:
        logger.exception(f"Error during scan: {str(e)}")
        results["success"] = False
        issue_dict = {
            "message": f"Error during scan: {str(e)}",
            "severity": IssueSeverity.CRITICAL.value,
            "details": {"exception_type": type(e).__name__},
        }
        issues_list = cast(list[dict[str, Any]], results["issues"])
        issues_list.append(issue_dict)

    # Add final timing information
    results["finish_time"] = time.time()
    results["duration"] = cast(float, results["finish_time"]) - cast(
        float,
        results["start_time"],
    )

    # Determine if there were operational scan errors vs security findings
    # has_errors should only be True for operational errors (scanner crashes,
    # file not found, etc.) not for security findings detected in models
    operational_error_indicators = [
        # Scanner execution errors
        "Error during scan",
        "Error checking file size",
        "Error scanning file",
        "Scanner crashed",
        "Scan timeout",
        # File system errors
        "Path does not exist",
        "Path is not readable",
        "Permission denied",
        "File not found",
        # Dependency/environment errors
        "not installed, cannot scan",
        "Missing dependency",
        "Import error",
        "Module not found",
        # File format/corruption errors
        "not a valid",
        "Invalid file format",
        "Corrupted file",
        "Bad file signature",
        "Unable to parse",
        # Resource/system errors
        "Out of memory",
        "Disk space",
        "Too many open files",
    ]

    issues_list = cast(list[dict[str, Any]], results["issues"])
    results["has_errors"] = (
        any(
            any(
                indicator in issue.get("message", "")
                for indicator in operational_error_indicators
            )
            for issue in issues_list
            if isinstance(issue, dict)
            and issue.get("severity") == IssueSeverity.CRITICAL.value
        )
        or not results["success"]
    )

    return results


def determine_exit_code(results: dict[str, Any]) -> int:
    """
    Determine the appropriate exit code based on scan results.

    Exit codes:
    - 0: Success, no security issues found
    - 1: Security issues found (scan completed successfully)
    - 2: Operational errors occurred during scanning

    Args:
        results: Dictionary with scan results

    Returns:
        Exit code (0, 1, or 2)
    """
    # Check for operational errors first (highest priority)
    if results.get("has_errors", False):
        return 2

    # Check for any security findings (warnings, errors, or info issues)
    issues = results.get("issues", [])
    if issues:
        # Filter out DEBUG level issues for exit code determination
        non_debug_issues = [
            issue
            for issue in issues
            if isinstance(issue, dict) and issue.get("severity") != "debug"
        ]
        if non_debug_issues:
            return 1

    # No issues found
    return 0


def scan_file(path: str, config: dict[str, Any] = None) -> ScanResult:
    """
    Scan a single file with the appropriate scanner.

    Args:
        path: Path to the file to scan
        config: Optional scanner configuration

    Returns:
        ScanResult object with the scan results
    """
    if config is None:
        config = {}

    # Check file size first
    max_file_size = config.get("max_file_size", 0)  # Default unlimited
    try:
        file_size = os.path.getsize(path)
        if max_file_size > 0 and file_size > max_file_size:
            sr = ScanResult(scanner_name="size_check")
            sr.add_issue(
                f"File too large to scan: {file_size} bytes (max: {max_file_size})",
                severity=IssueSeverity.WARNING,
                details={
                    "file_size": file_size,
                    "max_file_size": max_file_size,
                    "path": path,
                },
            )
            return sr
    except OSError as e:
        sr = ScanResult(scanner_name="error")
        sr.add_issue(
            f"Error checking file size: {e}",
            severity=IssueSeverity.CRITICAL,
            details={"error": str(e), "path": path},
        )
        return sr

    logger.info(f"Scanning file: {path}")

    # Try to use scanners from the registry
    for scanner_class in SCANNER_REGISTRY:
        # These are concrete scanner classes, not the abstract BaseScanner
        if scanner_class.can_handle(path):
            logger.debug(f"Using {scanner_class.name} scanner for {path}")
            scanner = scanner_class(config=config)  # type: ignore[abstract]
            return scanner.scan(path)

    # If no scanner could handle the file, create a default unknown format result
    format_ = detect_file_format(path)
    sr = ScanResult(scanner_name="unknown")
    sr.add_issue(
        f"Unknown or unhandled format: {format_}",
        severity=IssueSeverity.DEBUG,
        details={"format": format_, "path": path},
    )
    return sr


def merge_scan_result(
    results: dict[str, Any],
    scan_result: ScanResult,
) -> dict[str, Any]:
    """
    Merge a ScanResult object into the results dictionary.

    Args:
        results: The existing results dictionary
        scan_result: The ScanResult object to merge

    Returns:
        The updated results dictionary
    """
    # Convert scan_result to dict if it's a ScanResult object
    if isinstance(scan_result, ScanResult):
        scan_dict = scan_result.to_dict()
    else:
        scan_dict = scan_result

    # Merge issues
    issues_list = cast(list[dict[str, Any]], results["issues"])
    for issue in scan_dict.get("issues", []):
        issues_list.append(issue)

    # Update bytes scanned
    results["bytes_scanned"] = cast(int, results["bytes_scanned"]) + scan_dict.get(
        "bytes_scanned",
        0,
    )

    # Update scanner info if not already set
    if "scanner_name" not in results and "scanner" in scan_dict:
        results["scanner_name"] = scan_dict["scanner"]

    # Set success to False if any scan failed
    if not scan_dict.get("success", True):
        results["success"] = False

    return results


=====================================
FILE: ./modelaudit/explanations.py
=====================================

"""
Security issue explanations for ModelAudit.

This module provides centralized, security-team-friendly explanations
for common security issues found in ML model files.
"""

from typing import Optional

# Common explanations for dangerous imports and modules
DANGEROUS_IMPORTS = {
    "os": "The 'os' module provides direct access to operating system functions, allowing execution of arbitrary system commands, file system manipulation, and environment variable access. Malicious models can use this to compromise the host system, steal data, or install malware.",
    "posix": "The 'posix' module provides direct access to POSIX system calls on Unix-like systems. Like the 'os' module, it can execute arbitrary system commands and manipulate the file system. The 'posix.system' function is equivalent to 'os.system' and poses the same security risks.",
    "sys": "The 'sys' module provides access to interpreter internals and system-specific parameters. It can be used to modify the Python runtime, access command-line arguments, or manipulate the module import system to load malicious code.",
    "subprocess": "The 'subprocess' module allows spawning new processes and executing system commands. This is a critical security risk as it enables arbitrary command execution on the host system.",
    "eval": "The 'eval' function executes arbitrary Python code from strings. This is extremely dangerous as it allows dynamic code execution, potentially running any malicious code embedded in the model.",
    "exec": "The 'exec' function executes arbitrary Python statements from strings. Like eval, this enables unrestricted code execution and is a severe security risk.",
    "__import__": "The '__import__' function dynamically imports modules at runtime. Attackers can use this to load malicious modules or bypass import restrictions.",
    "importlib": "The 'importlib' module provides programmatic module importing capabilities. It can be used to dynamically load malicious code or bypass security controls.",
    "pickle": "Nested pickle operations (pickle.load/loads within a pickle) can indicate attempts to obfuscate malicious payloads or create multi-stage attacks.",
    "base64": "Base64 encoding/decoding functions are often used to obfuscate malicious payloads, making them harder to detect through static analysis.",
    "socket": "The 'socket' module enables network communication. Malicious models can use this to exfiltrate data, download additional payloads, or establish command & control channels.",
    "ctypes": "The 'ctypes' module provides low-level system access through foreign function interfaces. It can bypass Python's safety features and directly manipulate memory or call system libraries.",
    "pty": "The 'pty' module provides pseudo-terminal utilities. The 'spawn' function can be used to create interactive shells, potentially giving attackers remote access.",
    "platform": "Functions like 'platform.system' or 'platform.popen' can be used for system reconnaissance or command execution.",
    "shutil": "The 'shutil' module provides high-level file operations. Functions like 'rmtree' can recursively delete directories, potentially causing data loss.",
    "tempfile": "Unsafe temp file creation (like 'mktemp') can lead to race conditions and security vulnerabilities.",
    "runpy": "The 'runpy' module executes Python modules as scripts, potentially running malicious code embedded in the model.",
    "operator.attrgetter": "The 'attrgetter' function can be used to access object attributes dynamically, potentially bypassing access controls or reaching sensitive data.",
    "builtins": "Direct access to builtin functions can be used to bypass restrictions or access dangerous functionality like eval/exec.",
    "dill": "The 'dill' module extends pickle's capabilities to serialize almost any Python object, including lambda functions and code objects. This significantly increases the attack surface for code execution.",
}

# Explanations for dangerous pickle opcodes
DANGEROUS_OPCODES = {
    "REDUCE": "The REDUCE opcode calls a callable with arguments, effectively executing arbitrary Python functions. This is the primary mechanism for pickle-based code execution attacks through __reduce__ methods.",
    "INST": "The INST opcode instantiates objects by calling their class constructor. Malicious classes can execute code in __init__ methods during unpickling.",
    "OBJ": "The OBJ opcode creates class instances. Like INST, this can trigger code execution through object initialization.",
    "NEWOBJ": "The NEWOBJ opcode creates new-style class instances. It can execute initialization code and is commonly used in pickle exploits.",
    "NEWOBJ_EX": "The NEWOBJ_EX opcode is an extended version of NEWOBJ with additional capabilities for creating objects, potentially executing initialization code.",
    "BUILD": "The BUILD opcode updates object state and can trigger code execution through __setstate__ or __setattr__ methods.",
    "STACK_GLOBAL": "The STACK_GLOBAL opcode imports modules and retrieves attributes dynamically. Outside ML contexts, this often indicates attempts to access dangerous functionality.",
    "GLOBAL": "The GLOBAL opcode imports and accesses module attributes. When referencing dangerous modules, this indicates potential security risks.",
}

# Explanations for specific patterns and behaviors
PATTERN_EXPLANATIONS = {
    "base64_payload": "Base64-encoded data in models often conceals malicious payloads. Legitimate ML models rarely need encoded strings unless handling specific data formats.",
    "hex_encoded": "Hexadecimal-encoded strings (\\x00 format) can hide malicious code or data. This obfuscation technique is commonly used to evade detection.",
    "lambda_layer": "Lambda layers in Keras/TensorFlow can contain arbitrary Python code that executes during model inference. Unlike standard layers, these can perform system operations beyond tensor computations.",
    "executable_in_zip": "Executable files (.exe, .sh, .bat, etc.) within model archives are highly suspicious. ML models should only contain weights and configuration, not executables.",
    "dissimilar_weights": "Weight vectors that are completely dissimilar to others in the same layer may indicate injected malicious data masquerading as model parameters.",
    "outlier_neurons": "Neurons with weight distributions far outside the normal range might encode hidden functionality or backdoors rather than learned features.",
    "blacklisted_name": "This model name appears on security blacklists, indicating known malicious models or naming patterns associated with attacks.",
    "manifest_name_mismatch": "Model names in manifests that don't match expected patterns may indicate tampered or malicious models trying to impersonate legitimate ones.",
    "encoded_strings": "Encoded or obfuscated strings in model files often hide malicious payloads or commands from security scanners.",
    "pickle_size_limit": "Extremely large pickle files may indicate embedded malicious data or attempts to cause resource exhaustion.",
    "nested_pickle": "Pickle operations within pickled data (nested pickling) is often used to create multi-stage exploits or hide malicious payloads.",
    "torch_legacy": "Legacy PyTorch formats may have unpatched vulnerabilities. The _use_new_zipfile_serialization=False flag indicates use of the older, less secure format.",
}


# Function to get explanation for a security issue
def get_explanation(category: str, specific_item: str = None) -> Optional[str]:
    """
    Get a security explanation for a given category and item.

    Args:
        category: The category of security issue ('import', 'opcode', 'pattern')
        specific_item: The specific item (e.g., 'os', 'REDUCE', 'base64_payload')

    Returns:
        A security-team-friendly explanation, or None if not found
    """
    if category == "import" and specific_item in DANGEROUS_IMPORTS:
        return DANGEROUS_IMPORTS[specific_item]
    elif category == "opcode" and specific_item in DANGEROUS_OPCODES:
        return DANGEROUS_OPCODES[specific_item]
    elif category == "pattern" and specific_item in PATTERN_EXPLANATIONS:
        return PATTERN_EXPLANATIONS[specific_item]

    return None


# Convenience functions for common use cases
def get_import_explanation(module_name: str) -> Optional[str]:
    """Get explanation for a dangerous import/module."""
    # Handle module.function format (e.g., "os.system")
    base_module = module_name.split(".")[0]
    return get_explanation("import", base_module)


def get_opcode_explanation(opcode_name: str) -> Optional[str]:
    """Get explanation for a dangerous pickle opcode."""
    return get_explanation("opcode", opcode_name)


def get_pattern_explanation(pattern_name: str) -> Optional[str]:
    """Get explanation for a suspicious pattern."""
    return get_explanation("pattern", pattern_name)


=====================================
FILE: ./modelaudit/name_policies/__init__.py
=====================================

[BINARY FILE - CONTENT SKIPPED]


=====================================
FILE: ./modelaudit/name_policies/blacklist.py
=====================================

from typing import Optional

BLACKLIST_PATTERNS = [
    # Examples of patterns you might want to blacklist
    "malicious",
    "unsafe",
    # Add more patterns as needed
]


def check_model_name_policies(
    model_name: str,
    additional_patterns: Optional[list[str]] = None,
) -> tuple[bool, str]:
    """
    Return (blocked:boolean, reason:str) if model_name matches any pattern in
    the blacklist.

    Args:
        model_name: The name of the model to check
        additional_patterns: Optional list of additional patterns to check against
    """
    name_lower = model_name.lower()

    # Combine default patterns with any additional patterns
    patterns = list(BLACKLIST_PATTERNS)
    if additional_patterns:
        patterns.extend(additional_patterns)

    for pattern in patterns:
        if pattern.lower() in name_lower:
            return True, f"Model name matched blacklist pattern: {pattern}"
    return False, ""


=====================================
FILE: ./modelaudit/scanners/__init__.py
=====================================

from . import (
    base,
    gguf_scanner,
    joblib_scanner,
    keras_h5_scanner,
    manifest_scanner,
    numpy_scanner,
    oci_layer_scanner,
    onnx_scanner,
    pickle_scanner,
    pytorch_binary_scanner,
    pytorch_zip_scanner,
    safetensors_scanner,
    tf_savedmodel_scanner,
    weight_distribution_scanner,
    zip_scanner,
)

# Import scanner classes for direct use
from .base import BaseScanner, Issue, IssueSeverity, ScanResult
from .gguf_scanner import GgufScanner
from .joblib_scanner import JoblibScanner
from .keras_h5_scanner import KerasH5Scanner
from .manifest_scanner import ManifestScanner
from .numpy_scanner import NumPyScanner
from .oci_layer_scanner import OciLayerScanner
from .onnx_scanner import OnnxScanner
from .pickle_scanner import PickleScanner
from .pytorch_binary_scanner import PyTorchBinaryScanner
from .pytorch_zip_scanner import PyTorchZipScanner
from .safetensors_scanner import SafeTensorsScanner
from .tf_savedmodel_scanner import TensorFlowSavedModelScanner
from .weight_distribution_scanner import WeightDistributionScanner
from .zip_scanner import ZipScanner

# Create a registry of all available scanners
# Order matters - more specific scanners should come before generic ones
SCANNER_REGISTRY = [
    PickleScanner,
    PyTorchBinaryScanner,  # Must come before generic scanners for .bin files
    TensorFlowSavedModelScanner,
    KerasH5Scanner,
    OnnxScanner,
    PyTorchZipScanner,  # Must come before ZipScanner since .pt/.pth files are zip files
    GgufScanner,
    JoblibScanner,
    NumPyScanner,
    OciLayerScanner,
    ManifestScanner,
    WeightDistributionScanner,
    SafeTensorsScanner,
    ZipScanner,  # Generic zip scanner should be last
    # Add new scanners here as they are implemented
]

__all__ = [
    "BaseScanner",
    "GgufScanner",
    "Issue",
    "IssueSeverity",
    "JoblibScanner",
    "KerasH5Scanner",
    "ManifestScanner",
    "NumPyScanner",
    "OciLayerScanner",
    "OnnxScanner",
    "PickleScanner",
    "PyTorchBinaryScanner",
    "PyTorchZipScanner",
    "SCANNER_REGISTRY",
    "SafeTensorsScanner",
    "ScanResult",
    "TensorFlowSavedModelScanner",
    "WeightDistributionScanner",
    "ZipScanner",
    "base",
    "gguf_scanner",
    "joblib_scanner",
    "keras_h5_scanner",
    "manifest_scanner",
    "numpy_scanner",
    "oci_layer_scanner",
    "onnx_scanner",
    "pickle_scanner",
    "pytorch_binary_scanner",
    "pytorch_zip_scanner",
    "safetensors_scanner",
    "tf_savedmodel_scanner",
    "weight_distribution_scanner",
    "zip_scanner",
]


=====================================
FILE: ./modelaudit/scanners/base.py
=====================================

import json
import logging
import os
import time
from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, ClassVar, Optional

# Configure logging
logger = logging.getLogger("modelaudit.scanners")


class IssueSeverity(Enum):
    """Enum for issue severity levels"""

    DEBUG = "debug"  # Debug information
    INFO = "info"  # Informational, not a security concern
    WARNING = "warning"  # Potential issue, needs review
    CRITICAL = "critical"  # Definite security concern


class Issue:
    """Represents a single issue found during scanning"""

    def __init__(
        self,
        message: str,
        severity: IssueSeverity = IssueSeverity.WARNING,
        location: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
        why: Optional[str] = None,
    ):
        self.message = message
        self.severity = severity
        self.location = location  # File position, line number, etc.
        self.details = details or {}
        self.why = why  # Explanation of why this is a security concern
        self.timestamp = time.time()

    def to_dict(self) -> dict[str, Any]:
        """Convert the issue to a dictionary for serialization"""
        result = {
            "message": self.message,
            "severity": self.severity.value,
            "location": self.location,
            "details": self.details,
            "timestamp": self.timestamp,
        }
        if self.why:
            result["why"] = self.why
        return result

    def __str__(self) -> str:
        """String representation of the issue"""
        prefix = f"[{self.severity.value.upper()}]"
        if self.location:
            prefix += f" ({self.location})"
        return f"{prefix}: {self.message}"


class ScanResult:
    """Collects and manages issues found during scanning"""

    def __init__(self, scanner_name: str = "unknown"):
        self.scanner_name = scanner_name
        self.issues: list[Issue] = []
        self.start_time = time.time()
        self.end_time: Optional[float] = None
        self.bytes_scanned: int = 0
        self.success: bool = True
        self.metadata: dict[str, Any] = {}

    def add_issue(
        self,
        message: str,
        severity: IssueSeverity = IssueSeverity.WARNING,
        location: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
        why: Optional[str] = None,
    ) -> None:
        """Add an issue to the result"""
        issue = Issue(message, severity, location, details, why)
        self.issues.append(issue)
        log_level = (
            logging.CRITICAL
            if severity == IssueSeverity.CRITICAL
            else (
                logging.WARNING
                if severity == IssueSeverity.WARNING
                else (logging.INFO if severity == IssueSeverity.INFO else logging.DEBUG)
            )
        )
        logger.log(log_level, str(issue))

    def merge(self, other: "ScanResult") -> None:
        """Merge another scan result into this one"""
        self.issues.extend(other.issues)
        self.bytes_scanned += other.bytes_scanned
        # Merge metadata dictionaries
        for key, value in other.metadata.items():
            if (
                key in self.metadata
                and isinstance(self.metadata[key], dict)
                and isinstance(value, dict)
            ):
                self.metadata[key].update(value)
            else:
                self.metadata[key] = value

    def finish(self, success: bool = True) -> None:
        """Mark the scan as finished"""
        self.end_time = time.time()
        self.success = success

    @property
    def duration(self) -> float:
        """Return the duration of the scan in seconds"""
        if self.end_time is None:
            return time.time() - self.start_time
        return self.end_time - self.start_time

    @property
    def has_errors(self) -> bool:
        """Return True if there are any critical-level issues"""
        return any(issue.severity == IssueSeverity.CRITICAL for issue in self.issues)

    @property
    def has_warnings(self) -> bool:
        """Return True if there are any warning-level issues"""
        return any(issue.severity == IssueSeverity.WARNING for issue in self.issues)

    def to_dict(self) -> dict[str, Any]:
        """Convert the scan result to a dictionary for serialization"""
        return {
            "scanner": self.scanner_name,
            "success": self.success,
            "duration": self.duration,
            "bytes_scanned": self.bytes_scanned,
            "issues": [issue.to_dict() for issue in self.issues],
            "metadata": self.metadata,
            "has_errors": self.has_errors,
            "has_warnings": self.has_warnings,
        }

    def to_json(self, indent: int = 2) -> str:
        """Convert the scan result to a JSON string"""
        return json.dumps(self.to_dict(), indent=indent)

    def summary(self) -> str:
        """Return a human-readable summary of the scan result"""
        error_count = sum(
            1 for issue in self.issues if issue.severity == IssueSeverity.CRITICAL
        )
        warning_count = sum(
            1 for issue in self.issues if issue.severity == IssueSeverity.WARNING
        )
        info_count = sum(
            1 for issue in self.issues if issue.severity == IssueSeverity.INFO
        )

        result = []
        result.append(f"Scan completed in {self.duration:.2f}s")
        result.append(
            f"Scanned {self.bytes_scanned} bytes with scanner '{self.scanner_name}'",
        )
        result.append(
            f"Found {len(self.issues)} issues ({error_count} critical, "
            f"{warning_count} warnings, {info_count} info)",
        )

        # If there are any issues, show them
        if self.issues:
            result.append("\nIssues:")
            for issue in self.issues:
                result.append(f"  {issue}")

        return "\n".join(result)

    def __str__(self) -> str:
        """String representation of the scan result"""
        return self.summary()


class BaseScanner(ABC):
    """Base class for all scanners"""

    name: ClassVar[str] = "base"
    description: ClassVar[str] = "Base scanner class"
    supported_extensions: ClassVar[list[str]] = []

    def __init__(self, config: Optional[dict[str, Any]] = None):
        """Initialize the scanner with configuration"""
        self.config = config or {}
        self.timeout = self.config.get("timeout", 300)  # Default 5 minutes
        self.current_file_path = ""  # Track the current file being scanned
        self.chunk_size = self.config.get(
            "chunk_size",
            10 * 1024 * 1024,
        )  # Default: 10MB chunks

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Return True if this scanner can handle the file at the given path"""
        # Basic implementation checks file extension
        # Subclasses should override for more sophisticated detection
        file_ext = os.path.splitext(path)[1].lower()
        return file_ext in cls.supported_extensions

    @abstractmethod
    def scan(self, path: str) -> ScanResult:
        """Scan the model file or directory at the given path"""
        pass

    def _create_result(self) -> ScanResult:
        """Create a new ScanResult instance for this scanner"""
        return ScanResult(scanner_name=self.name)

    def _check_path(self, path: str) -> Optional[ScanResult]:
        """Common path checks and validation

        Returns:
            None if path is valid, otherwise a ScanResult with errors
        """
        result = self._create_result()

        # Check if path exists
        if not os.path.exists(path):
            result.add_issue(
                f"Path does not exist: {path}",
                severity=IssueSeverity.CRITICAL,
                details={"path": path},
            )
            result.finish(success=False)
            return result

        # Check if path is readable
        if not os.access(path, os.R_OK):
            result.add_issue(
                f"Path is not readable: {path}",
                severity=IssueSeverity.CRITICAL,
                details={"path": path},
            )
            result.finish(success=False)
            return result

        return None  # Path is valid

    def get_file_size(self, path: str) -> int:
        """Get the size of a file in bytes"""
        return os.path.getsize(path) if os.path.isfile(path) else 0


=====================================
FILE: ./modelaudit/scanners/gguf_scanner.py
=====================================

"""GGUF/GGML scanner that combines comprehensive parsing with security checks."""

from __future__ import annotations

import os
import struct
from typing import Any, BinaryIO, Dict, Optional

from .base import BaseScanner, IssueSeverity, ScanResult

# Map ggml_type enum to (block_size, type_size) for comprehensive validation
# Values derived from ggml source
_GGML_TYPE_INFO = {
    0: (1, 4),  # F32
    1: (1, 2),  # F16
    2: (32, 18),  # Q4_0
    3: (32, 20),  # Q4_1
    6: (32, 22),  # Q5_0
    7: (32, 24),  # Q5_1
    8: (32, 34),  # Q8_0
    9: (32, 36),  # Q8_1
    10: (256, 84),  # Q2_K
    11: (256, 110),  # Q3_K
    12: (256, 144),  # Q4_K
    13: (256, 176),  # Q5_K
    14: (256, 210),  # Q6_K
    15: (256, 292),  # Q8_K
}

# Type sizes for metadata parsing
_TYPE_SIZES = {
    0: 1,  # UINT8
    1: 1,  # INT8
    2: 2,  # UINT16
    3: 2,  # INT16
    4: 4,  # UINT32
    5: 4,  # INT32
    6: 4,  # FLOAT32
    7: 1,  # BOOL
    8: 8,  # STRING
    9: 0,  # ARRAY (variable size)
    10: 8,  # UINT64
    11: 8,  # INT64
    12: 8,  # FLOAT64
}


class GgufScanner(BaseScanner):
    """Scanner for GGUF/GGML model files with comprehensive parsing and security checks."""

    name = "gguf"
    description = (
        "Validates GGUF/GGML model file headers, metadata, and tensor integrity"
    )
    supported_extensions = [".gguf", ".ggml"]

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self.max_uncompressed = self.config.get(
            "max_uncompressed", 2 * 1024 * 1024 * 1024
        )

    @classmethod
    def can_handle(cls, path: str) -> bool:
        if not os.path.isfile(path):
            return False

        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False

        try:
            with open(path, "rb") as f:
                magic = f.read(4)
            return magic in (b"GGUF", b"GGML")
        except Exception:
            return False

    def scan(self, path: str) -> ScanResult:
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            with open(path, "rb") as f:
                magic = f.read(4)
                if magic == b"GGUF":
                    self._scan_gguf(f, file_size, result)
                elif magic == b"GGML":
                    self._scan_ggml(f, file_size, magic, result)
                else:
                    result.add_issue(
                        f"Unrecognized file format: {magic!r}",
                        IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result
        except Exception as e:
            result.add_issue(
                f"Error scanning GGUF/GGML file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(
            success=not any(i.severity == IssueSeverity.CRITICAL for i in result.issues)
        )
        return result

    def _read_string(self, f: BinaryIO, max_length: int = 1024 * 1024) -> str:
        """Read a string with length checking for security."""
        (length,) = struct.unpack("<Q", f.read(8))
        if length > max_length:
            raise ValueError(f"String length {length} exceeds maximum {max_length}")
        data = f.read(length)
        if len(data) != length:
            raise ValueError("Unexpected end of file while reading string")
        return data.decode("utf-8", "ignore")

    def _scan_gguf(self, f: BinaryIO, file_size: int, result: ScanResult) -> None:
        """Comprehensive GGUF file scanning with security checks."""
        # Read header
        version = struct.unpack("<I", f.read(4))[0]
        n_tensors = struct.unpack("<Q", f.read(8))[0]
        n_kv = struct.unpack("<Q", f.read(8))[0]

        result.metadata.update(
            {
                "format": "gguf",
                "version": version,
                "n_tensors": n_tensors,
                "n_kv": n_kv,
            }
        )

        # Security checks on header values
        if n_kv > 1_000_000:
            result.add_issue(
                f"GGUF header appears invalid (declared {n_kv} KV entries)",
                severity=IssueSeverity.CRITICAL,
            )
            return

        if n_tensors > 100_000:
            result.add_issue(
                f"GGUF header appears invalid (declared {n_tensors} tensors)",
                severity=IssueSeverity.CRITICAL,
            )
            return

        if file_size < 24:
            result.add_issue(
                "File too small to contain GGUF metadata",
                severity=IssueSeverity.CRITICAL,
            )
            return

        # Parse metadata with security checks
        metadata: Dict[str, Any] = {}
        try:
            for i in range(min(n_kv, 10000)):  # Limit to prevent DoS
                key = self._read_string(f)

                # Security check for suspicious keys
                if any(x in key for x in ("../", "..\\", "/", "\\")):
                    result.add_issue(
                        f"Suspicious metadata key with path traversal: {key}",
                        severity=IssueSeverity.WARNING,
                    )

                (value_type,) = struct.unpack("<I", f.read(4))
                value = self._read_value(f, value_type)
                metadata[key] = value

                # Security check for suspicious values
                if isinstance(value, str) and any(
                    p in value for p in ("/", "\\", ";", "&&", "|", "`")
                ):
                    result.add_issue(
                        f"Suspicious metadata value for key '{key}': {value}",
                        severity=IssueSeverity.INFO,
                    )

            result.metadata["metadata"] = metadata
        except Exception as e:
            result.add_issue(
                f"GGUF metadata parse error: {e}",
                severity=IssueSeverity.CRITICAL,
            )
            return

        # Validate alignment
        alignment = metadata.get("general.alignment", 32)
        if alignment < 8 or alignment % 8 != 0 or alignment > 1024:
            result.add_issue(
                f"Invalid alignment value: {alignment}",
                IssueSeverity.WARNING,
            )

        # Align to tensor data
        current = f.tell()
        pad = (alignment - (current % alignment)) % alignment
        if pad:
            f.seek(pad, os.SEEK_CUR)

        # Parse tensor information
        tensors = []
        try:
            for i in range(min(n_tensors, 10000)):  # Limit to prevent DoS
                t_name = self._read_string(f)
                (nd,) = struct.unpack("<I", f.read(4))

                # Hard limit on dimensions to prevent DoS attacks
                if nd > 1000:  # Extremely large dimension count - skip this tensor
                    result.add_issue(
                        f"Tensor {t_name} has excessive dimensions ({nd}), skipping for security",
                        IssueSeverity.CRITICAL,
                    )
                    # Skip the rest of this tensor's data to prevent DoS
                    f.seek(nd * 8 + 4 + 8, os.SEEK_CUR)  # Skip dims + type + offset
                    continue

                if nd > 8:  # Reasonable limit for tensor dimensions
                    result.add_issue(
                        f"Tensor {t_name} has suspicious number of dimensions: {nd}",
                        IssueSeverity.WARNING,
                    )

                dims = [struct.unpack("<Q", f.read(8))[0] for _ in range(nd)]
                (t_type,) = struct.unpack("<I", f.read(4))
                (offset,) = struct.unpack("<Q", f.read(8))

                tensors.append(
                    {
                        "name": t_name,
                        "dims": dims,
                        "type": t_type,
                        "offset": offset,
                    }
                )

            result.metadata["tensors"] = [
                {"name": t["name"], "type": t["type"], "dims": t["dims"]}
                for t in tensors
            ]
        except Exception as e:
            result.add_issue(
                f"GGUF tensor parse error: {e}",
                severity=IssueSeverity.CRITICAL,
            )
            return

        # Validate tensor sizes and offsets
        for idx, tensor in enumerate(tensors):
            try:
                nelements = 1
                has_invalid_dimension = False
                for d in tensor["dims"]:
                    if d <= 0 or d > 2**31:
                        result.add_issue(
                            f"Tensor {tensor['name']} has invalid dimension: {d}",
                            IssueSeverity.WARNING,
                        )
                        has_invalid_dimension = True
                        break
                    nelements *= d

                # Skip tensor validation if any dimension is invalid
                if has_invalid_dimension:
                    continue

                # Check for extremely large tensors using correct size calculation
                # For quantized types, use the actual type information
                info = _GGML_TYPE_INFO.get(tensor["type"])
                if info:
                    # For quantized types, calculate based on block and type size
                    blck, ts = info
                    estimated_size = ((nelements + blck - 1) // blck) * ts
                else:
                    # Fallback for unknown types - assume 4 bytes per element
                    estimated_size = nelements * 4

                if estimated_size > self.max_uncompressed:
                    result.add_issue(
                        f"Tensor {tensor['name']} estimated size ({estimated_size}) exceeds limit",
                        IssueSeverity.CRITICAL,
                    )

                # Validate tensor type and size
                info = _GGML_TYPE_INFO.get(tensor["type"])
                if info:
                    blck, ts = info
                    if nelements % blck != 0:
                        result.add_issue(
                            f"Tensor {tensor['name']} not aligned to block size {blck}",
                            IssueSeverity.WARNING,
                        )

                    expected = ((nelements + blck - 1) // blck) * ts
                    next_offset = (
                        tensors[idx + 1]["offset"]
                        if idx + 1 < len(tensors)
                        else file_size
                    )
                    actual = next_offset - tensor["offset"]

                    if expected != actual:
                        result.add_issue(
                            f"Size mismatch for tensor {tensor['name']}",
                            IssueSeverity.CRITICAL,
                            details={"expected": expected, "actual": actual},
                        )
            except (OverflowError, ValueError) as e:
                result.add_issue(
                    f"Error validating tensor {tensor['name']}: {e}",
                    IssueSeverity.WARNING,
                )

        result.bytes_scanned = f.tell()

    def _scan_ggml(
        self, f: BinaryIO, file_size: int, magic: bytes, result: ScanResult
    ) -> None:
        """Basic GGML file validation with security checks."""
        result.metadata["format"] = "ggml"
        result.metadata["magic"] = magic.decode("ascii", "ignore")

        if file_size < 32:
            result.add_issue(
                "File too small to be valid GGML",
                severity=IssueSeverity.CRITICAL,
            )
            return

        # Basic heuristic validation
        try:
            version_bytes = f.read(4)
            if len(version_bytes) < 4:
                result.add_issue(
                    "Truncated GGML header",
                    severity=IssueSeverity.CRITICAL,
                )
                return

            version = struct.unpack("<I", version_bytes)[0]
            result.metadata["version"] = version

            if version > 10000:  # Reasonable upper bound
                result.add_issue(
                    f"Suspicious GGML version: {version}",
                    severity=IssueSeverity.WARNING,
                )
        except Exception as e:
            result.add_issue(
                f"Error parsing GGML header: {e}",
                severity=IssueSeverity.CRITICAL,
            )

        result.bytes_scanned = file_size

    def _read_value(self, f: BinaryIO, vtype: int) -> Any:
        """Read a value of the specified type with security checks."""
        if vtype == 0:  # UINT8
            return struct.unpack("<B", f.read(1))[0]
        elif vtype == 1:  # INT8
            return struct.unpack("<b", f.read(1))[0]
        elif vtype == 2:  # UINT16
            return struct.unpack("<H", f.read(2))[0]
        elif vtype == 3:  # INT16
            return struct.unpack("<h", f.read(2))[0]
        elif vtype == 4:  # UINT32
            return struct.unpack("<I", f.read(4))[0]
        elif vtype == 5:  # INT32
            return struct.unpack("<i", f.read(4))[0]
        elif vtype == 6:  # FLOAT32
            return struct.unpack("<f", f.read(4))[0]
        elif vtype == 7:  # BOOL
            return struct.unpack("<B", f.read(1))[0] != 0
        elif vtype == 8:  # STRING
            return self._read_string(f)
        elif vtype == 9:  # ARRAY
            subtype = struct.unpack("<I", f.read(4))[0]
            (count,) = struct.unpack("<Q", f.read(8))
            if count > 10000:  # Prevent DoS
                raise ValueError(f"Array too large: {count} elements")
            return [self._read_value(f, subtype) for _ in range(count)]
        elif vtype == 10:  # UINT64
            return struct.unpack("<Q", f.read(8))[0]
        elif vtype == 11:  # INT64
            return struct.unpack("<q", f.read(8))[0]
        elif vtype == 12:  # FLOAT64
            return struct.unpack("<d", f.read(8))[0]
        else:
            raise ValueError(f"Unknown metadata type {vtype}")


=====================================
FILE: ./modelaudit/scanners/joblib_scanner.py
=====================================

from __future__ import annotations

import io
import lzma
import os
import zlib
from typing import Any, Optional

from ..utils.filetype import read_magic_bytes
from .base import BaseScanner, IssueSeverity, ScanResult
from .pickle_scanner import PickleScanner


class JoblibScanner(BaseScanner):
    """Scanner for joblib serialized files."""

    name = "joblib"
    description = "Scans joblib files by decompressing and analyzing embedded pickle"
    supported_extensions = [".joblib"]

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        self.pickle_scanner = PickleScanner(config)
        # Security limits
        self.max_decompression_ratio = self.config.get("max_decompression_ratio", 100.0)
        self.max_decompressed_size = self.config.get(
            "max_decompressed_size", 100 * 1024 * 1024
        )  # 100MB
        self.max_file_read_size = self.config.get(
            "max_file_read_size", 100 * 1024 * 1024
        )  # 100MB
        self.chunk_size = self.config.get("chunk_size", 8192)  # 8KB chunks

    @classmethod
    def can_handle(cls, path: str) -> bool:
        if not os.path.isfile(path):
            return False
        ext = os.path.splitext(path)[1].lower()
        if ext != ".joblib":
            return False
        return True

    def _read_file_safely(self, path: str) -> bytes:
        """Read file in chunks with size validation"""
        data = b""
        file_size = self.get_file_size(path)

        if file_size > self.max_file_read_size:
            raise ValueError(
                f"File too large: {file_size} bytes (max: {self.max_file_read_size})"
            )

        with open(path, "rb") as f:
            while True:
                chunk = f.read(self.chunk_size)
                if not chunk:
                    break
                data += chunk
                if len(data) > self.max_file_read_size:
                    raise ValueError(f"File read exceeds limit: {len(data)} bytes")
        return data

    def _safe_decompress(self, data: bytes) -> bytes:
        """Safely decompress data with bomb protection"""
        compressed_size = len(data)

        # Try zlib first
        decompressed = None
        try:
            decompressed = zlib.decompress(data)
        except Exception:
            # Try lzma
            try:
                decompressed = lzma.decompress(data)
            except Exception as e:
                raise ValueError(f"Unable to decompress joblib file: {e}")

        # Check decompression ratio for compression bomb detection
        if compressed_size > 0:
            ratio = len(decompressed) / compressed_size
            if ratio > self.max_decompression_ratio:
                raise ValueError(
                    f"Suspicious compression ratio: {ratio:.1f}x "
                    f"(max: {self.max_decompression_ratio}x) - possible compression bomb"
                )

        # Check absolute decompressed size
        if len(decompressed) > self.max_decompressed_size:
            raise ValueError(
                f"Decompressed size too large: {len(decompressed)} bytes "
                f"(max: {self.max_decompressed_size})"
            )

        return decompressed

    def scan(self, path: str) -> ScanResult:
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            self.current_file_path = path
            magic = read_magic_bytes(path, 4)
            data = self._read_file_safely(path)

            if magic.startswith(b"PK"):
                # Treat as zip archive
                from .zip_scanner import ZipScanner

                zip_scanner = ZipScanner(self.config)
                sub_result = zip_scanner.scan(path)
                result.merge(sub_result)
                result.bytes_scanned = sub_result.bytes_scanned
                result.metadata.update(sub_result.metadata)
                result.finish(success=sub_result.success)
                return result

            if magic.startswith(b"\x80"):
                file_like = io.BytesIO(data)
                sub_result = self.pickle_scanner._scan_pickle_bytes(
                    file_like, len(data)
                )
                result.merge(sub_result)
                result.bytes_scanned = len(data)
            else:
                # Try safe decompression
                try:
                    decompressed = self._safe_decompress(data)
                except ValueError as e:
                    result.add_issue(
                        str(e),
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                        details={"security_check": "compression_bomb_detection"},
                    )
                    result.finish(success=False)
                    return result
                except Exception as e:
                    result.add_issue(
                        f"Error decompressing joblib file: {e}",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result
                file_like = io.BytesIO(decompressed)
                sub_result = self.pickle_scanner._scan_pickle_bytes(
                    file_like, len(decompressed)
                )
                result.merge(sub_result)
                result.bytes_scanned = len(decompressed)
        except Exception as e:  # pragma: no cover
            result.add_issue(
                f"Error scanning joblib file: {e}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result


=====================================
FILE: ./modelaudit/scanners/keras_h5_scanner.py
=====================================

import json
import os
from typing import Any, Optional

from modelaudit.suspicious_symbols import (
    SUSPICIOUS_CONFIG_PROPERTIES,
    SUSPICIOUS_LAYER_TYPES,
)

from ..explanations import get_pattern_explanation
from .base import BaseScanner, IssueSeverity, ScanResult

# Try to import h5py, but handle the case where it's not installed
try:
    import h5py

    HAS_H5PY = True
except ImportError:
    HAS_H5PY = False


class KerasH5Scanner(BaseScanner):
    """Scanner for Keras H5 model files"""

    name = "keras_h5"
    description = "Scans Keras H5 model files for suspicious layer configurations"
    supported_extensions = [".h5", ".hdf5", ".keras"]

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        # Additional scanner-specific configuration
        self.suspicious_layer_types = dict(SUSPICIOUS_LAYER_TYPES)
        if config and "suspicious_layer_types" in config:
            self.suspicious_layer_types.update(config["suspicious_layer_types"])

        self.suspicious_config_props = list(SUSPICIOUS_CONFIG_PROPERTIES)
        if config and "suspicious_config_properties" in config:
            self.suspicious_config_props.extend(config["suspicious_config_properties"])

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not HAS_H5PY:
            return False

        if not os.path.isfile(path):
            return False

        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False

        # Try to open as HDF5 file
        try:
            with h5py.File(path, "r") as _:
                return True
        except Exception:
            return False

    def scan(self, path: str) -> ScanResult:
        """Scan a Keras model file for suspicious configurations"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        # Check if h5py is installed
        if not HAS_H5PY:
            result = self._create_result()
            result.add_issue(
                "h5py not installed, cannot scan Keras H5 files. Install with "
                "'pip install modelaudit[h5]'.",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"path": path},
            )
            result.finish(success=False)
            return result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            # Store the file path for use in issue locations
            self.current_file_path = path

            with h5py.File(path, "r") as f:
                result.bytes_scanned = file_size

                # Check if this is a Keras model file
                if "model_config" not in f.attrs:
                    # Check if this might be a TensorFlow SavedModel H5 file instead
                    # Look for common TensorFlow H5 structure patterns
                    is_tensorflow_h5 = any(
                        key.startswith(
                            ("model_weights", "optimizer_weights", "variables")
                        )
                        for key in f.keys()
                    )

                    if is_tensorflow_h5:
                        result.add_issue(
                            "File appears to be a TensorFlow H5 model, not Keras format "
                            "(no model_config attribute)",
                            severity=IssueSeverity.DEBUG,  # Reduced severity - this is expected
                            location=self.current_file_path,
                        )
                    else:
                        result.add_issue(
                            "File does not appear to be a Keras model "
                            "(no model_config attribute)",
                            severity=IssueSeverity.DEBUG,  # Reduced severity - not necessarily suspicious
                            location=self.current_file_path,
                        )
                    result.finish(success=True)  # Still success, just not a Keras file
                    return result

                # Parse model config
                model_config_str = f.attrs["model_config"]
                model_config = json.loads(model_config_str)

                # Scan model configuration
                self._scan_model_config(model_config, result)

                # Check for custom objects in the model
                if "custom_objects" in f.attrs:
                    result.add_issue(
                        "Model contains custom objects which could contain "
                        "arbitrary code",
                        severity=IssueSeverity.WARNING,
                        location=f"{self.current_file_path} (model_config)",
                        details={"custom_objects": list(f.attrs["custom_objects"])},
                    )

                # Check for custom metrics
                if "training_config" in f.attrs:
                    training_config = json.loads(f.attrs["training_config"])
                    if "metrics" in training_config:
                        for metric in training_config["metrics"]:
                            if isinstance(metric, dict) and metric.get(
                                "class_name",
                            ) not in [
                                "Accuracy",
                                "CategoricalAccuracy",
                                "BinaryAccuracy",
                            ]:
                                result.add_issue(
                                    f"Model contains custom metric: "
                                    f"{metric.get('class_name', 'unknown')}",
                                    severity=IssueSeverity.WARNING,
                                    location=f"{self.current_file_path} (metrics)",
                                    details={"metric": metric},
                                )

        except Exception as e:
            result.add_issue(
                f"Error scanning Keras H5 file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _scan_model_config(
        self,
        model_config: dict[str, Any],
        result: ScanResult,
    ) -> None:
        """Scan the model configuration for suspicious elements"""
        if not isinstance(model_config, dict):
            result.add_issue(
                "Invalid model configuration format",
                severity=IssueSeverity.WARNING,
                location=self.current_file_path,
            )
            return

        # Check model class name
        model_class = model_config.get("class_name", "")
        result.metadata["model_class"] = model_class

        # Collect all layers
        layers = []
        if "config" in model_config and "layers" in model_config["config"]:
            layers = model_config["config"]["layers"]

        # Count of each layer type
        layer_counts: dict[str, int] = {}

        # Check each layer
        for layer in layers:
            layer_class = layer.get("class_name", "")

            # Update layer count
            if layer_class in layer_counts:
                layer_counts[layer_class] += 1
            else:
                layer_counts[layer_class] = 1

            # Check for suspicious layer types
            if layer_class in self.suspicious_layer_types:
                result.add_issue(
                    f"Suspicious layer type found: {layer_class}",
                    severity=IssueSeverity.CRITICAL,
                    location=self.current_file_path,
                    details={
                        "layer_class": layer_class,
                        "description": self.suspicious_layer_types[layer_class],
                        "layer_config": layer.get("config", {}),
                    },
                    why=get_pattern_explanation("lambda_layer")
                    if layer_class == "Lambda"
                    else None,
                )

            # Check layer configuration for suspicious strings
            self._check_config_for_suspicious_strings(
                layer.get("config", {}),
                result,
                layer_class,
            )

            # If there are nested models, scan them recursively
            if (
                layer_class == "Model"
                and "config" in layer
                and "layers" in layer["config"]
            ):
                self._scan_model_config(layer, result)

        # Add layer counts to metadata
        result.metadata["layer_counts"] = layer_counts

    def _check_config_for_suspicious_strings(
        self,
        config: dict[str, Any],
        result: ScanResult,
        context: str = "",
    ) -> None:
        """Recursively check a configuration dictionary for suspicious strings"""
        if not isinstance(config, dict):
            return

        # Check all string values in the config
        for key, value in config.items():
            if isinstance(value, str):
                # Check for suspicious strings
                for suspicious_term in self.suspicious_config_props:
                    if suspicious_term in value.lower():
                        result.add_issue(
                            f"Suspicious configuration string found in {context}: "
                            f"'{suspicious_term}'",
                            severity=IssueSeverity.WARNING,
                            location=f"{self.current_file_path} ({context})",
                            details={
                                "suspicious_term": suspicious_term,
                                "context": context,
                            },
                        )
            elif isinstance(value, dict):
                # Recursively check nested dictionaries
                self._check_config_for_suspicious_strings(
                    value,
                    result,
                    f"{context}.{key}",
                )
            elif isinstance(value, list):
                # Check each item in the list
                for i, item in enumerate(value):
                    if isinstance(item, dict):
                        self._check_config_for_suspicious_strings(
                            item,
                            result,
                            f"{context}.{key}[{i}]",
                        )


=====================================
FILE: ./modelaudit/scanners/manifest_scanner.py
=====================================

import json
import os
from typing import Any, Optional

from modelaudit.suspicious_symbols import SUSPICIOUS_CONFIG_PATTERNS

from .base import BaseScanner, IssueSeverity, ScanResult, logger

# Try to import the name policies module
try:
    from modelaudit.name_policies.blacklist import check_model_name_policies

    HAS_NAME_POLICIES = True
except ImportError:
    HAS_NAME_POLICIES = False

    # Create a placeholder function when the module is not available
    def check_model_name_policies(
        model_name: str,
        additional_patterns: Optional[list[str]] = None,
    ) -> tuple[bool, str]:
        return False, ""


# Try to import yaml, but handle the case where it's not installed
try:
    import yaml

    HAS_YAML = True
except ImportError:
    HAS_YAML = False

# Common manifest and config file formats
MANIFEST_EXTENSIONS = [
    ".json",
    ".yaml",
    ".yml",
    ".xml",
    ".toml",
    ".ini",
    ".cfg",
    ".config",
    ".manifest",
    ".model",
    ".metadata",
]

# Keys that might contain model names
MODEL_NAME_KEYS = [
    "name",
    "model_name",
    "model",
    "model_id",
    "id",
    "title",
    "artifact_name",
    "artifact_id",
    "package_name",
]

# Pre-compute lowercase versions for faster checks
MODEL_NAME_KEYS_LOWER = [key.lower() for key in MODEL_NAME_KEYS]


class ManifestScanner(BaseScanner):
    """Scanner for model manifest and configuration files"""

    name = "manifest"
    description = (
        "Scans model manifest and configuration files for suspicious content "
        "and blacklisted names"
    )
    supported_extensions = MANIFEST_EXTENSIONS

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        # Get blacklist patterns from config
        self.blacklist_patterns = self.config.get("blacklist_patterns", [])

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not os.path.isfile(path):
            return False

        filename = os.path.basename(path).lower()

        # Whitelist: Only scan files that are unique to AI/ML models
        aiml_specific_patterns = [
            # HuggingFace/Transformers specific configuration files
            "config.json",  # Model architecture config (when in ML model context)
            "generation_config.json",  # Text generation parameters
            "preprocessor_config.json",  # Data preprocessing config
            "feature_extractor_config.json",  # Feature extraction config
            "image_processor_config.json",  # Image processing config
            "scheduler_config.json",  # Learning rate scheduler config
            # Model metadata and manifest files specific to ML
            "model_index.json",  # Diffusion model index
            "model_card.json",  # Model card metadata
            "pytorch_model.bin.index.json",  # PyTorch model shard index
            "model.safetensors.index.json",  # SafeTensors model index
            "tf_model.h5.index.json",  # TensorFlow model index
            # ML-specific execution and deployment configs
            "inference_config.json",  # Model inference configuration
            "deployment_config.json",  # Model deployment configuration
            "serving_config.json",  # Model serving configuration
            # ONNX model specific
            "onnx_config.json",  # ONNX export configuration
            # Custom model configs that might contain execution parameters
            "custom_config.json",  # Custom model configurations
            "runtime_config.json",  # Runtime execution parameters
        ]

        # Check if filename matches any AI/ML specific pattern
        if any(pattern in filename for pattern in aiml_specific_patterns):
            return True

        # Additional check: files with "config" in name that are in ML model context
        # (but exclude tokenizer configs and general software configs)
        if (
            "config" in filename
            and "tokenizer" not in filename
            and filename
            not in [
                "config.py",
                "config.yaml",
                "config.yml",
                "config.ini",
                "config.cfg",
            ]
        ):
            # Only if it's likely an ML model config
            # (has model-related terms in path or specific extensions)
            path_lower = path.lower()
            if any(
                ml_term in path_lower
                for ml_term in ["model", "checkpoint", "huggingface", "transformers"]
            ) or os.path.splitext(path)[1].lower() in [".json"]:
                return True

        return False

    def scan(self, path: str) -> ScanResult:
        """Scan a manifest or configuration file"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            # Store the file path for use in issue locations
            self.current_file_path = path

            # First, check the raw file content for blacklisted terms
            self._check_file_for_blacklist(path, result)

            # Parse the file based on its extension
            ext = os.path.splitext(path)[1].lower()
            content = self._parse_file(path, ext, result)

            if content:
                result.bytes_scanned = file_size

                # Check for suspicious configuration patterns
                self._check_suspicious_patterns(content, result)

            else:
                result.add_issue(
                    f"Unable to parse file as a manifest or configuration: {path}",
                    severity=IssueSeverity.DEBUG,
                    location=path,
                )

        except Exception as e:
            result.add_issue(
                f"Error scanning manifest file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _check_file_for_blacklist(self, path: str, result: ScanResult) -> None:
        """Check the entire file content for blacklisted terms"""
        if not self.blacklist_patterns:
            return

        try:
            with open(path, encoding="utf-8") as f:
                content = (
                    f.read().lower()
                )  # Convert to lowercase for case-insensitive matching

                for pattern in self.blacklist_patterns:
                    pattern_lower = pattern.lower()
                    if pattern_lower in content:
                        result.add_issue(
                            f"Blacklisted term '{pattern}' found in file",
                            severity=IssueSeverity.CRITICAL,
                            location=self.current_file_path,
                            details={"blacklisted_term": pattern, "file_path": path},
                            why="This term matches a user-defined blacklist pattern. Organizations use blacklists to identify models or configurations that violate security policies or contain known malicious indicators.",
                        )
        except Exception as e:
            result.add_issue(
                f"Error checking file for blacklist: {str(e)}",
                severity=IssueSeverity.WARNING,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )

    def _parse_file(
        self, path: str, ext: str, result: Optional[ScanResult] = None
    ) -> Optional[dict[str, Any]]:
        """Parse the file based on its extension"""
        try:
            with open(path, encoding="utf-8") as f:
                content = f.read()

                # Try JSON format first
                if ext in [
                    ".json",
                    ".manifest",
                    ".model",
                    ".metadata",
                ] or content.strip().startswith(("{", "[")):
                    return json.loads(content)

                # Try YAML format if available
                if HAS_YAML and (
                    ext in [".yaml", ".yml"] or content.strip().startswith("---")
                ):
                    return yaml.safe_load(content)

                # For other formats, try JSON and then YAML if available
                try:
                    return json.loads(content)
                except json.JSONDecodeError:
                    if HAS_YAML:
                        try:
                            return yaml.safe_load(content)
                        except Exception:
                            pass

        except Exception as e:
            # Log the error but don't raise, as we want to continue scanning
            logger.warning(f"Error parsing file {path}: {str(e)}")
            if result is not None:
                result.add_issue(
                    f"Error parsing file: {path}",
                    severity=IssueSeverity.DEBUG,
                    location=path,
                    details={"exception": str(e), "exception_type": type(e).__name__},
                )

        return None

    def _check_suspicious_patterns(
        self,
        content: dict[str, Any],
        result: ScanResult,
    ) -> None:
        """Smart pattern detection with value analysis and ML context awareness"""

        # STEP 1: Detect ML context for smart filtering
        ml_context = self._detect_ml_context(content)

        def check_dict(d, prefix=""):
            if not isinstance(d, dict):
                return

            for key, value in d.items():
                key_lower = key.lower()
                full_key = f"{prefix}.{key}" if prefix else key

                # STEP 1.5: Check for blacklisted model names (integrated from original)
                if key_lower in MODEL_NAME_KEYS_LOWER:
                    blocked, reason = check_model_name_policies(
                        str(value), self.blacklist_patterns
                    )
                    if blocked:
                        result.add_issue(
                            f"Model name blocked by policy: {value}",
                            severity=IssueSeverity.CRITICAL,
                            location=self.current_file_path,
                            details={
                                "model_name": str(value),
                                "reason": reason,
                                "key": full_key,
                            },
                        )

                # STEP 2: Value-based analysis - check for actually dangerous content
                if self._is_actually_dangerous_value(key, value):
                    result.add_issue(
                        f"Dangerous configuration content: {full_key}",
                        severity=IssueSeverity.CRITICAL,
                        location=self.current_file_path,
                        details={
                            "key": full_key,
                            "analysis": "value_based",
                            "danger": "executable_content",
                            "value": self._format_value(value),
                        },
                    )
                    # Don't continue here - still check for patterns and recurse

                # STEP 3: Smart pattern matching
                matches = self._find_suspicious_matches(key_lower)
                if matches:
                    # STEP 4: Context-aware filtering
                    if not self._should_ignore_in_context(
                        key, value, matches, ml_context
                    ):
                        # STEP 5: Report with context-aware severity
                        severity = self._get_context_aware_severity(matches, ml_context)
                        why = None
                        if severity == IssueSeverity.INFO:
                            if "file_access" in matches and "network_access" in matches:
                                why = "File and network access patterns in ML model configurations are common for loading datasets and downloading resources. They are flagged for awareness but are typically benign in ML contexts."
                            elif "file_access" in matches:
                                why = "File access patterns in ML model configurations often indicate dataset paths or model checkpoints. This is flagged for awareness but is typical in ML workflows."
                            elif "network_access" in matches:
                                why = "Network access patterns in ML model configurations may indicate remote model repositories or dataset URLs. This is common in ML pipelines but worth reviewing."

                        result.add_issue(
                            f"Suspicious configuration pattern: {full_key} "
                            f"(category: {', '.join(matches)})",
                            severity=severity,
                            location=self.current_file_path,
                            details={
                                "key": full_key,
                                "value": self._format_value(value),
                                "categories": matches,
                                "ml_context": ml_context,
                                "analysis": "pattern_based",
                            },
                            why=why,
                        )

                # ALWAYS recursively check nested structures,
                # regardless of pattern matches
                if isinstance(value, dict):
                    check_dict(value, full_key)
                elif isinstance(value, list):
                    for i, item in enumerate(value):
                        if isinstance(item, dict):
                            check_dict(item, f"{full_key}[{i}]")

        check_dict(content)

    def _is_actually_dangerous_value(self, key: str, value: Any) -> bool:
        """Check if value content is actually dangerous executable code"""
        if not isinstance(value, str):
            return False

        value_lower = value.lower().strip()

        # Look for ACTUAL executable content patterns
        dangerous_patterns = [
            "import os",
            "subprocess.",
            "eval(",
            "exec(",
            "os.system",
            "__import__",
            "runpy",
            "shell=true",
            "rm -rf",
            "/bin/sh",
            "cmd.exe",
            # Add more specific patterns
            "exec('",
            'exec("',
            "eval('",
            'eval("',
        ]

        return any(pattern in value_lower for pattern in dangerous_patterns)

    def _detect_ml_context(self, content: dict[str, Any]) -> dict[str, Any]:
        """Detect ML model context to adjust sensitivity"""
        indicators = {
            "framework": None,
            "model_type": None,
            "confidence": 0,
            "is_tokenizer": False,
            "is_model_config": False,
        }

        # Framework detection patterns
        framework_patterns = {
            "huggingface": [
                "tokenizer_class",
                "transformers_version",
                "model_type",
                "architectures",
                "auto_map",
                "_name_or_path",
            ],
            "pytorch": [
                "torch",
                "state_dict",
                "pytorch_model",
                "model.pt",
                "torch_dtype",
            ],
            "tensorflow": [
                "tensorflow",
                "saved_model",
                "model.h5",
                "tf_version",
                "keras",
            ],
            "sklearn": ["sklearn", "pickle_module", "scikit"],
        }

        # Model type indicators
        tokenizer_indicators = [
            "tokenizer_class",
            "added_tokens_decoder",
            "model_input_names",
            "special_tokens_map",
            "bos_token",
            "eos_token",
            "pad_token",
        ]

        model_config_indicators = [
            "hidden_size",
            "num_attention_heads",
            "num_hidden_layers",
            "vocab_size",
            "max_position_embeddings",
            "architectures",
        ]

        def check_indicators(d):
            if not isinstance(d, dict):
                return

            for key, val in d.items():
                key_str = str(key).lower()
                val_str = str(val).lower()

                # Check framework patterns
                for framework, patterns in framework_patterns.items():
                    if any(
                        pattern in key_str or pattern in val_str for pattern in patterns
                    ):
                        indicators["framework"] = framework
                        indicators["confidence"] += 1

                # Check tokenizer indicators
                if any(indicator in key_str for indicator in tokenizer_indicators):
                    indicators["is_tokenizer"] = True
                    indicators["confidence"] += 1

                # Check model config indicators
                if any(indicator in key_str for indicator in model_config_indicators):
                    indicators["is_model_config"] = True
                    indicators["confidence"] += 1

                # Recursive check
                if isinstance(val, dict):
                    check_indicators(val)

        check_indicators(content)
        return indicators

    def _find_suspicious_matches(self, key_lower: str) -> list[str]:
        """Find all categories that match this key"""
        matches = []
        for category, patterns in SUSPICIOUS_CONFIG_PATTERNS.items():
            if any(pattern in key_lower for pattern in patterns):
                matches.append(category)
        return matches

    def _should_ignore_in_context(
        self, key: str, value: Any, matches: list[str], ml_context: dict
    ) -> bool:
        """Context-aware ignore logic combining smart patterns with value analysis"""
        key_lower = key.lower()

        # Special case for HuggingFace patterns - check this FIRST before other logic
        if ml_context.get("framework") == "huggingface" or "_name_or_path" in key_lower:
            huggingface_safe_patterns = [
                "_name_or_path",
                "name_or_path",
                "model_input_names",
                "model_output_names",
                "transformers_version",
                "torch_dtype",
                "architectures",
            ]
            if any(pattern in key_lower for pattern in huggingface_safe_patterns):
                return True

        # High-confidence ML context gets more lenient treatment
        if ml_context.get("confidence", 0) >= 2:
            # File access patterns in ML context
            if "file_access" in matches:
                # First check if this is an actual file path - never ignore those
                if key_lower.endswith(
                    ("_dir", "_path", "_file")
                ) and self._is_file_path_value(value):
                    return False  # Don't ignore actual file paths

                # Common ML config patterns that aren't actual file access
                safe_ml_patterns = [
                    "_input",
                    "input_",
                    "_output",
                    "output_",
                    "_size",
                    "_dim",
                    "hidden_",
                    "attention_",
                    "embedding_",
                    "_token_",
                    "vocab_",
                    "_names",
                    "model_input_names",
                    "model_output_names",
                ]

                if any(pattern in key_lower for pattern in safe_ml_patterns):
                    return True

            # Credentials in ML context
            if "credentials" in matches:
                # Token IDs and model tokens are not credentials in ML context
                if any(
                    pattern in key_lower
                    for pattern in ["_token_id", "token_id_", "_token", "token_type"]
                ):
                    return True

        # Special case for tokenizer configs
        if ml_context.get("is_tokenizer"):
            tokenizer_safe_keys = [
                "added_tokens_decoder",
                "model_input_names",
                "special_tokens_map",
                "tokenizer_class",
                "model_max_length",
            ]
            if any(safe_key in key_lower for safe_key in tokenizer_safe_keys):
                return True

        return False

    def _is_file_path_value(self, value: Any) -> bool:
        """Check if value appears to be an actual file system path"""
        if not isinstance(value, str):
            return False

        # Absolute paths
        if value.startswith(("/", "\\", "C:", "D:")):
            return True

        # Relative paths with separators
        if "/" in value or "\\" in value:
            return True

        # File extensions that suggest actual files (using endswith for better matching)
        file_extensions = [
            ".json",
            ".h5",
            ".pt",
            ".onnx",
            ".pkl",
            ".model",
            ".txt",
            ".log",
            ".csv",
            ".xml",
            ".yaml",
            ".yml",
            ".py",
            ".js",
            ".html",
            ".css",
            ".sql",
            ".md",
        ]
        if any(value.lower().endswith(ext) for ext in file_extensions):
            return True

        # Common path indicators
        if any(
            indicator in value.lower()
            for indicator in ["/tmp", "/var", "/data", "/home", "/etc", "c:\\", "d:\\"]
        ):
            return True

        return False

    def _get_context_aware_severity(
        self, matches: list[str], ml_context: dict
    ) -> IssueSeverity:
        """Determine severity based on context and match types"""
        # Execution patterns are always ERROR
        if "execution" in matches:
            return IssueSeverity.CRITICAL

        # In high-confidence ML context, downgrade some warnings
        if ml_context.get("confidence", 0) >= 2:
            # In ML context, file_access and network_access are less concerning
            if all(match in ["file_access", "network_access"] for match in matches):
                return IssueSeverity.INFO

        # Credentials are high priority
        if "credentials" in matches:
            return IssueSeverity.WARNING

        return IssueSeverity.WARNING

    def _format_value(self, value: Any) -> str:
        """Format a value for display, truncating if necessary"""
        str_value = str(value)
        if len(str_value) > 100:
            return str_value[:100] + "..."
        return str_value


=====================================
FILE: ./modelaudit/scanners/numpy_scanner.py
=====================================

from __future__ import annotations

import sys

import numpy.lib.format as fmt

from .base import BaseScanner, IssueSeverity, ScanResult


class NumPyScanner(BaseScanner):
    """Scanner for NumPy binary files (.npy)."""

    name = "numpy"
    description = "Scans NumPy .npy files for integrity issues"
    supported_extensions = [".npy"]

    def __init__(self, config=None):
        super().__init__(config)
        # Security limits
        self.max_array_bytes = self.config.get(
            "max_array_bytes", 1024 * 1024 * 1024
        )  # 1GB
        self.max_dimensions = self.config.get("max_dimensions", 32)
        self.max_dimension_size = self.config.get("max_dimension_size", 100_000_000)
        self.max_itemsize = self.config.get("max_itemsize", 1024)  # 1KB per element

    def _validate_array_dimensions(self, shape: tuple) -> None:
        """Validate array dimensions for security"""
        # Check number of dimensions
        if len(shape) > self.max_dimensions:
            raise ValueError(
                f"Too many dimensions: {len(shape)} (max: {self.max_dimensions})"
            )

        # Check individual dimension sizes
        for i, dim in enumerate(shape):
            if dim < 0:
                raise ValueError(f"Negative dimension at index {i}: {dim}")
            if dim > self.max_dimension_size:
                raise ValueError(
                    f"Dimension {i} too large: {dim} (max: {self.max_dimension_size})"
                )

    def _validate_dtype(self, dtype) -> None:
        """Validate numpy dtype for security"""
        # Check for problematic data types
        dangerous_names = ["object"]
        dangerous_kinds = ["O", "V"]  # Object and Void kinds

        if dtype.name in dangerous_names or dtype.kind in dangerous_kinds:
            raise ValueError(
                f"Dangerous dtype not allowed: {dtype.name} (kind: {dtype.kind})"
            )

        # Check for extremely large item sizes
        if dtype.itemsize > self.max_itemsize:
            raise ValueError(
                f"Itemsize too large: {dtype.itemsize} bytes (max: {self.max_itemsize})"
            )

    def _calculate_safe_array_size(self, shape: tuple, dtype) -> int:
        """Calculate array size with overflow protection"""
        total_elements = 1
        max_elements = sys.maxsize // max(dtype.itemsize, 1)

        for dim in shape:
            # Check for overflow before multiplication
            if total_elements > max_elements // max(dim, 1):
                raise ValueError(
                    f"Array size would overflow: shape={shape}, dtype={dtype}"
                )

            total_elements *= dim

        total_bytes = total_elements * dtype.itemsize

        if total_bytes > self.max_array_bytes:
            raise ValueError(
                f"Array too large: {total_bytes} bytes "
                f"(max: {self.max_array_bytes}) for shape={shape}, dtype={dtype}"
            )

        return total_bytes

    def scan(self, path: str) -> ScanResult:
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            self.current_file_path = path
            with open(path, "rb") as f:
                # Verify magic string
                magic = f.read(6)
                if magic != b"\x93NUMPY":
                    result.add_issue(
                        "Invalid NumPy file magic",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result
                f.seek(0)
                major, minor = fmt.read_magic(f)
                if (major, minor) == (1, 0):
                    shape, fortran, dtype = fmt.read_array_header_1_0(f)
                elif (major, minor) == (2, 0):
                    shape, fortran, dtype = fmt.read_array_header_2_0(f)
                else:
                    shape, fortran, dtype = fmt._read_array_header(  # type: ignore[attr-defined]
                        f, version=(major, minor)
                    )
                data_offset = f.tell()

                # Validate array dimensions and dtype for security
                try:
                    self._validate_array_dimensions(shape)
                    self._validate_dtype(dtype)
                    expected_data_size = self._calculate_safe_array_size(shape, dtype)
                    expected_size = data_offset + expected_data_size
                except ValueError as e:
                    result.add_issue(
                        f"Array validation failed: {e}",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                        details={
                            "security_check": "array_validation",
                            "shape": shape,
                            "dtype": str(dtype),
                        },
                    )
                    result.finish(success=False)
                    return result

                if file_size != expected_size:
                    result.add_issue(
                        "File size does not match header information",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                        details={
                            "expected_size": expected_size,
                            "actual_size": file_size,
                            "shape": shape,
                            "dtype": str(dtype),
                        },
                    )

                # Note: Dimension validation is now handled in _validate_array_dimensions
                # which is called earlier and has configurable limits

                result.bytes_scanned = file_size
                result.metadata.update(
                    {"shape": shape, "dtype": str(dtype), "fortran_order": fortran}
                )
        except Exception as e:  # pragma: no cover - unexpected errors
            result.add_issue(
                f"Error scanning NumPy file: {e}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result


=====================================
FILE: ./modelaudit/scanners/oci_layer_scanner.py
=====================================

import json
import os
import tarfile
import tempfile
from typing import Any

from .base import BaseScanner, IssueSeverity, ScanResult

# Try to import yaml for YAML manifests
try:
    import yaml  # type: ignore

    HAS_YAML = True
except Exception:
    HAS_YAML = False


class OciLayerScanner(BaseScanner):
    """Scanner for OCI/Artifactory manifest files with .tar.gz layers."""

    name = "oci_layer"
    description = "Scans container manifests and embedded layers for model files"
    supported_extensions = [".manifest"]

    @classmethod
    def can_handle(cls, path: str) -> bool:
        if not os.path.isfile(path):
            return False
        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False
        # Quick check for .tar.gz references to avoid conflicts with ManifestScanner
        try:
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                snippet = f.read(2048)
            return ".tar.gz" in snippet
        except Exception:
            return False

    def scan(self, path: str) -> ScanResult:
        path_check = self._check_path(path)
        if path_check:
            return path_check

        result = self._create_result()
        manifest_data: Any = None

        try:
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                text = f.read()
            try:
                manifest_data = json.loads(text)
            except Exception:
                if HAS_YAML:
                    manifest_data = yaml.safe_load(text)
                else:
                    raise
        except Exception as e:
            result.add_issue(
                f"Error parsing manifest: {e}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        # Find layer paths ending with .tar.gz
        layer_paths: list[str] = []

        def _search(obj: Any) -> None:
            if isinstance(obj, dict):
                for v in obj.values():
                    _search(v)
            elif isinstance(obj, list):
                for item in obj:
                    _search(item)
            elif isinstance(obj, str) and obj.endswith(".tar.gz"):
                layer_paths.append(obj)

        _search(manifest_data)

        for layer_ref in layer_paths:
            layer_path = layer_ref
            if not os.path.isabs(layer_path):
                layer_path = os.path.join(os.path.dirname(path), layer_ref)
            if not os.path.exists(layer_path):
                result.add_issue(
                    f"Layer not found: {layer_ref}",
                    severity=IssueSeverity.WARNING,
                    location=f"{path}:{layer_ref}",
                )
                continue
            try:
                from . import SCANNER_REGISTRY

                with tarfile.open(layer_path, "r:gz") as tar:
                    for member in tar:
                        if not member.isfile():
                            continue
                        name = member.name
                        _, ext = os.path.splitext(name)
                        if not any(s.can_handle(name) for s in SCANNER_REGISTRY):
                            continue
                        fileobj = tar.extractfile(member)
                        if fileobj is None:
                            continue
                        with tempfile.NamedTemporaryFile(
                            suffix=ext, delete=False
                        ) as tmp:
                            tmp.write(fileobj.read())
                            tmp_path = tmp.name
                        try:
                            from .. import core

                            file_result = core.scan_file(tmp_path, self.config)
                            for issue in file_result.issues:
                                if issue.location:
                                    issue.location = (
                                        f"{path}:{layer_ref}:{name} {issue.location}"
                                    )
                                else:
                                    issue.location = f"{path}:{layer_ref}:{name}"
                                if issue.details is None:
                                    issue.details = {}
                                issue.details["layer"] = layer_ref
                            result.merge(file_result)
                        finally:
                            os.unlink(tmp_path)
            except Exception as e:
                result.add_issue(
                    f"Error processing layer {layer_ref}: {e}",
                    severity=IssueSeverity.WARNING,
                    location=f"{path}:{layer_ref}",
                    details={"exception_type": type(e).__name__},
                )

        result.finish(success=True)
        return result


=====================================
FILE: ./modelaudit/scanners/onnx_scanner.py
=====================================

import os
from pathlib import Path
from typing import Any

from .base import BaseScanner, IssueSeverity, ScanResult

try:
    import numpy as np
    import onnx
    from onnx import mapping

    HAS_ONNX = True
except Exception:
    HAS_ONNX = False


class OnnxScanner(BaseScanner):
    """Scanner for ONNX model files."""

    name = "onnx"
    description = "Scans ONNX models for custom operators and integrity issues"
    supported_extensions = [".onnx"]

    @classmethod
    def can_handle(cls, path: str) -> bool:
        if not HAS_ONNX:
            return False
        if not os.path.isfile(path):
            return False
        return os.path.splitext(path)[1].lower() in cls.supported_extensions

    def scan(self, path: str) -> ScanResult:
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        if not HAS_ONNX:
            result.add_issue(
                "onnx package not installed, cannot scan ONNX files.",
                severity=IssueSeverity.CRITICAL,
                location=path,
            )
            result.finish(success=False)
            return result

        try:
            model = onnx.load(path, load_external_data=False)
            result.bytes_scanned = file_size
        except Exception as e:  # pragma: no cover - unexpected parse errors
            result.add_issue(
                f"Error parsing ONNX model: {e}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.metadata.update(
            {
                "ir_version": model.ir_version,
                "producer_name": model.producer_name,
                "node_count": len(model.graph.node),
            }
        )

        self._check_custom_ops(model, path, result)
        self._check_external_data(model, path, result)
        self._check_tensor_sizes(model, path, result)

        result.finish(success=True)
        return result

    def _check_custom_ops(self, model: Any, path: str, result: ScanResult) -> None:
        custom_domains = set()
        for node in model.graph.node:
            if node.domain and node.domain not in ("", "ai.onnx"):
                custom_domains.add(node.domain)
                result.add_issue(
                    f"Model uses custom operator domain '{node.domain}'",
                    severity=IssueSeverity.WARNING,
                    location=f"{path} (node: {node.name})",
                    details={"op_type": node.op_type, "domain": node.domain},
                )
        if custom_domains:
            result.metadata["custom_domains"] = sorted(custom_domains)

    def _check_external_data(self, model: Any, path: str, result: ScanResult) -> None:
        model_dir = Path(path).resolve().parent
        for tensor in model.graph.initializer:
            if tensor.data_location == onnx.TensorProto.EXTERNAL:
                info = {entry.key: entry.value for entry in tensor.external_data}
                location = info.get("location")
                if location is None:
                    result.add_issue(
                        f"Tensor '{tensor.name}' uses external data without location",
                        severity=IssueSeverity.WARNING,
                        location=path,
                        details={"tensor": tensor.name},
                    )
                    continue
                external_path = (model_dir / location).resolve()
                if not external_path.exists():
                    result.add_issue(
                        f"External data file not found for tensor '{tensor.name}'",
                        severity=IssueSeverity.CRITICAL,
                        location=str(external_path),
                        details={"tensor": tensor.name, "file": location},
                    )
                elif not str(external_path).startswith(str(model_dir)):
                    result.add_issue(
                        f"External data file outside model directory for tensor '{tensor.name}'",
                        severity=IssueSeverity.CRITICAL,
                        location=str(external_path),
                        details={"tensor": tensor.name, "file": location},
                    )
                else:
                    self._validate_external_size(tensor, external_path, result)

    def _validate_external_size(
        self, tensor: Any, external_path: Path, result: ScanResult
    ) -> None:
        try:
            dtype = np.dtype(mapping.TENSOR_TYPE_TO_NP_TYPE[tensor.data_type])
            num_elem = 1
            for d in tensor.dims:
                num_elem *= d
            expected_size = int(num_elem) * int(dtype.itemsize)
            actual_size = external_path.stat().st_size
            if actual_size < expected_size:
                result.add_issue(
                    "External data file size mismatch",
                    severity=IssueSeverity.CRITICAL,
                    location=str(external_path),
                    details={
                        "tensor": tensor.name,
                        "expected_size": expected_size,
                        "actual_size": actual_size,
                    },
                )
        except Exception as e:
            result.add_issue(
                f"Could not validate external data size: {e}",
                severity=IssueSeverity.DEBUG,
                location=str(external_path),
            )

    def _check_tensor_sizes(self, model: Any, path: str, result: ScanResult) -> None:
        for tensor in model.graph.initializer:
            if tensor.data_location == onnx.TensorProto.EXTERNAL:
                continue
            if tensor.raw_data:
                try:
                    dtype = np.dtype(mapping.TENSOR_TYPE_TO_NP_TYPE[tensor.data_type])
                    num_elem = 1
                    for d in tensor.dims:
                        num_elem *= d
                    expected_size = int(num_elem) * int(dtype.itemsize)
                    actual_size = len(tensor.raw_data)
                    if actual_size < expected_size:
                        result.add_issue(
                            f"Tensor '{tensor.name}' data appears truncated",
                            severity=IssueSeverity.CRITICAL,
                            location=f"{path} (tensor: {tensor.name})",
                            details={
                                "expected_size": expected_size,
                                "actual_size": actual_size,
                            },
                        )
                except Exception as e:
                    result.add_issue(
                        f"Could not validate tensor '{tensor.name}': {e}",
                        severity=IssueSeverity.DEBUG,
                        location=path,
                    )


=====================================
FILE: ./modelaudit/scanners/pickle_scanner.py
=====================================

import os
import pickletools
import time
from typing import Any, BinaryIO, Dict, List, Optional, Union

from modelaudit.suspicious_symbols import (
    SUSPICIOUS_GLOBALS,
    SUSPICIOUS_STRING_PATTERNS,
)

from ..explanations import (
    get_import_explanation,
    get_opcode_explanation,
    get_pattern_explanation,
)
from .base import BaseScanner, IssueSeverity, ScanResult

# Add dangerous builtin functions that might be used in __reduce__ methods
DANGEROUS_BUILTINS = ["eval", "exec", "compile", "open", "input", "__import__"]

# Dangerous opcodes that can lead to code execution
DANGEROUS_OPCODES = [
    "REDUCE",
    "INST",
    "OBJ",
    "NEWOBJ",
    "NEWOBJ_EX",
    "GLOBAL",
    "BUILD",
    "STACK_GLOBAL",
]


# ============================================================================
# SMART DETECTION SYSTEM - ML Context Awareness
# ============================================================================

# ML Framework Detection Patterns
ML_FRAMEWORK_PATTERNS: Dict[str, Dict[str, Union[List[str], float]]] = {
    "pytorch": {
        "modules": [
            "torch",
            "torchvision",
            "torch.nn",
            "torch.optim",
            "torch.utils",
            "_pickle",
        ],
        "classes": [
            "OrderedDict",
            "Parameter",
            "Module",
            "Linear",
            "Conv2d",
            "BatchNorm2d",
            "ReLU",
            "MaxPool2d",
            "AdaptiveAvgPool2d",
            "Sequential",
            "ModuleList",
        ],
        "patterns": [r"torch\..*", r"_pickle\..*", r"collections\.OrderedDict"],
        "confidence_boost": 0.8,
    },
    "yolo": {
        "modules": ["ultralytics", "yolo", "models"],
        "classes": ["YOLO", "YOLOv8", "Detect", "C2f", "Conv", "Bottleneck", "SPPF"],
        "patterns": [
            r"yolo.*",
            r"ultralytics\..*",
            r".*\.detect",
            r".*\.backbone",
            r".*\.head",
        ],
        "confidence_boost": 0.9,
    },
    "tensorflow": {
        "modules": ["tensorflow", "keras", "tf"],
        "classes": ["Model", "Layer", "Dense", "Conv2D", "Flatten"],
        "patterns": [r"tensorflow\..*", r"keras\..*"],
        "confidence_boost": 0.8,
    },
    "sklearn": {
        "modules": ["sklearn", "joblib"],
        "classes": ["Pipeline", "StandardScaler", "PCA"],
        "patterns": [r"sklearn\..*", r"joblib\..*"],
        "confidence_boost": 0.7,
    },
    "huggingface": {
        "modules": ["transformers", "tokenizers"],
        "classes": ["AutoModel", "AutoTokenizer", "BertModel", "GPT2Model"],
        "patterns": [r"transformers\..*", r"tokenizers\..*"],
        "confidence_boost": 0.8,
    },
}

# Safe ML-specific global patterns
ML_SAFE_GLOBALS: Dict[str, List[str]] = {
    # PyTorch safe patterns
    "torch": ["*"],  # All torch functions are generally safe
    "torch.nn": ["*"],
    "torch.optim": ["*"],
    "torch.utils": ["*"],
    "_pickle": ["*"],  # PyTorch uses _pickle internally
    "collections": ["OrderedDict", "defaultdict", "namedtuple"],
    "typing": ["*"],
    "numpy": ["*"],  # NumPy operations are safe
    "math": ["*"],  # Math operations are safe
    # YOLO/Ultralytics safe patterns
    "ultralytics": ["*"],
    "yolo": ["*"],
    # Standard ML libraries
    "sklearn": ["*"],
    "transformers": ["*"],
    "tokenizers": ["*"],
    "tensorflow": ["*"],
    "keras": ["*"],
}

# Dangerous actual code execution patterns in strings
ACTUAL_DANGEROUS_STRING_PATTERNS = [
    r"os\.system\s*\(",
    r"subprocess\.",
    r"exec\s*\(",
    r"eval\s*\(",
    r"__import__\s*\(",
    r"compile\s*\(",
    r"open\s*\(['\"].*['\"],\s*['\"]w",  # File write operations
    r"\.popen\s*\(",
    r"\.spawn\s*\(",
]


def _detect_ml_context(opcodes: list[tuple]) -> dict[str, Any]:
    """
    Detect ML framework context from opcodes with confidence scoring.
    Uses improved scoring that focuses on presence and diversity of ML patterns
    rather than their proportion of total opcodes.
    """
    context: dict[str, Any] = {
        "frameworks": {},
        "overall_confidence": 0.0,
        "is_ml_content": False,
        "detected_patterns": [],
    }

    total_opcodes = len(opcodes)
    if total_opcodes == 0:
        return context

    # Analyze GLOBAL opcodes for ML patterns
    global_refs: dict[str, int] = {}
    total_global_opcodes = 0

    for opcode, arg, pos in opcodes:
        if opcode.name == "GLOBAL" and isinstance(arg, str):
            total_global_opcodes += 1
            # Extract module name from global reference
            if "." in arg:
                module = arg.split(".")[0]
            elif " " in arg:
                module = arg.split(" ")[0]
            else:
                module = arg

            global_refs[module] = global_refs.get(module, 0) + 1

    # Check each framework with improved scoring
    for framework, patterns in ML_FRAMEWORK_PATTERNS.items():
        framework_score = 0.0
        matches: list[str] = []

        # Check module matches with improved scoring
        modules = patterns["modules"]
        if isinstance(modules, list):
            for module in modules:
                if module in global_refs:
                    # Score based on presence and frequency,
                    # not proportion of total opcodes
                    ref_count = global_refs[module]

                    # Base score for presence
                    module_score = 10.0  # Base score for any ML module presence

                    # Bonus for frequency (up to 20 more points)
                    if ref_count >= 5:
                        module_score += 20.0
                    elif ref_count >= 2:
                        module_score += 10.0
                    elif ref_count >= 1:
                        module_score += 5.0

                    framework_score += module_score
                    matches.append(f"module:{module}({ref_count})")

        # Store framework detection with much lower threshold
        if framework_score > 5.0:  # Much lower threshold - any ML module presence
            # Normalize confidence to 0-1 range
            confidence_boost = patterns["confidence_boost"]
            if isinstance(confidence_boost, (int, float)):
                confidence = min(framework_score / 100.0 * confidence_boost, 1.0)
                context["frameworks"][framework] = {
                    "confidence": confidence,
                    "matches": matches,
                    "raw_score": framework_score,
                }
                context["detected_patterns"].extend(matches)

    # Calculate overall ML confidence - highest framework confidence
    if context["frameworks"]:
        context["overall_confidence"] = max(
            fw["confidence"] for fw in context["frameworks"].values()
        )
        # Much more lenient threshold - any significant ML pattern detection
        context["is_ml_content"] = context["overall_confidence"] > 0.15  # Was 0.3

    return context


def _is_actually_dangerous_global(mod: str, func: str, ml_context: dict) -> bool:
    """
    Smart global reference analysis - distinguishes between legitimate ML operations
    and actual dangerous operations.
    """
    # If we have high ML confidence, be more lenient with "suspicious" globals
    if (
        ml_context.get("is_ml_content")
        and ml_context.get("overall_confidence", 0) > 0.5
    ):
        # Check if this is a known safe ML global
        if mod in ML_SAFE_GLOBALS:
            safe_funcs = ML_SAFE_GLOBALS[mod]
            if safe_funcs == ["*"] or func in safe_funcs:
                return False

    # Use original suspicious global check for genuinely suspicious patterns
    return is_suspicious_global(mod, func)


def _is_actually_dangerous_string(s: str, ml_context: dict) -> Optional[str]:
    """
    Smart string analysis - looks for actual executable code rather than ML patterns.
    """
    import re

    if not isinstance(s, str):
        return None

    # Check for ACTUAL dangerous patterns (not just ML magic methods)
    for pattern in ACTUAL_DANGEROUS_STRING_PATTERNS:
        if re.search(pattern, s, re.IGNORECASE):
            return pattern

    # If we have strong ML context, ignore common ML patterns
    if (
        ml_context.get("is_ml_content")
        and ml_context.get("overall_confidence", 0) > 0.6
    ):
        # Skip common ML magic method patterns
        if re.match(r"^__\w+__$", s):  # Simple magic methods like __call__, __init__
            return None

        # Skip tensor/layer names
        if any(
            term in s.lower()
            for term in ["layer", "conv", "batch", "norm", "relu", "pool", "linear"]
        ):
            return None

    # Check for base64-like strings (still suspicious)
    if len(s) > 100 and re.match(r"^[A-Za-z0-9+/=]+$", s):
        return "potential_base64"

    return None


def _should_ignore_opcode_sequence(opcodes: list[tuple], ml_context: dict) -> bool:
    """
    Determine if an opcode sequence should be ignored based on ML context.
    """
    if not ml_context.get("is_ml_content"):
        return False

    # High confidence ML content - be very permissive with opcode sequences
    if ml_context.get("overall_confidence", 0) > 0.7:
        return True

    # Medium confidence - check for specific ML patterns
    if ml_context.get("overall_confidence", 0) > 0.4:
        # Look for legitimate ML construction patterns
        reduce_count = sum(1 for opcode, _, _ in opcodes if opcode.name == "REDUCE")
        global_count = sum(1 for opcode, _, _ in opcodes if opcode.name == "GLOBAL")

        # High REDUCE/GLOBAL ratio suggests ML object construction
        if global_count > 0 and reduce_count / global_count > 0.5:
            return True

    return False


def _get_context_aware_severity(
    base_severity: IssueSeverity, ml_context: dict
) -> IssueSeverity:
    """
    Adjust severity based on ML context confidence.
    """
    if not ml_context.get("is_ml_content"):
        return base_severity

    confidence = ml_context.get("overall_confidence", 0)

    # High confidence ML content - downgrade severity
    if confidence > 0.8:
        if base_severity == IssueSeverity.CRITICAL:
            return IssueSeverity.WARNING
        elif base_severity == IssueSeverity.WARNING:
            return IssueSeverity.INFO
    elif confidence > 0.5:
        if base_severity == IssueSeverity.CRITICAL:
            return IssueSeverity.WARNING

    return base_severity


# ============================================================================
# END SMART DETECTION SYSTEM
# ============================================================================


def is_suspicious_global(mod: str, func: str) -> bool:
    """Check if a module.function reference is suspicious"""
    if mod in SUSPICIOUS_GLOBALS:
        val = SUSPICIOUS_GLOBALS[mod]
        if val == "*":
            return True
        if isinstance(val, list) and func in val:
            return True
    return False


def is_suspicious_string(s: str) -> Optional[str]:
    """Check if a string contains suspicious patterns"""
    import re

    if not isinstance(s, str):
        return None

    for pattern in SUSPICIOUS_STRING_PATTERNS:
        match = re.search(pattern, s)
        if match:
            return pattern

    # Check for base64-like strings (long strings with base64 charset)
    if len(s) > 40 and re.match(r"^[A-Za-z0-9+/=]+$", s):
        return "potential_base64"

    return None


def is_dangerous_reduce_pattern(opcodes: list[tuple]) -> Optional[dict[str, Any]]:
    """
    Check for patterns that indicate a dangerous __reduce__ method
    Returns details about the dangerous pattern if found, None otherwise
    """
    # Look for common patterns in __reduce__ exploits
    for i, (opcode, arg, pos) in enumerate(opcodes):
        # Check for GLOBAL followed by REDUCE - common in exploits
        if (
            opcode.name == "GLOBAL"
            and i + 1 < len(opcodes)
            and opcodes[i + 1][0].name == "REDUCE"
        ):
            if isinstance(arg, str):
                parts = (
                    arg.split(" ", 1)
                    if " " in arg
                    else arg.rsplit(".", 1)
                    if "." in arg
                    else [arg, ""]
                )
                if len(parts) == 2:
                    mod, func = parts
                    return {
                        "pattern": "GLOBAL+REDUCE",
                        "module": mod,
                        "function": func,
                        "position": pos,
                        "opcode": opcode.name,
                    }

        # Check for INST or OBJ opcodes which can also be used for code execution
        if opcode.name in ["INST", "OBJ", "NEWOBJ"] and isinstance(arg, str):
            return {
                "pattern": f"{opcode.name}_EXECUTION",
                "argument": arg,
                "position": pos,
                "opcode": opcode.name,
            }

        # Check for suspicious attribute access patterns (GETATTR followed by CALL)
        if (
            opcode.name == "GETATTR"
            and i + 1 < len(opcodes)
            and opcodes[i + 1][0].name == "CALL"
        ):
            return {
                "pattern": "GETATTR+CALL",
                "attribute": arg,
                "position": pos,
                "opcode": opcode.name,
            }

        # Check for suspicious strings in STRING or BINSTRING opcodes
        if opcode.name in [
            "STRING",
            "BINSTRING",
            "SHORT_BINSTRING",
            "UNICODE",
        ] and isinstance(arg, str):
            suspicious_pattern = is_suspicious_string(arg)
            if suspicious_pattern:
                return {
                    "pattern": "SUSPICIOUS_STRING",
                    "string_pattern": suspicious_pattern,
                    "string_preview": arg[:50] + ("..." if len(arg) > 50 else ""),
                    "position": pos,
                    "opcode": opcode.name,
                }

    return None


def check_opcode_sequence(
    opcodes: list[tuple], ml_context: dict
) -> list[dict[str, Any]]:
    """
    Analyze the full sequence of opcodes for suspicious patterns
    with ML context awareness.
    Returns a list of suspicious patterns found.
    """
    suspicious_patterns: list[dict[str, Any]] = []

    # SMART DETECTION: Check if we should ignore this sequence based on ML context
    if _should_ignore_opcode_sequence(opcodes, ml_context):
        return suspicious_patterns  # Return empty list for legitimate ML content

    # Count dangerous opcodes with ML context awareness
    dangerous_opcode_count = 0
    consecutive_dangerous = 0
    max_consecutive = 0

    for i, (opcode, arg, pos) in enumerate(opcodes):
        # Track dangerous opcodes
        if opcode.name in DANGEROUS_OPCODES:
            dangerous_opcode_count += 1
            consecutive_dangerous += 1
            max_consecutive = max(max_consecutive, consecutive_dangerous)
        else:
            consecutive_dangerous = 0

        # SMART DETECTION: Much higher threshold for ML content
        ml_confidence = ml_context.get("overall_confidence", 0)
        if ml_confidence > 0.7:
            threshold = 100  # Very high threshold for high-confidence ML
        elif ml_confidence > 0.4:
            threshold = 50  # Higher threshold for medium-confidence ML
        else:
            threshold = 20  # Still higher than original 5 for unknown content

        # If we see too many dangerous opcodes AND it's not clearly ML content
        if dangerous_opcode_count > threshold:
            suspicious_patterns.append(
                {
                    "pattern": "MANY_DANGEROUS_OPCODES",
                    "count": dangerous_opcode_count,
                    "max_consecutive": max_consecutive,
                    "ml_confidence": ml_confidence,
                    "position": pos,
                    "opcode": opcode.name,
                },
            )
            # Reset counter to avoid multiple alerts
            dangerous_opcode_count = 0
            max_consecutive = 0

    return suspicious_patterns


class PickleScanner(BaseScanner):
    """Scanner for Python Pickle files"""

    name = "pickle"
    description = "Scans Python pickle files for suspicious code references"
    supported_extensions = [
        ".pkl",
        ".pickle",
        ".dill",
        ".joblib",
        ".bin",
        ".pt",
        ".pth",
        ".ckpt",
    ]

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        # Additional pickle-specific configuration
        self.max_opcodes = self.config.get("max_opcodes", 1000000)

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if the file is a pickle based on extension and content"""
        file_ext = os.path.splitext(path)[1].lower()

        # For known pickle extensions, always handle
        if file_ext in [".pkl", ".pickle", ".dill", ".joblib"]:
            return True

        # For ambiguous extensions, check the actual file format
        if file_ext in [".bin", ".pt", ".pth", ".ckpt"]:
            try:
                # Import here to avoid circular dependency
                from modelaudit.utils.filetype import detect_file_format

                file_format = detect_file_format(path)
                return file_format == "pickle"
            except Exception:
                # If detection fails, fall back to extension check
                return file_ext in cls.supported_extensions

        return False

    def scan(self, path: str) -> ScanResult:
        """Scan a pickle file for suspicious content"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        # Check if this is a .bin file that might be a PyTorch file
        is_bin_file = os.path.splitext(path)[1].lower() == ".bin"

        try:
            with open(path, "rb") as f:
                # Store the file path for use in issue locations
                self.current_file_path = path
                scan_result = self._scan_pickle_bytes(f, file_size)
                result.merge(scan_result)

                # For .bin files, also scan the remaining binary content
                # PyTorch files have pickle header followed by tensor data
                if is_bin_file and scan_result.success:
                    pickle_end_pos = f.tell()
                    remaining_bytes = file_size - pickle_end_pos

                    if remaining_bytes > 0:
                        # Check if this is likely a PyTorch model based on ML context
                        ml_context = scan_result.metadata.get("ml_context", {})
                        is_pytorch = "pytorch" in ml_context.get("frameworks", {})
                        ml_confidence = ml_context.get("overall_confidence", 0)

                        # Skip binary scanning for high-confidence ML model files
                        # as they contain tensor data that can trigger false positives
                        if is_pytorch and ml_confidence > 0.7:
                            result.metadata["binary_scan_skipped"] = True
                            result.metadata["skip_reason"] = (
                                "High-confidence PyTorch model detected"
                            )
                            result.bytes_scanned = file_size
                            result.metadata["pickle_bytes"] = pickle_end_pos
                            result.metadata["binary_bytes"] = remaining_bytes
                        else:
                            # Scan the binary content after pickle
                            binary_result = self._scan_binary_content(
                                f, pickle_end_pos, file_size
                            )

                            # Add binary scanning results
                            for issue in binary_result.issues:
                                result.add_issue(
                                    message=issue.message,
                                    severity=issue.severity,
                                    location=issue.location,
                                    details=issue.details,
                                    why=issue.why,
                                )

                            # Update total bytes scanned
                            result.bytes_scanned = file_size
                            result.metadata["pickle_bytes"] = pickle_end_pos
                            result.metadata["binary_bytes"] = remaining_bytes

        except Exception as e:
            result.add_issue(
                f"Error opening pickle file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _scan_pickle_bytes(self, file_obj: BinaryIO, file_size: int) -> ScanResult:
        """Scan pickle file content for suspicious opcodes"""
        result = self._create_result()
        opcode_count = 0
        suspicious_count = 0

        try:
            # Process the pickle
            start_pos = file_obj.tell()

            # Store opcodes for pattern analysis
            opcodes = []
            # Track strings on the stack for STACK_GLOBAL opcode analysis
            string_stack = []

            for opcode, arg, pos in pickletools.genops(file_obj):
                opcodes.append((opcode, arg, pos))
                opcode_count += 1

                # Track strings for STACK_GLOBAL analysis
                if opcode.name in [
                    "STRING",
                    "BINSTRING",
                    "SHORT_BINSTRING",
                    "SHORT_BINUNICODE",
                    "UNICODE",
                ] and isinstance(arg, str):
                    string_stack.append(arg)
                    # Keep only the last 10 strings to avoid memory issues
                    if len(string_stack) > 10:
                        string_stack.pop(0)

                # Check for too many opcodes
                if opcode_count > self.max_opcodes:
                    result.add_issue(
                        f"Too many opcodes in pickle (> {self.max_opcodes})",
                        severity=IssueSeverity.WARNING,
                        location=self.current_file_path,
                        details={
                            "opcode_count": opcode_count,
                            "max_opcodes": self.max_opcodes,
                        },
                        why=get_pattern_explanation("pickle_size_limit"),
                    )
                    break

                # Check for timeout
                if time.time() - result.start_time > self.timeout:
                    result.add_issue(
                        f"Scanning timed out after {self.timeout} seconds",
                        severity=IssueSeverity.WARNING,
                        location=self.current_file_path,
                        details={"opcode_count": opcode_count, "timeout": self.timeout},
                        why="The scan exceeded the configured time limit. Large or complex pickle files may take longer to analyze due to the number of opcodes that need to be processed.",
                    )
                    break

            # SMART DETECTION: Analyze ML context once for the entire pickle
            ml_context = _detect_ml_context(opcodes)

            # Add ML context to metadata for debugging
            result.metadata.update(
                {
                    "ml_context": ml_context,
                    "opcode_count": opcode_count,
                    "suspicious_count": suspicious_count,
                }
            )

            # Now analyze the collected opcodes with ML context awareness
            for opcode, arg, pos in opcodes:
                # Check for GLOBAL opcodes that might reference suspicious modules
                if opcode.name == "GLOBAL":
                    if isinstance(arg, str):
                        # Handle both "module function" and "module.function" formats
                        parts = (
                            arg.split(" ", 1)
                            if " " in arg
                            else arg.rsplit(".", 1)
                            if "." in arg
                            else [arg, ""]
                        )

                        if len(parts) == 2:
                            mod, func = parts
                            if _is_actually_dangerous_global(mod, func, ml_context):
                                suspicious_count += 1
                                severity = _get_context_aware_severity(
                                    IssueSeverity.CRITICAL, ml_context
                                )
                                result.add_issue(
                                    f"Suspicious reference {mod}.{func}",
                                    severity=severity,
                                    location=f"{self.current_file_path} (pos {pos})",
                                    details={
                                        "module": mod,
                                        "function": func,
                                        "position": pos,
                                        "opcode": opcode.name,
                                        "ml_context_confidence": ml_context.get(
                                            "overall_confidence", 0
                                        ),
                                    },
                                    why=get_import_explanation(mod),
                                )

                # SMART DETECTION: Only flag REDUCE opcodes if not clearly ML content
                if opcode.name == "REDUCE" and not ml_context.get(
                    "is_ml_content", False
                ):
                    severity = _get_context_aware_severity(
                        IssueSeverity.WARNING, ml_context
                    )
                    result.add_issue(
                        "Found REDUCE opcode - potential __reduce__ method execution",
                        severity=severity,
                        location=f"{self.current_file_path} (pos {pos})",
                        details={
                            "position": pos,
                            "opcode": opcode.name,
                            "ml_context_confidence": ml_context.get(
                                "overall_confidence", 0
                            ),
                        },
                        why=get_opcode_explanation("REDUCE"),
                    )

                # SMART DETECTION: Only flag other dangerous opcodes
                # if not clearly ML content
                if opcode.name in ["INST", "OBJ", "NEWOBJ"] and not ml_context.get(
                    "is_ml_content", False
                ):
                    severity = _get_context_aware_severity(
                        IssueSeverity.WARNING, ml_context
                    )
                    result.add_issue(
                        f"Found {opcode.name} opcode - potential code execution",
                        severity=severity,
                        location=f"{self.current_file_path} (pos {pos})",
                        details={
                            "position": pos,
                            "opcode": opcode.name,
                            "argument": str(arg),
                            "ml_context_confidence": ml_context.get(
                                "overall_confidence", 0
                            ),
                        },
                        why=get_opcode_explanation(opcode.name),
                    )

                # Check for suspicious strings
                if opcode.name in [
                    "STRING",
                    "BINSTRING",
                    "SHORT_BINSTRING",
                    "UNICODE",
                ] and isinstance(arg, str):
                    suspicious_pattern = _is_actually_dangerous_string(arg, ml_context)
                    if suspicious_pattern:
                        severity = _get_context_aware_severity(
                            IssueSeverity.WARNING, ml_context
                        )
                        result.add_issue(
                            f"Suspicious string pattern: {suspicious_pattern}",
                            severity=severity,
                            location=f"{self.current_file_path} (pos {pos})",
                            details={
                                "position": pos,
                                "opcode": opcode.name,
                                "pattern": suspicious_pattern,
                                "string_preview": arg[:50]
                                + ("..." if len(arg) > 50 else ""),
                                "ml_context_confidence": ml_context.get(
                                    "overall_confidence", 0
                                ),
                            },
                            why=get_pattern_explanation("encoded_strings")
                            if suspicious_pattern == "potential_base64"
                            else "This string contains patterns that match known security risks such as shell commands, code execution functions, or encoded data.",
                        )

            # Check for STACK_GLOBAL patterns
            # (rebuild from opcodes to get proper context)
            for i, (opcode, arg, pos) in enumerate(opcodes):
                if opcode.name == "STACK_GLOBAL":
                    # Find the two immediately preceding STRING-like opcodes
                    # STACK_GLOBAL expects exactly two strings on the stack:
                    # module and function
                    recent_strings: list[str] = []
                    for j in range(
                        i - 1, max(0, i - 10), -1
                    ):  # Look back at most 10 opcodes
                        prev_opcode, prev_arg, prev_pos = opcodes[j]
                        if prev_opcode.name in [
                            "STRING",
                            "BINSTRING",
                            "SHORT_BINSTRING",
                            "SHORT_BINUNICODE",
                            "UNICODE",
                        ] and isinstance(prev_arg, str):
                            recent_strings.insert(
                                0, prev_arg
                            )  # Insert at beginning to maintain order
                            if len(recent_strings) >= 2:
                                break

                    if len(recent_strings) >= 2:
                        # The two strings are module and function in that order
                        mod = recent_strings[0]  # First string pushed (module)
                        func = recent_strings[1]  # Second string pushed (function)
                        if _is_actually_dangerous_global(mod, func, ml_context):
                            suspicious_count += 1
                            severity = _get_context_aware_severity(
                                IssueSeverity.CRITICAL, ml_context
                            )
                            result.add_issue(
                                f"Suspicious module reference found: {mod}.{func}",
                                severity=severity,
                                location=f"{self.current_file_path} (pos {pos})",
                                details={
                                    "module": mod,
                                    "function": func,
                                    "position": pos,
                                    "opcode": opcode.name,
                                    "ml_context_confidence": ml_context.get(
                                        "overall_confidence", 0
                                    ),
                                },
                                why=get_import_explanation(mod),
                            )
                    else:
                        # Only warn about insufficient context if not ML content
                        if not ml_context.get("is_ml_content", False):
                            result.add_issue(
                                "STACK_GLOBAL opcode found without "
                                "sufficient string context",
                                severity=IssueSeverity.WARNING,
                                location=f"{self.current_file_path} (pos {pos})",
                                details={
                                    "position": pos,
                                    "opcode": opcode.name,
                                    "stack_size": len(recent_strings),
                                    "ml_context_confidence": ml_context.get(
                                        "overall_confidence", 0
                                    ),
                                },
                                why="STACK_GLOBAL requires two strings on the stack (module and function name) to import and access module attributes. Insufficient context prevents determining which module is being accessed.",
                            )

            # Check for dangerous patterns in the opcodes
            dangerous_pattern = is_dangerous_reduce_pattern(opcodes)
            if dangerous_pattern and not ml_context.get("is_ml_content", False):
                suspicious_count += 1
                severity = _get_context_aware_severity(
                    IssueSeverity.CRITICAL, ml_context
                )
                module_name = dangerous_pattern.get("module", "")
                result.add_issue(
                    f"Detected dangerous __reduce__ pattern with "
                    f"{dangerous_pattern.get('module', '')}."
                    f"{dangerous_pattern.get('function', '')}",
                    severity=severity,
                    location=f"{self.current_file_path} "
                    f"(pos {dangerous_pattern.get('position', 0)})",
                    details={
                        **dangerous_pattern,
                        "ml_context_confidence": ml_context.get(
                            "overall_confidence", 0
                        ),
                    },
                    why=get_import_explanation(module_name)
                    if module_name
                    else "A dangerous pattern was detected that could execute arbitrary code during unpickling.",
                )

            # Check for suspicious opcode sequences with ML context
            suspicious_sequences = check_opcode_sequence(opcodes, ml_context)
            for sequence in suspicious_sequences:
                suspicious_count += 1
                severity = _get_context_aware_severity(
                    IssueSeverity.WARNING, ml_context
                )
                result.add_issue(
                    f"Suspicious opcode sequence: {sequence.get('pattern', 'unknown')}",
                    severity=severity,
                    location=f"{self.current_file_path} "
                    f"(pos {sequence.get('position', 0)})",
                    details={
                        **sequence,
                        "ml_context_confidence": ml_context.get(
                            "overall_confidence", 0
                        ),
                    },
                    why="This pickle contains an unusually high concentration of opcodes that can execute code (REDUCE, INST, OBJ, NEWOBJ). Such patterns are uncommon in legitimate model files.",
                )

            # Update metadata
            end_pos = file_obj.tell()
            result.bytes_scanned = end_pos - start_pos
            result.metadata.update(
                {"opcode_count": opcode_count, "suspicious_count": suspicious_count},
            )

        except Exception as e:
            result.add_issue(
                f"Error analyzing pickle ops: {e}",
                severity=IssueSeverity.CRITICAL,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )

        return result

    def _scan_binary_content(
        self, file_obj: BinaryIO, start_pos: int, file_size: int
    ) -> ScanResult:
        """Scan the binary content after pickle data for suspicious patterns"""
        result = self._create_result()

        try:
            # Common patterns that might indicate embedded Python code
            code_patterns = [
                b"import os",
                b"import sys",
                b"import subprocess",
                b"eval(",
                b"exec(",
                b"__import__",
                b"compile(",
                b"os.system",
                b"subprocess.call",
                b"subprocess.Popen",
                b"socket.socket",
            ]

            # Executable signatures with additional validation
            # For PE files, we need to check for the full DOS header structure
            # to avoid false positives from random "MZ" bytes in model weights
            executable_sigs = {
                b"\x7fELF": "Linux executable (ELF)",
                b"\xfe\xed\xfa\xce": "macOS executable (Mach-O 32-bit)",
                b"\xfe\xed\xfa\xcf": "macOS executable (Mach-O 64-bit)",
                b"\xcf\xfa\xed\xfe": "macOS executable (Mach-O)",
                b"#!/bin/": "Shell script shebang",
                b"#!/usr/bin/": "Shell script shebang",
            }

            # Read in chunks
            chunk_size = 1024 * 1024  # 1MB chunks
            bytes_scanned = 0

            while True:
                chunk = file_obj.read(chunk_size)
                if not chunk:
                    break

                current_offset = start_pos + bytes_scanned
                bytes_scanned += len(chunk)

                # Check for code patterns
                for pattern in code_patterns:
                    if pattern in chunk:
                        pos = chunk.find(pattern)
                        result.add_issue(
                            f"Suspicious code pattern in binary data: {pattern.decode('ascii', errors='ignore')}",
                            severity=IssueSeverity.WARNING,
                            location=f"{self.current_file_path} (offset: {current_offset + pos})",
                            details={
                                "pattern": pattern.decode("ascii", errors="ignore"),
                                "offset": current_offset + pos,
                                "section": "binary_data",
                            },
                            why="Python code patterns found in binary sections of the file. Model weights are typically numeric data and should not contain readable code strings.",
                        )

                # Check for executable signatures
                for sig, description in executable_sigs.items():
                    if sig in chunk:
                        pos = chunk.find(sig)
                        result.add_issue(
                            f"Executable signature found in binary data: {description}",
                            severity=IssueSeverity.CRITICAL,
                            location=f"{self.current_file_path} (offset: {current_offset + pos})",
                            details={
                                "signature": sig.hex(),
                                "description": description,
                                "offset": current_offset + pos,
                                "section": "binary_data",
                            },
                            why="Executable files embedded in model data can run arbitrary code on the system. Model files should contain only serialized weights and configuration data.",
                        )

                # Special check for Windows PE files with more validation
                # to reduce false positives from random "MZ" bytes
                pe_sig = b"MZ"
                if pe_sig in chunk:
                    pos = chunk.find(pe_sig)
                    # For PE files, check if we have enough data to validate DOS header
                    if pos + 64 <= len(chunk):  # DOS header is 64 bytes
                        # Check for "This program cannot be run in DOS mode" string
                        # which appears in all PE files
                        dos_stub_msg = b"This program cannot be run in DOS mode"
                        # Look for this message within reasonable distance from MZ
                        search_end = min(pos + 512, len(chunk))
                        if dos_stub_msg in chunk[pos:search_end]:
                            result.add_issue(
                                "Executable signature found in binary data: Windows executable (PE)",
                                severity=IssueSeverity.CRITICAL,
                                location=f"{self.current_file_path} (offset: {current_offset + pos})",
                                details={
                                    "signature": pe_sig.hex(),
                                    "description": "Windows executable (PE) with valid DOS stub",
                                    "offset": current_offset + pos,
                                    "section": "binary_data",
                                },
                                why="Windows executable files embedded in model data can run arbitrary code on the system. The presence of a valid DOS stub confirms this is an actual PE executable.",
                            )

                # Check for timeout
                if time.time() - result.start_time > self.timeout:
                    result.add_issue(
                        f"Binary scanning timed out after {self.timeout} seconds",
                        severity=IssueSeverity.WARNING,
                        location=self.current_file_path,
                        details={
                            "bytes_scanned": start_pos + bytes_scanned,
                            "timeout": self.timeout,
                        },
                        why="The binary content scan exceeded the configured time limit. Large model files may require more time to fully analyze.",
                    )
                    break

            result.bytes_scanned = bytes_scanned

        except Exception as e:
            result.add_issue(
                f"Error scanning binary content: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=self.current_file_path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )

        return result


=====================================
FILE: ./modelaudit/scanners/pytorch_binary_scanner.py
=====================================

import os
import struct
from typing import Any, Optional

from .base import BaseScanner, IssueSeverity, ScanResult


class PyTorchBinaryScanner(BaseScanner):
    """Scanner for raw PyTorch binary tensor files (.bin)"""

    name = "pytorch_binary"
    description = "Scans PyTorch binary tensor files for suspicious patterns"
    supported_extensions = [".bin"]

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        # Get blacklist patterns from config
        self.blacklist_patterns = self.config.get("blacklist_patterns", [])

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not os.path.isfile(path):
            return False

        # Check file extension
        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False

        # Check if it's actually a pytorch binary file
        try:
            from modelaudit.utils.filetype import detect_file_format

            file_format = detect_file_format(path)
            return file_format == "pytorch_binary"
        except Exception:
            return False

    def scan(self, path: str) -> ScanResult:
        """Scan a PyTorch binary file for suspicious patterns"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            self.current_file_path = path

            # Check for suspiciously small files
            if file_size < 100:
                result.add_issue(
                    f"Suspiciously small binary file: {file_size} bytes",
                    severity=IssueSeverity.WARNING,
                    location=path,
                    details={"file_size": file_size},
                )

            # Read file in chunks to look for suspicious patterns
            bytes_scanned = 0
            chunk_size = 1024 * 1024  # 1MB chunks

            with open(path, "rb") as f:
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break

                    bytes_scanned += len(chunk)

                    # Check for embedded Python code patterns
                    self._check_for_code_patterns(
                        chunk, result, bytes_scanned - len(chunk)
                    )

                    # Check for blacklisted patterns
                    if self.blacklist_patterns:
                        self._check_for_blacklist_patterns(
                            chunk, result, bytes_scanned - len(chunk)
                        )

                    # Check for executable file signatures
                    self._check_for_executable_signatures(
                        chunk, result, bytes_scanned - len(chunk)
                    )

            result.bytes_scanned = bytes_scanned

            # Check if file appears to be a valid tensor file
            self._validate_tensor_structure(path, result)

        except Exception as e:
            result.add_issue(
                f"Error scanning binary file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _check_for_code_patterns(
        self, chunk: bytes, result: ScanResult, offset: int
    ) -> None:
        """Check for patterns that might indicate embedded code"""
        # Common patterns that might indicate embedded Python code
        code_patterns = [
            b"import os",
            b"import sys",
            b"import subprocess",
            b"eval(",
            b"exec(",
            b"__import__",
            b"compile(",
            b"globals()",
            b"locals()",
            b"open(",
            b"file(",
            b"input(",
            b"raw_input(",
            b"execfile(",
            b"os.system",
            b"subprocess.call",
            b"subprocess.Popen",
            b"socket.socket",
        ]

        for pattern in code_patterns:
            if pattern in chunk:
                # Find the position within the chunk
                pos = chunk.find(pattern)
                result.add_issue(
                    f"Suspicious code pattern found: {pattern.decode('ascii', errors='ignore')}",
                    severity=IssueSeverity.WARNING,
                    location=f"{self.current_file_path} (offset: {offset + pos})",
                    details={
                        "pattern": pattern.decode("ascii", errors="ignore"),
                        "offset": offset + pos,
                    },
                )

    def _check_for_blacklist_patterns(
        self, chunk: bytes, result: ScanResult, offset: int
    ) -> None:
        """Check for blacklisted patterns in the binary data"""
        for pattern in self.blacklist_patterns:
            pattern_bytes = pattern.encode("utf-8")
            if pattern_bytes in chunk:
                pos = chunk.find(pattern_bytes)
                result.add_issue(
                    f"Blacklisted pattern found: {pattern}",
                    severity=IssueSeverity.CRITICAL,
                    location=f"{self.current_file_path} (offset: {offset + pos})",
                    details={
                        "pattern": pattern,
                        "offset": offset + pos,
                    },
                )

    def _check_for_executable_signatures(
        self, chunk: bytes, result: ScanResult, offset: int
    ) -> None:
        """Check for executable file signatures"""
        # Common executable signatures
        executable_sigs = {
            b"MZ": "Windows executable (PE)",
            b"\x7fELF": "Linux executable (ELF)",
            b"\xfe\xed\xfa\xce": "macOS executable (Mach-O 32-bit)",
            b"\xfe\xed\xfa\xcf": "macOS executable (Mach-O 64-bit)",
            b"\xcf\xfa\xed\xfe": "macOS executable (Mach-O)",
            b"#!/": "Shell script shebang",
        }

        for sig, description in executable_sigs.items():
            if sig in chunk:
                pos = chunk.find(sig)
                result.add_issue(
                    f"Executable signature found: {description}",
                    severity=IssueSeverity.CRITICAL,
                    location=f"{self.current_file_path} (offset: {offset + pos})",
                    details={
                        "signature": sig.hex(),
                        "description": description,
                        "offset": offset + pos,
                    },
                )

    def _validate_tensor_structure(self, path: str, result: ScanResult) -> None:
        """Validate that the file appears to have valid tensor structure"""
        try:
            with open(path, "rb") as f:
                # Read first few bytes to check for common tensor patterns
                header = f.read(32)

                # PyTorch tensors often start with specific patterns
                # This is a basic check - real validation would require parsing the format
                if len(header) < 8:
                    result.add_issue(
                        "File too small to be a valid tensor file",
                        severity=IssueSeverity.WARNING,
                        location=self.current_file_path,
                        details={"header_size": len(header)},
                    )
                    return

                # Check if it looks like it might contain float32/float64 data
                # by looking for patterns of IEEE 754 floats
                # This is a heuristic - not definitive

                # Try to interpret first 8 bytes as double
                try:
                    value = struct.unpack("d", header[:8])[0]
                    # Check if it's a reasonable float value (not NaN, not huge)
                    if not (-1e100 < value < 1e100) or value != value:  # NaN check
                        result.metadata["tensor_validation"] = "unusual_float_values"
                except struct.error:
                    pass

        except Exception as e:
            result.add_issue(
                f"Error validating tensor structure: {str(e)}",
                severity=IssueSeverity.DEBUG,
                location=self.current_file_path,
                details={"exception": str(e)},
            )


=====================================
FILE: ./modelaudit/scanners/pytorch_zip_scanner.py
=====================================

import io
import os
import zipfile
from typing import Any, Optional

from ..utils import sanitize_archive_path
from .base import BaseScanner, IssueSeverity, ScanResult
from .pickle_scanner import PickleScanner


class PyTorchZipScanner(BaseScanner):
    """Scanner for PyTorch Zip-based model files (.pt, .pth)"""

    name = "pytorch_zip"
    description = "Scans PyTorch model files for suspicious code in embedded pickles"
    supported_extensions = [".pt", ".pth"]

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        # Initialize a pickle scanner for embedded pickles
        self.pickle_scanner = PickleScanner(config)

    @staticmethod
    def _read_header(path: str, length: int = 4) -> bytes:
        """Return the first few bytes of a file."""
        try:
            with open(path, "rb") as f:
                return f.read(length)
        except Exception:
            return b""

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not os.path.isfile(path):
            return False

        # Check file extension
        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False

        return True

    def scan(self, path: str) -> ScanResult:
        """Scan a PyTorch model file for suspicious code"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        header = self._read_header(path)
        if not header.startswith(b"PK"):
            result.add_issue(
                f"Not a valid zip file: {path}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"path": path},
            )
            result.finish(success=False)
            return result

        try:
            # Store the file path for use in issue locations
            self.current_file_path = path

            with zipfile.ZipFile(path, "r") as z:
                safe_entries: list[str] = []
                for name in z.namelist():
                    _, is_safe = sanitize_archive_path(name, "/tmp/extract")
                    if not is_safe:
                        result.add_issue(
                            f"Archive entry {name} attempted path traversal outside the archive",
                            severity=IssueSeverity.CRITICAL,
                            location=f"{path}:{name}",
                            details={"entry": name},
                        )
                        continue
                    safe_entries.append(name)
                pickle_files = [n for n in safe_entries if n.endswith(".pkl")]
                result.metadata["pickle_files"] = pickle_files

                # Track number of bytes scanned
                bytes_scanned = 0

                # Scan each pickle file
                for name in pickle_files:
                    data = z.read(name)
                    bytes_scanned += len(data)

                    file_like = io.BytesIO(data)
                    # Use the pickle scanner directly
                    sub_result = self.pickle_scanner._scan_pickle_bytes(
                        file_like,
                        len(data),
                    )

                    # Include the pickle filename in each issue
                    for issue in sub_result.issues:
                        if issue.details:
                            issue.details["pickle_filename"] = name
                        else:
                            issue.details = {"pickle_filename": name}

                        # Update location to include the main file path
                        if not issue.location:
                            issue.location = f"{path}:{name}"
                        elif "pos" in issue.location:
                            # If it's a position from the pickle scanner,
                            # prepend the file path
                            issue.location = f"{path}:{name} {issue.location}"

                    # Merge results
                    result.merge(sub_result)

                # Check for other suspicious files
                for name in safe_entries:
                    # Check for Python code files
                    if name.endswith(".py"):
                        result.add_issue(
                            f"Python code file found in PyTorch model: {name}",
                            severity=IssueSeverity.WARNING,
                            location=f"{path}:{name}",
                            details={"file": name},
                        )
                    # Check for shell scripts or other executable files
                    elif name.endswith((".sh", ".bash", ".cmd", ".exe")):
                        result.add_issue(
                            f"Executable file found in PyTorch model: {name}",
                            severity=IssueSeverity.CRITICAL,
                            location=f"{path}:{name}",
                            details={"file": name},
                        )

                # Check for missing data.pkl (common in PyTorch models)
                if not pickle_files or "data.pkl" not in [
                    os.path.basename(f) for f in pickle_files
                ]:
                    result.add_issue(
                        "PyTorch model is missing 'data.pkl', which is "
                        "unusual for standard PyTorch models.",
                        severity=IssueSeverity.WARNING,
                        location=self.current_file_path,
                        details={"missing_file": "data.pkl"},
                    )

                # Check for blacklist patterns in all files
                if (
                    hasattr(self, "config")
                    and self.config
                    and "blacklist_patterns" in self.config
                ):
                    blacklist_patterns = self.config["blacklist_patterns"]
                    for name in safe_entries:
                        try:
                            file_data = z.read(name)

                            # For pickled files, check for patterns in the binary data
                            if name.endswith(".pkl"):
                                for pattern in blacklist_patterns:
                                    # Convert pattern to bytes for binary search
                                    pattern_bytes = pattern.encode("utf-8")
                                    if pattern_bytes in file_data:
                                        result.add_issue(
                                            f"Blacklisted pattern '{pattern}' "
                                            f"found in pickled file {name}",
                                            severity=IssueSeverity.WARNING,
                                            location=f"{self.current_file_path} "
                                            f"({name})",
                                            details={
                                                "pattern": pattern,
                                                "file": name,
                                                "file_type": "pickle",
                                            },
                                        )
                            else:
                                # For text files, decode and search as text
                                try:
                                    content = file_data.decode("utf-8")
                                    for pattern in blacklist_patterns:
                                        if pattern in content:
                                            result.add_issue(
                                                f"Blacklisted pattern '{pattern}' "
                                                f"found in file {name}",
                                                severity=IssueSeverity.WARNING,
                                                location=f"{self.current_file_path} "
                                                f"({name})",
                                                details={
                                                    "pattern": pattern,
                                                    "file": name,
                                                    "file_type": "text",
                                                },
                                            )
                                except UnicodeDecodeError:
                                    # Skip blacklist checking for binary files
                                    # that can't be decoded as text
                                    pass
                        except Exception:
                            # Skip files we can't read
                            pass

                result.bytes_scanned = bytes_scanned

        except zipfile.BadZipFile:
            result.add_issue(
                f"Not a valid zip file: {path}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"path": path},
            )
            result.finish(success=False)
            return result
        except Exception as e:
            result.add_issue(
                f"Error scanning PyTorch zip file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result


=====================================
FILE: ./modelaudit/scanners/safetensors_scanner.py
=====================================

"""SafeTensors model scanner."""

from __future__ import annotations

import json
import os
import struct
from typing import Any, Optional

from .base import BaseScanner, IssueSeverity, ScanResult

# Map SafeTensors dtypes to byte sizes for integrity checking
_DTYPE_SIZES = {
    "F16": 2,
    "F32": 4,
    "F64": 8,
    "I8": 1,
    "I16": 2,
    "I32": 4,
    "I64": 8,
    "U8": 1,
    "U16": 2,
    "U32": 4,
    "U64": 8,
}


class SafeTensorsScanner(BaseScanner):
    """Scanner for SafeTensors model files."""

    name = "safetensors"
    description = "Scans SafeTensors model files for integrity issues"
    supported_extensions = [".safetensors"]

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path."""
        if not os.path.isfile(path):
            return False

        ext = os.path.splitext(path)[1].lower()
        if ext in cls.supported_extensions:
            return True

        try:
            from modelaudit.utils.filetype import detect_file_format

            return detect_file_format(path) == "safetensors"
        except Exception:
            return False

    def scan(self, path: str) -> ScanResult:
        """Scan a SafeTensors file."""
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            self.current_file_path = path
            with open(path, "rb") as f:
                header_len_bytes = f.read(8)
                if len(header_len_bytes) != 8:
                    result.add_issue(
                        "File too small to contain SafeTensors header length",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result

                header_len = struct.unpack("<Q", header_len_bytes)[0]
                if header_len <= 0 or header_len > file_size - 8:
                    result.add_issue(
                        "Invalid SafeTensors header length",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                        details={"header_len": header_len},
                    )
                    result.finish(success=False)
                    return result

                header_bytes = f.read(header_len)
                if len(header_bytes) != header_len:
                    result.add_issue(
                        "Failed to read SafeTensors header",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result

                if not header_bytes.strip().startswith(b"{"):
                    result.add_issue(
                        "SafeTensors header does not start with '{'",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result

                try:
                    header = json.loads(header_bytes.decode("utf-8"))
                except json.JSONDecodeError as e:
                    result.add_issue(
                        f"Invalid JSON header: {str(e)}",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result

                result.metadata["tensor_count"] = len(
                    [k for k in header.keys() if k != "__metadata__"]
                )

                # Validate tensor offsets and sizes
                tensor_entries: list[tuple[str, Any]] = [
                    (k, v) for k, v in header.items() if k != "__metadata__"
                ]

                data_size = file_size - (8 + header_len)
                offsets = []
                for name, info in tensor_entries:
                    if not isinstance(info, dict):
                        result.add_issue(
                            f"Invalid tensor entry for {name}",
                            severity=IssueSeverity.CRITICAL,
                            location=path,
                        )
                        continue

                    begin, end = info.get("data_offsets", [0, 0])
                    dtype = info.get("dtype")
                    shape = info.get("shape", [])

                    if not isinstance(begin, int) or not isinstance(end, int):
                        result.add_issue(
                            f"Invalid data_offsets for {name}",
                            severity=IssueSeverity.CRITICAL,
                            location=path,
                        )
                        continue

                    if begin < 0 or end <= begin or end > data_size:
                        result.add_issue(
                            f"Tensor {name} offsets out of bounds",
                            severity=IssueSeverity.CRITICAL,
                            location=path,
                            details={"begin": begin, "end": end},
                        )
                        continue

                    offsets.append((begin, end))

                    # Validate dtype/shape size
                    expected_size = self._expected_size(dtype, shape)
                    if expected_size is not None and expected_size != end - begin:
                        result.add_issue(
                            f"Size mismatch for tensor {name}",
                            severity=IssueSeverity.CRITICAL,
                            location=path,
                            details={
                                "expected_size": expected_size,
                                "actual_size": end - begin,
                            },
                        )

                # Check offset continuity
                offsets.sort(key=lambda x: x[0])
                last_end = 0
                for begin, end in offsets:
                    if begin != last_end:
                        result.add_issue(
                            "Tensor data offsets have gaps or overlap",
                            severity=IssueSeverity.CRITICAL,
                            location=path,
                        )
                        break
                    last_end = end

                data_size = file_size - (8 + header_len)
                if last_end != data_size:
                    result.add_issue(
                        "Tensor data does not cover entire file",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )

                # Check metadata
                metadata = header.get("__metadata__", {})
                if isinstance(metadata, dict):
                    for key, value in metadata.items():
                        if isinstance(value, str) and len(value) > 1000:
                            result.add_issue(
                                f"Metadata value for {key} is very long",
                                severity=IssueSeverity.INFO,
                                location=path,
                                why="Metadata fields over 1000 characters are unusual in model files. Long strings in metadata could contain encoded payloads, scripts, or data exfiltration attempts.",
                            )
                        if isinstance(value, str) and any(
                            s in value.lower() for s in ["import ", "#!/", "\\"]
                        ):
                            result.add_issue(
                                f"Suspicious metadata value for {key}",
                                severity=IssueSeverity.INFO,
                                location=path,
                                why="Metadata containing code-like patterns (import statements, shebangs, escape sequences) is atypical for model files and may indicate embedded scripts or injection attempts.",
                            )

                # Bytes scanned = file size
                result.bytes_scanned = file_size

        except Exception as e:
            result.add_issue(
                f"Error scanning SafeTensors file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=not result.has_errors)
        return result

    @staticmethod
    def _expected_size(dtype: Optional[str], shape: list[int]) -> Optional[int]:
        """Return expected tensor byte size from dtype and shape."""
        if dtype not in _DTYPE_SIZES:
            return None
        size = _DTYPE_SIZES[dtype]
        total = 1
        for dim in shape:
            if not isinstance(dim, int) or dim < 0:
                return None
            total *= dim
        return total * size


=====================================
FILE: ./modelaudit/scanners/tf_savedmodel_scanner.py
=====================================

import os
from pathlib import Path
from typing import Any, Optional

from modelaudit.suspicious_symbols import SUSPICIOUS_OPS

from .base import BaseScanner, IssueSeverity, ScanResult

# Try to import TensorFlow, but handle the case where it's not installed
try:
    import tensorflow as tf  # noqa: F401
    from tensorflow.core.protobuf.saved_model_pb2 import SavedModel

    HAS_TENSORFLOW = True
    SavedModelType: type = SavedModel
except ImportError:
    HAS_TENSORFLOW = False

    # Create a placeholder for type hints when TensorFlow is not available
    class SavedModel:  # type: ignore[no-redef]
        """Placeholder for SavedModel when TensorFlow is not installed"""

        meta_graphs: list = []

    SavedModelType = SavedModel


class TensorFlowSavedModelScanner(BaseScanner):
    """Scanner for TensorFlow SavedModel format"""

    name = "tf_savedmodel"
    description = "Scans TensorFlow SavedModel for suspicious operations"
    supported_extensions = [".pb", ""]  # Empty string for directories

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        # Additional scanner-specific configuration
        self.suspicious_ops = set(self.config.get("suspicious_ops", SUSPICIOUS_OPS))

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not HAS_TENSORFLOW:
            return False

        if os.path.isfile(path):
            # Handle any .pb file (protobuf format)
            ext = os.path.splitext(path)[1].lower()
            return ext == ".pb"
        if os.path.isdir(path):
            # For directory, check if saved_model.pb exists
            return os.path.exists(os.path.join(path, "saved_model.pb"))
        return False

    def scan(self, path: str) -> ScanResult:
        """Scan a TensorFlow SavedModel file or directory"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        # Store the file path for use in issue locations
        self.current_file_path = path

        # Check if TensorFlow is installed
        if not HAS_TENSORFLOW:
            result = self._create_result()
            result.add_issue(
                "TensorFlow not installed, cannot scan SavedModel. Install with "
                "'pip install modelaudit[tensorflow]'.",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"path": path},
            )
            result.finish(success=False)
            return result

        # Determine if path is file or directory
        if os.path.isfile(path):
            return self._scan_saved_model_file(path)
        if os.path.isdir(path):
            return self._scan_saved_model_directory(path)
        result = self._create_result()
        result.add_issue(
            f"Path is neither a file nor a directory: {path}",
            severity=IssueSeverity.CRITICAL,
            location=path,
            details={"path": path},
        )
        result.finish(success=False)
        return result

    def _scan_saved_model_file(self, path: str) -> ScanResult:
        """Scan a single SavedModel protobuf file"""
        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            with open(path, "rb") as f:
                content = f.read()
                result.bytes_scanned = len(content)

                saved_model = SavedModelType()
                saved_model.ParseFromString(content)

                self._analyze_saved_model(saved_model, result)

        except Exception as e:
            result.add_issue(
                f"Error scanning TF SavedModel file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _scan_saved_model_directory(self, dir_path: str) -> ScanResult:
        """Scan a SavedModel directory"""
        result = self._create_result()

        # Look for saved_model.pb in the directory
        saved_model_path = Path(dir_path) / "saved_model.pb"
        if not saved_model_path.exists():
            result.add_issue(
                "No saved_model.pb found in directory.",
                severity=IssueSeverity.CRITICAL,
                location=dir_path,
            )
            result.finish(success=False)
            return result

        # Scan the saved_model.pb file
        file_scan_result = self._scan_saved_model_file(str(saved_model_path))
        result.merge(file_scan_result)

        # Check for other suspicious files in the directory
        for root, _dirs, files in os.walk(dir_path):
            for file in files:
                file_path = Path(root) / file
                # Look for potentially suspicious Python files
                if file.endswith(".py"):
                    result.add_issue(
                        f"Python file found in SavedModel: {file}",
                        severity=IssueSeverity.WARNING,
                        location=str(file_path),
                        details={"file": file, "directory": root},
                    )

                # Check for blacklist patterns in text files
                if (
                    hasattr(self, "config")
                    and self.config
                    and "blacklist_patterns" in self.config
                ):
                    blacklist_patterns = self.config["blacklist_patterns"]
                    try:
                        # Only check text files
                        if file.endswith(
                            (
                                ".txt",
                                ".md",
                                ".json",
                                ".yaml",
                                ".yml",
                                ".py",
                                ".cfg",
                                ".conf",
                            ),
                        ):
                            with Path(file_path).open(
                                encoding="utf-8",
                                errors="ignore",
                            ) as f:
                                content = f.read()
                                for pattern in blacklist_patterns:
                                    if pattern in content:
                                        result.add_issue(
                                            f"Blacklisted pattern '{pattern}' "
                                            f"found in file {file}",
                                            severity=IssueSeverity.WARNING,
                                            location=str(file_path),
                                            details={"pattern": pattern, "file": file},
                                        )
                    except Exception:
                        # Skip files we can't read
                        pass

        result.finish(success=True)
        return result

    def _analyze_saved_model(self, saved_model: Any, result: ScanResult) -> None:
        """Analyze the saved model for suspicious operations"""
        suspicious_op_found = False
        op_counts: dict[str, int] = {}

        for meta_graph in saved_model.meta_graphs:
            graph_def = meta_graph.graph_def

            # Scan all nodes in the graph for suspicious operations
            for node in graph_def.node:
                # Count all operation types
                if node.op in op_counts:
                    op_counts[node.op] += 1
                else:
                    op_counts[node.op] = 1

                # Check if the operation is suspicious
                if node.op in self.suspicious_ops:
                    suspicious_op_found = True
                    result.add_issue(
                        f"Suspicious TensorFlow operation: {node.op}",
                        severity=IssueSeverity.CRITICAL,
                        location=f"{self.current_file_path} (node: {node.name})",
                        details={
                            "op_type": node.op,
                            "node_name": node.name,
                            "meta_graph": (
                                meta_graph.meta_info_def.tags[0]
                                if meta_graph.meta_info_def.tags
                                else "unknown"
                            ),
                        },
                    )

        # Add operation counts to metadata
        result.metadata["op_counts"] = op_counts
        result.metadata["suspicious_op_found"] = suspicious_op_found


=====================================
FILE: ./modelaudit/scanners/weight_distribution_scanner.py
=====================================

import os
import zipfile
from typing import Any, Dict, List, Optional

import numpy as np
from scipy import stats

from .base import BaseScanner, IssueSeverity, ScanResult, logger

# Try to import format-specific libraries
try:
    import h5py

    HAS_H5PY = True
except ImportError:
    HAS_H5PY = False

try:
    import torch

    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False

try:
    import tensorflow as tf  # noqa: F401

    HAS_TENSORFLOW = True
except ImportError:
    HAS_TENSORFLOW = False


class WeightDistributionScanner(BaseScanner):
    """Scanner that detects anomalous weight distributions potentially indicating trojaned models"""

    name = "weight_distribution"
    description = (
        "Analyzes weight distributions to detect potential backdoors or trojans"
    )
    supported_extensions = [
        ".pt",
        ".pth",
        ".h5",
        ".keras",
        ".hdf5",
        ".pb",
        ".onnx",
        ".safetensors",
    ]

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        # Configuration parameters
        self.z_score_threshold = self.config.get("z_score_threshold", 3.0)
        self.cosine_similarity_threshold = self.config.get(
            "cosine_similarity_threshold", 0.7
        )
        self.weight_magnitude_threshold = self.config.get(
            "weight_magnitude_threshold", 3.0
        )
        self.llm_vocab_threshold = self.config.get("llm_vocab_threshold", 10000)
        self.enable_llm_checks = self.config.get("enable_llm_checks", False)

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not os.path.isfile(path):
            return False

        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False

        # Check if we have the necessary libraries for the format
        if ext in [".pt", ".pth"] and not HAS_TORCH:
            return False
        if ext in [".h5", ".keras", ".hdf5"] and not HAS_H5PY:
            return False
        if ext == ".pb" and not HAS_TENSORFLOW:
            return False

        return True

    def scan(self, path: str) -> ScanResult:
        """Scan a model file for weight distribution anomalies"""
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            # Extract weights based on file format
            ext = os.path.splitext(path)[1].lower()

            if ext in [".pt", ".pth"]:
                weights_info = self._extract_pytorch_weights(path)
            elif ext in [".h5", ".keras", ".hdf5"]:
                weights_info = self._extract_keras_weights(path)
            elif ext == ".pb":
                weights_info = self._extract_tensorflow_weights(path)
            elif ext == ".onnx":
                weights_info = self._extract_onnx_weights(path)
            elif ext == ".safetensors":
                weights_info = self._extract_safetensors_weights(path)
            else:
                result.add_issue(
                    f"Unsupported model format for weight distribution scanner: {ext}",
                    severity=IssueSeverity.DEBUG,
                    location=path,
                )
                result.finish(success=False)
                return result

            if not weights_info:
                result.add_issue(
                    "Could not extract weights from model",
                    severity=IssueSeverity.DEBUG,
                    location=path,
                )
                result.finish(success=True)
                return result

            # Analyze the weights
            anomalies = self._analyze_weight_distributions(weights_info)

            # Add issues for any anomalies found
            for anomaly in anomalies:
                result.add_issue(
                    anomaly["description"],
                    severity=anomaly["severity"],
                    location=path,
                    details=anomaly["details"],
                    why=anomaly.get("why"),
                )

            # Add metadata
            result.metadata["layers_analyzed"] = len(weights_info)
            result.metadata["anomalies_found"] = len(anomalies)

            result.bytes_scanned = file_size

        except Exception as e:
            result.add_issue(
                f"Error analyzing weight distributions: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _extract_pytorch_weights(self, path: str) -> Dict[str, np.ndarray]:
        """Extract weights from PyTorch model files"""
        if not HAS_TORCH:
            return {}

        weights_info: Dict[str, np.ndarray] = {}

        try:
            # Load model with map_location to CPU to avoid GPU requirements
            model_data = torch.load(path, map_location=torch.device("cpu"))

            # Handle different PyTorch save formats
            if isinstance(model_data, dict):
                # State dict format
                state_dict = model_data.get("state_dict", model_data)

                # Find final layer weights (classification head)
                for key, value in state_dict.items():
                    if isinstance(value, torch.Tensor):
                        # Look for final layer patterns
                        if (
                            any(
                                pattern in key.lower()
                                for pattern in [
                                    "fc",
                                    "classifier",
                                    "head",
                                    "output",
                                    "final",
                                ]
                            )
                            and "weight" in key.lower()
                        ):
                            # PyTorch uses (out_features, in_features) but we expect (in_features, out_features)
                            weights_info[key] = value.detach().cpu().numpy().T
                        # Also include all weight tensors for comprehensive analysis
                        elif "weight" in key.lower() and len(value.shape) >= 2:
                            # PyTorch uses (out_features, in_features) but we expect (in_features, out_features)
                            weights_info[key] = value.detach().cpu().numpy().T

            elif hasattr(model_data, "state_dict"):
                # Full model format
                state_dict = model_data.state_dict()
                for key, value in state_dict.items():
                    if "weight" in key.lower() and isinstance(value, torch.Tensor):
                        # PyTorch uses (out_features, in_features) but we expect (in_features, out_features)
                        weights_info[key] = value.detach().cpu().numpy().T

        except Exception as e:
            logger.debug(f"Failed to extract weights from {path}: {e}")
            # Try loading as a zip file (newer PyTorch format)
            try:
                with zipfile.ZipFile(path, "r") as z:
                    # Look for data.pkl which contains the weights
                    if "data.pkl" in z.namelist():
                        # We can't easily extract weights from pickle without executing it
                        # This is a limitation we should document
                        pass
            except Exception as e:
                logger.debug(f"Failed to extract weights from {path}: {e}")

        return weights_info

    def _extract_keras_weights(self, path: str) -> Dict[str, np.ndarray]:
        """Extract weights from Keras/TensorFlow H5 model files"""
        if not HAS_H5PY:
            return {}

        weights_info: Dict[str, np.ndarray] = {}

        try:
            with h5py.File(path, "r") as f:
                # Navigate through the HDF5 structure to find weights
                def extract_weights(name, obj):
                    if isinstance(obj, h5py.Dataset):
                        # Check if this is a weight array
                        if "kernel" in name or "weight" in name:
                            weights_info[name] = np.array(obj)

                f.visititems(extract_weights)

        except Exception as e:
            logger.debug(f"Failed to extract weights from {path}: {e}")

        return weights_info

    def _extract_tensorflow_weights(self, path: str) -> Dict[str, np.ndarray]:
        """Extract weights from TensorFlow SavedModel files"""
        if not HAS_TENSORFLOW:
            return {}

        weights_info: Dict[str, np.ndarray] = {}

        # TensorFlow SavedModel weight extraction is complex and would require
        # loading the full graph. For now, we'll return empty.
        # This is a limitation that should be documented.

        return weights_info

    def _extract_onnx_weights(self, path: str) -> Dict[str, np.ndarray]:
        """Extract weights from ONNX model files"""
        try:
            import onnx

            HAS_ONNX = True
        except ImportError:
            HAS_ONNX = False

        if not HAS_ONNX:
            return {}

        weights_info: Dict[str, np.ndarray] = {}

        try:
            model = onnx.load(path)

            # Extract initializers (weights)
            for initializer in model.graph.initializer:
                if "weight" in initializer.name.lower():
                    weights_info[initializer.name] = onnx.numpy_helper.to_array(
                        initializer
                    )

        except Exception as e:
            logger.debug(f"Failed to extract weights from {path}: {e}")

        return weights_info

    def _extract_safetensors_weights(self, path: str) -> Dict[str, np.ndarray]:
        """Extract weights from SafeTensors files"""
        try:
            from safetensors import safe_open

            HAS_SAFETENSORS = True
        except ImportError:
            HAS_SAFETENSORS = False

        if not HAS_SAFETENSORS:
            return {}

        weights_info: Dict[str, np.ndarray] = {}

        try:
            with safe_open(path, framework="numpy") as f:
                for key in f.keys():
                    if "weight" in key.lower():
                        weights_info[key] = f.get_tensor(key)

        except Exception as e:
            logger.debug(f"Failed to extract weights from {path}: {e}")

        return weights_info

    def _analyze_weight_distributions(
        self, weights_info: Dict[str, np.ndarray]
    ) -> List[Dict[str, Any]]:
        """Analyze weight distributions for anomalies"""
        anomalies = []

        # Focus on final layer weights (classification heads)
        final_layer_candidates = {}
        for name, weights in weights_info.items():
            if (
                any(
                    pattern in name.lower()
                    for pattern in [
                        "fc",
                        "classifier",
                        "head",
                        "output",
                        "final",
                        "dense",
                    ]
                )
                and "weight" in name.lower()
            ):
                if len(weights.shape) == 2:  # Ensure it's a 2D weight matrix
                    final_layer_candidates[name] = weights

        # If no clear final layer found, analyze all 2D weight matrices
        if not final_layer_candidates:
            final_layer_candidates = {
                name: weights
                for name, weights in weights_info.items()
                if len(weights.shape) == 2
            }

        # Analyze each candidate layer
        for layer_name, weights in final_layer_candidates.items():
            layer_anomalies = self._analyze_layer_weights(layer_name, weights)
            anomalies.extend(layer_anomalies)

        return anomalies

    def _analyze_layer_weights(
        self, layer_name: str, weights: np.ndarray
    ) -> List[Dict[str, Any]]:
        """Analyze a single layer's weights for anomalies"""
        anomalies: List[Dict[str, Any]] = []

        # Weights shape is typically (input_features, output_features) for dense layers
        if len(weights.shape) != 2:
            return anomalies

        n_inputs, n_outputs = weights.shape

        # Detect if this is likely an LLM vocabulary layer or large transformer model
        is_likely_llm = (
            n_outputs > self.llm_vocab_threshold  # Large vocab layer
            or n_inputs
            > 768  # Large hidden dimensions typical of LLMs (768+ for BERT/GPT)
            or "transformer" in layer_name.lower()  # Transformer architecture
            or "attention" in layer_name.lower()  # Attention layers
            or "gpt" in layer_name.lower()  # GPT models
            or "bert" in layer_name.lower()  # BERT models
            or "llama" in layer_name.lower()  # LLaMA models
            or "t5" in layer_name.lower()  # T5 models
            or
            # GPT-style layer patterns
            layer_name.startswith("h.")  # GPT-2 style layers (h.0, h.1, etc.)
            or "mlp" in layer_name.lower()  # Multi-layer perceptron in transformers
            or "c_fc" in layer_name.lower()  # GPT-2 feed-forward layers
            or "c_attn" in layer_name.lower()  # GPT-2 attention layers
            or "c_proj" in layer_name.lower()  # GPT-2 projection layers
        )

        # Skip checks for LLMs if disabled (default behavior)
        if is_likely_llm and not self.enable_llm_checks:
            return []

        # For LLMs, we need much stricter thresholds to avoid false positives
        if is_likely_llm:
            # For LLMs, only check for extreme outliers with much higher thresholds
            z_score_threshold = max(
                8.0, self.z_score_threshold * 2.5
            )  # Much higher threshold
            outlier_percentage_threshold = 0.0001  # 0.01% for LLMs - very restrictive
        else:
            z_score_threshold = self.z_score_threshold
            outlier_percentage_threshold = 0.01  # 1% for classification models

        # 1. Check for outlier output neurons using Z-score
        output_norms = np.linalg.norm(weights, axis=0)  # L2 norm of each output neuron
        if len(output_norms) > 1:
            z_scores = np.abs(stats.zscore(output_norms))
            outlier_indices = np.where(z_scores > z_score_threshold)[0]

            # Only flag if the number of outliers is reasonable
            outlier_percentage = len(outlier_indices) / n_outputs
            if (
                len(outlier_indices) > 0
                and outlier_percentage < outlier_percentage_threshold
            ):
                anomalies.append(
                    {
                        "description": f"Layer '{layer_name}' has {len(outlier_indices)} output neurons with abnormal weight magnitudes",
                        "severity": IssueSeverity.INFO,
                        "details": {
                            "layer": layer_name,
                            "outlier_neurons": outlier_indices.tolist()[
                                :10
                            ],  # Limit to first 10
                            "total_outliers": len(outlier_indices),
                            "outlier_percentage": float(outlier_percentage * 100),
                            "z_scores": z_scores[outlier_indices].tolist()[:10],
                            "weight_norms": output_norms[outlier_indices].tolist()[:10],
                            "mean_norm": float(np.mean(output_norms)),
                            "std_norm": float(np.std(output_norms)),
                        },
                        "why": "Neurons with weight magnitudes significantly different from others in the same layer may indicate tampering, backdoors, or training anomalies. These outliers are flagged when their statistical z-score exceeds the threshold.",
                    }
                )

        # 2. Check for dissimilar weight vectors using cosine similarity
        # Only perform this check for classification models (small number of outputs)
        if 2 < n_outputs <= 1000:  # Skip for large vocabulary models
            # Compute pairwise cosine similarities
            normalized_weights = weights / (np.linalg.norm(weights, axis=0) + 1e-8)
            similarities = np.dot(normalized_weights.T, normalized_weights)

            dissimilar_neurons = []
            # Find neurons that are dissimilar to all others
            for i in range(n_outputs):
                # Get similarities to other neurons
                other_similarities = np.concatenate(
                    [similarities[i, :i], similarities[i, i + 1 :]]
                )
                max_similarity = (
                    np.max(np.abs(other_similarities))
                    if len(other_similarities) > 0
                    else 0
                )

                if max_similarity < self.cosine_similarity_threshold:
                    dissimilar_neurons.append((i, max_similarity))

            # Only flag if we have a small number of dissimilar neurons (< 5% or max 3)
            if 0 < len(dissimilar_neurons) <= max(3, int(0.05 * n_outputs)):
                for neuron_idx, max_sim in dissimilar_neurons:
                    anomalies.append(
                        {
                            "description": f"Layer '{layer_name}' output neuron {neuron_idx} has unusually dissimilar weights",
                            "severity": IssueSeverity.INFO,
                            "details": {
                                "layer": layer_name,
                                "neuron_index": neuron_idx,
                                "max_similarity_to_others": float(max_sim),
                                "weight_norm": float(output_norms[neuron_idx]),
                                "total_outputs": n_outputs,
                            },
                            "why": "Neurons with weight patterns completely unlike others in the same layer are uncommon in standard training. This dissimilarity (measured by cosine similarity below threshold) may indicate injected functionality or training irregularities.",
                        }
                    )

        # 3. Check for extreme weight values
        weight_magnitudes = np.abs(weights)
        mean_magnitude = np.mean(weight_magnitudes)
        std_magnitude = np.std(weight_magnitudes)
        threshold = mean_magnitude + self.weight_magnitude_threshold * std_magnitude

        extreme_weights = np.where(weight_magnitudes > threshold)
        if len(extreme_weights[0]) > 0:
            # Group by output neuron
            neurons_with_extreme_weights = np.unique(extreme_weights[1])
            # Only flag if very few neurons affected (< 0.1% or max 5)
            if len(neurons_with_extreme_weights) <= max(5, int(0.001 * n_outputs)):
                anomalies.append(
                    {
                        "description": f"Layer '{layer_name}' has neurons with extremely large weight values",
                        "severity": IssueSeverity.INFO,
                        "details": {
                            "layer": layer_name,
                            "affected_neurons": neurons_with_extreme_weights.tolist()[
                                :10
                            ],  # Limit list
                            "total_affected": len(neurons_with_extreme_weights),
                            "num_extreme_weights": len(extreme_weights[0]),
                            "threshold": float(threshold),
                            "max_weight": float(np.max(weight_magnitudes)),
                            "total_outputs": n_outputs,
                        },
                        "why": "Weight values that are orders of magnitude larger than typical can cause numerical instability, overflow attacks, or may encode hidden data. The threshold is set at 100 times the mean magnitude.",
                    }
                )

        return anomalies


=====================================
FILE: ./modelaudit/scanners/zip_scanner.py
=====================================

import os
import zipfile
from typing import Any, Dict, Optional

from ..utils import sanitize_archive_path
from .base import BaseScanner, IssueSeverity, ScanResult


class ZipScanner(BaseScanner):
    """Scanner for generic ZIP archive files"""

    name = "zip"
    description = "Scans ZIP archive files and their contents recursively"
    supported_extensions = [".zip", ".npz"]

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self.max_depth = self.config.get("max_zip_depth", 5)  # Prevent zip bomb attacks
        self.max_entries = self.config.get(
            "max_zip_entries", 10000
        )  # Limit number of entries

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not os.path.isfile(path):
            return False

        # Check file extension
        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False

        # Verify it's actually a zip file
        try:
            with zipfile.ZipFile(path, "r") as _:
                pass
            return True
        except zipfile.BadZipFile:
            return False
        except Exception:
            return False

    def scan(self, path: str) -> ScanResult:
        """Scan a ZIP file and its contents"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            # Store the file path for use in issue locations
            self.current_file_path = path

            # Scan the zip file recursively
            scan_result = self._scan_zip_file(path, depth=0)
            result.merge(scan_result)

        except zipfile.BadZipFile:
            result.add_issue(
                f"Not a valid zip file: {path}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"path": path},
            )
            result.finish(success=False)
            return result
        except Exception as e:
            result.add_issue(
                f"Error scanning zip file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _scan_zip_file(self, path: str, depth: int = 0) -> ScanResult:
        """Recursively scan a ZIP file and its contents"""
        result = ScanResult(scanner_name=self.name)

        # Check depth to prevent zip bomb attacks
        if depth >= self.max_depth:
            result.add_issue(
                f"Maximum ZIP nesting depth ({self.max_depth}) exceeded",
                severity=IssueSeverity.WARNING,
                location=path,
                details={"depth": depth, "max_depth": self.max_depth},
            )
            return result

        with zipfile.ZipFile(path, "r") as z:
            # Check number of entries
            if len(z.namelist()) > self.max_entries:
                result.add_issue(
                    f"ZIP file contains too many entries "
                    f"({len(z.namelist())} > {self.max_entries})",
                    severity=IssueSeverity.WARNING,
                    location=path,
                    details={
                        "entries": len(z.namelist()),
                        "max_entries": self.max_entries,
                    },
                )
                return result

            # Scan each file in the archive
            for name in z.namelist():
                info = z.getinfo(name)

                _, is_safe = sanitize_archive_path(name, "/tmp/extract")
                if not is_safe:
                    result.add_issue(
                        f"Archive entry {name} attempted path traversal outside the archive",
                        severity=IssueSeverity.CRITICAL,
                        location=f"{path}:{name}",
                        details={"entry": name},
                    )
                    continue

                # Skip directories
                if name.endswith("/"):
                    continue

                # Check compression ratio for zip bomb detection
                if info.compress_size > 0:
                    compression_ratio = info.file_size / info.compress_size
                    if compression_ratio > 100:
                        result.add_issue(
                            f"Suspicious compression ratio ({compression_ratio:.1f}x) "
                            f"in entry: {name}",
                            severity=IssueSeverity.WARNING,
                            location=f"{path}:{name}",
                            details={
                                "entry": name,
                                "compressed_size": info.compress_size,
                                "uncompressed_size": info.file_size,
                                "ratio": compression_ratio,
                            },
                        )

                # Extract and scan the file
                try:
                    max_entry_size = self.config.get(
                        "max_entry_size", 10485760
                    )  # 10 MB default
                    data = b""
                    with z.open(name) as entry:
                        while True:
                            chunk = entry.read(4096)  # Read in 4 KB chunks
                            if not chunk:
                                break
                            data += chunk
                            if len(data) > max_entry_size:
                                raise ValueError(
                                    f"ZIP entry {name} exceeds maximum size of "
                                    f"{max_entry_size} bytes"
                                )

                    # Check if it's another zip file
                    if name.lower().endswith(".zip"):
                        # Write to temporary file and scan recursively
                        import tempfile

                        with tempfile.NamedTemporaryFile(
                            suffix=".zip", delete=False
                        ) as tmp:
                            tmp.write(data)
                            tmp_path = tmp.name

                        try:
                            nested_result = self._scan_zip_file(tmp_path, depth + 1)
                            # Update locations in nested results
                            for issue in nested_result.issues:
                                if issue.location and issue.location.startswith(
                                    tmp_path
                                ):
                                    issue.location = issue.location.replace(
                                        tmp_path, f"{path}:{name}", 1
                                    )
                            result.merge(nested_result)
                        finally:
                            os.unlink(tmp_path)
                    else:
                        # Try to scan the file with appropriate scanner
                        # Write to temporary file with proper extension
                        import tempfile

                        _, ext = os.path.splitext(name)
                        with tempfile.NamedTemporaryFile(
                            suffix=ext, delete=False
                        ) as tmp:
                            tmp.write(data)
                            tmp_path = tmp.name

                        try:
                            # Import core here to avoid circular import
                            from .. import core

                            # Use core.scan_file to scan with appropriate scanner
                            file_result = core.scan_file(tmp_path, self.config)

                            # Update locations in file results
                            for issue in file_result.issues:
                                if issue.location:
                                    if issue.location.startswith(tmp_path):
                                        issue.location = issue.location.replace(
                                            tmp_path, f"{path}:{name}", 1
                                        )
                                    else:
                                        issue.location = (
                                            f"{path}:{name} {issue.location}"
                                        )
                                else:
                                    issue.location = f"{path}:{name}"

                                # Add zip entry name to details
                                if issue.details:
                                    issue.details["zip_entry"] = name
                                else:
                                    issue.details = {"zip_entry": name}

                            result.merge(file_result)

                            # If no scanner handled the file, count the bytes ourselves
                            if file_result.scanner_name == "unknown":
                                result.bytes_scanned += len(data)
                        finally:
                            os.unlink(tmp_path)

                except Exception as e:
                    result.add_issue(
                        f"Error scanning ZIP entry {name}: {str(e)}",
                        severity=IssueSeverity.WARNING,
                        location=f"{path}:{name}",
                        details={"entry": name, "exception": str(e)},
                    )

        return result


=====================================
FILE: ./modelaudit/suspicious_symbols.py
=====================================

"""
Consolidated suspicious symbols used by ModelAudit security scanners.

This module centralizes all security pattern definitions used across ModelAudit scanners
to ensure consistency, maintainability, and comprehensive threat detection.

Architecture Overview:
    The suspicious symbols system provides a centralized repository of security patterns
    that are imported by individual scanners (PickleScanner, TensorFlowScanner, etc.).
    This approach ensures:

    1. **Consistency**: All scanners use the same threat definitions
    2. **Maintainability**: Security patterns are updated in one location
    3. **Extensibility**: New patterns can be added without modifying multiple files
    4. **Performance**: Compiled regex patterns are shared across scanners

Usage Examples:
    >>> from modelaudit.suspicious_symbols import SUSPICIOUS_GLOBALS, SUSPICIOUS_OPS
    >>>
    >>> # Check if a global reference is suspicious
    >>> if "os" in SUSPICIOUS_GLOBALS:
    >>>     print("os module flagged as suspicious")
    >>>
    >>> # Check TensorFlow operations
    >>> if "PyFunc" in SUSPICIOUS_OPS:
    >>>     print("PyFunc operation flagged as suspicious")

Security Pattern Categories:
    - SUSPICIOUS_GLOBALS: Dangerous Python modules/functions (pickle files)
    - SUSPICIOUS_STRING_PATTERNS: Regex patterns for malicious code strings
    - SUSPICIOUS_OPS: Dangerous TensorFlow operations
    - SUSPICIOUS_LAYER_TYPES: Risky Keras layer types
    - SUSPICIOUS_CONFIG_PROPERTIES: Dangerous configuration keys
    - SUSPICIOUS_CONFIG_PATTERNS: Manifest file security patterns

Maintenance Guidelines:
    When adding new patterns:
    1. Document the security rationale in comments
    2. Add corresponding test cases
    3. Consider false positive impact on legitimate ML models
    4. Test against real-world model samples
    5. Update this module's docstring with new pattern categories

Performance Considerations:
    - String patterns use compiled regex for efficiency
    - Dictionary lookups are O(1) for module checks
    - Patterns are loaded once at import time
    - Consider pattern complexity for large model files

Version History:
    - v1.0: Initial consolidation from individual scanner files
    - v1.1: Added documentation and architecture overview
"""

from typing import Any

# =============================================================================
# PICKLE SECURITY PATTERNS
# =============================================================================

# Suspicious globals used by PickleScanner
# These represent Python modules/functions that can execute arbitrary code
# when encountered in pickle files during deserialization
SUSPICIOUS_GLOBALS = {
    # System interaction modules - HIGH RISK
    "os": "*",  # File system operations, command execution
    "posix": "*",  # Unix system calls (os.system equivalent)
    "sys": "*",  # Python runtime manipulation
    "subprocess": "*",  # Process spawning and control
    "runpy": "*",  # Dynamic module execution
    # Code execution functions - CRITICAL RISK
    "builtins": ["eval", "exec", "__import__"],  # Dynamic code evaluation
    "operator": ["attrgetter"],  # Attribute access bypass
    "importlib": ["import_module"],  # Dynamic module loading
    # Serialization/deserialization - MEDIUM RISK
    "pickle": ["loads", "load"],  # Recursive pickle loading
    "base64": ["b64decode", "b64encode", "decode"],  # Encoding/obfuscation
    "codecs": ["decode", "encode"],  # Text encoding manipulation
    # File system operations - HIGH RISK
    "shutil": ["rmtree", "copy", "move"],  # File system modifications
    "tempfile": ["mktemp"],  # Temporary file creation
    # Process control - CRITICAL RISK
    "pty": ["spawn"],  # Pseudo-terminal spawning
    "platform": ["system", "popen"],  # System information/execution
    # Low-level system access - CRITICAL RISK
    "ctypes": ["*"],  # C library access
    "socket": ["*"],  # Network communication
    # Serialization libraries that can execute arbitrary code - HIGH RISK
    "dill": [
        "load",
        "loads",
        "load_module",
        "load_module_asdict",
        "load_session",
    ],  # dill's load helpers can execute arbitrary code when unpickling
    # References to the private dill._dill module are also suspicious
    "dill._dill": "*",
}

# Suspicious string patterns used by PickleScanner
# Regex patterns that match potentially malicious code in string literals
SUSPICIOUS_STRING_PATTERNS = [
    # Python magic methods - can hide malicious code
    r"__[\w]+__",  # Magic methods like __reduce__, __setstate__
    # Encoding/decoding operations - often used for obfuscation
    r"base64\.b64decode",  # Base64 decoding
    # Dynamic code execution - CRITICAL
    r"eval\(",  # Dynamic expression evaluation
    r"exec\(",  # Dynamic code execution
    # System command execution - CRITICAL
    r"os\.system",  # Direct system command execution
    r"subprocess\.(?:Popen|call|check_output)",  # Process spawning
    # Dynamic imports - HIGH RISK
    r"import ",  # Import statements in strings
    r"importlib",  # Dynamic import library
    r"__import__",  # Built-in import function
    # Code construction - MEDIUM RISK
    r"lambda",  # Anonymous function creation
    # Hex encoding - possible obfuscation
    r"\\x[0-9a-fA-F]{2}",  # Hex-encoded characters
]

# =============================================================================
# TENSORFLOW/KERAS SECURITY PATTERNS
# =============================================================================

# Suspicious TensorFlow operations
# These operations can perform file I/O, code execution, or system interaction
SUSPICIOUS_OPS = {
    # File system operations - HIGH RISK
    "ReadFile",  # Read arbitrary files
    "WriteFile",  # Write arbitrary files
    "MergeV2Checkpoints",  # Checkpoint manipulation
    "Save",  # Save operations (potential overwrite)
    "SaveV2",  # SaveV2 operations
    # Code execution - CRITICAL RISK
    "PyFunc",  # Execute Python functions
    "PyCall",  # Call Python code
    # System operations - CRITICAL RISK
    "ShellExecute",  # Execute shell commands
    "ExecuteOp",  # Execute arbitrary operations
    "SystemConfig",  # System configuration access
    # Data decoding - MEDIUM RISK (can process untrusted data)
    "DecodeRaw",  # Raw data decoding
    "DecodeJpeg",  # JPEG decoding (image processing)
    "DecodePng",  # PNG decoding (image processing)
}

# Suspicious Keras layer types
# Layer types that can contain arbitrary code or complex functionality
SUSPICIOUS_LAYER_TYPES = {
    "Lambda": "Can contain arbitrary Python code",
    "TFOpLambda": "Can call TensorFlow operations",
    "Functional": "Complex layer that might hide malicious components",
    "PyFunc": "Can execute Python code",
    "CallbackLambda": "Can execute callbacks at runtime",
}

# Suspicious configuration properties for Keras models
# Configuration keys that might contain executable code
SUSPICIOUS_CONFIG_PROPERTIES = [
    "function",  # Function references
    "module",  # Module specifications
    "code",  # Code strings
    "eval",  # Evaluation expressions
    "exec",  # Execution commands
    "import",  # Import statements
    "subprocess",  # Process control
    "os.",  # Operating system calls
    "system",  # System function calls
    "popen",  # Process opening
    "shell",  # Shell access
]

# =============================================================================
# MANIFEST/CONFIGURATION SECURITY PATTERNS
# =============================================================================

# Suspicious configuration patterns for manifest files
# Grouped by threat category for easier maintenance and understanding
SUSPICIOUS_CONFIG_PATTERNS = {
    # Network access patterns - MEDIUM RISK
    # These patterns indicate potential for unauthorized network communication
    "network_access": [
        "url",  # URLs for data exfiltration
        "endpoint",  # API endpoints
        "server",  # Server specifications
        "host",  # Host configurations
        "callback",  # Callback URLs
        "webhook",  # Webhook endpoints
        "http",  # HTTP protocol usage
        "https",  # HTTPS protocol usage
        "ftp",  # FTP protocol usage
        "socket",  # Socket connections
    ],
    # File access patterns - HIGH RISK
    # These patterns indicate potential for unauthorized file system access
    "file_access": [
        "file",  # File references
        "path",  # Path specifications
        "directory",  # Directory access
        "folder",  # Folder references
        "output",  # Output file specifications
        "save",  # Save operations
        "load",  # Load operations
        "write",  # Write operations
        "read",  # Read operations
    ],
    # Code execution patterns - CRITICAL RISK
    # These patterns indicate potential for arbitrary code execution
    "execution": [
        "exec",  # Execution commands
        "eval",  # Evaluation expressions
        "execute",  # Execute operations
        "run",  # Run commands
        "command",  # Command specifications
        "script",  # Script references
        "shell",  # Shell access
        "subprocess",  # Process spawning
        "system",  # System calls
        "code",  # Code strings
    ],
    # Credential patterns - HIGH RISK (data exposure)
    # These patterns indicate potential credential exposure
    "credentials": [
        "password",  # Password fields
        "secret",  # Secret values
        "credential",  # Credential specifications
        "auth",  # Authentication data
        "authentication",  # Authentication configuration
        "api_key",  # API key storage
    ],
}

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================


def get_all_suspicious_patterns() -> dict[str, Any]:
    """
    Get all suspicious patterns for testing or analysis.

    Returns:
        Dictionary containing all pattern categories with metadata
    """
    return {
        "pickle_globals": {
            "patterns": SUSPICIOUS_GLOBALS,
            "description": "Dangerous Python modules/functions in pickle files",
            "risk_level": "HIGH",
        },
        "pickle_strings": {
            "patterns": SUSPICIOUS_STRING_PATTERNS,
            "description": "Regex patterns for malicious code strings",
            "risk_level": "MEDIUM-HIGH",
        },
        "tensorflow_ops": {
            "patterns": SUSPICIOUS_OPS,
            "description": "Dangerous TensorFlow operations",
            "risk_level": "HIGH",
        },
        "keras_layers": {
            "patterns": SUSPICIOUS_LAYER_TYPES,
            "description": "Risky Keras layer types",
            "risk_level": "MEDIUM",
        },
        "config_properties": {
            "patterns": SUSPICIOUS_CONFIG_PROPERTIES,
            "description": "Dangerous configuration keys",
            "risk_level": "MEDIUM",
        },
        "manifest_patterns": {
            "patterns": SUSPICIOUS_CONFIG_PATTERNS,
            "description": "Manifest file security patterns",
            "risk_level": "MEDIUM",
        },
    }


def validate_patterns() -> list[str]:
    """
    Validate all suspicious patterns for correctness.

    Returns:
        List of validation warnings/errors (empty list if all valid)
    """
    import re

    warnings = []

    # Validate regex patterns
    for pattern in SUSPICIOUS_STRING_PATTERNS:
        try:
            re.compile(pattern)
        except re.error as e:
            warnings.append(f"Invalid regex pattern '{pattern}': {e}")

    # Validate global patterns structure
    for module, funcs in SUSPICIOUS_GLOBALS.items():
        if not isinstance(module, str):
            warnings.append(f"Module name must be string: {module}")
        if not (funcs == "*" or isinstance(funcs, list)):
            warnings.append(f"Functions must be '*' or list for module {module}")

    return warnings


=====================================
FILE: ./modelaudit/utils/__init__.py
=====================================

import os
from pathlib import Path


def sanitize_archive_path(entry_name: str, base_dir: str) -> tuple[str, bool]:
    """Return normalized path for archive entry and whether it stays within base.

    Parameters
    ----------
    entry_name: str
        Name of the entry in the archive.
    base_dir: str
        Intended extraction directory used for normalization.

    Returns
    -------
    tuple[str, bool]
        (resolved_path, is_safe) where ``is_safe`` is ``False`` if the entry
        would escape ``base_dir`` when extracted.
    """
    base_path = Path(base_dir).resolve()
    # Normalize separators
    entry = entry_name.replace("\\", "/")
    if entry.startswith("/") or (len(entry) > 1 and entry[1] == ":"):
        # Absolute paths are not allowed
        return str((base_path / entry.lstrip("/")).resolve()), False
    entry = entry.lstrip("/")
    resolved = (base_path / entry).resolve()
    try:
        is_safe = resolved.is_relative_to(base_path)
    except AttributeError:  # Python < 3.9
        try:
            is_safe = os.path.commonpath([resolved, base_path]) == str(base_path)
        except ValueError:  # Windows: different drives
            is_safe = False
    return str(resolved), is_safe


=====================================
FILE: ./modelaudit/utils/filetype.py
=====================================

import re
from pathlib import Path


def is_zipfile(path: str) -> bool:
    """Check if file is a ZIP by reading the signature."""
    file_path = Path(path)
    if not file_path.is_file():
        return False
    try:
        with file_path.open("rb") as f:
            signature = f.read(4)
        return signature.startswith(b"PK")
    except OSError:
        return False


def read_magic_bytes(path: str, num_bytes: int = 8) -> bytes:
    with Path(path).open("rb") as f:
        return f.read(num_bytes)


def detect_file_format(path: str) -> str:
    """
    Attempt to identify the format:
    - TensorFlow SavedModel (directory with saved_model.pb)
    - Keras HDF5 (.h5 file with HDF5 magic bytes)
    - PyTorch ZIP (.pt/.pth file that's a ZIP)
    - Pickle (.pkl/.pickle or other files with pickle magic)
    - PyTorch binary (.bin files with various formats)
    - If extension indicates pickle/pt/h5/pb, etc.
    """
    file_path = Path(path)
    if file_path.is_dir():
        # We'll let the caller handle directory logic.
        # But we do a quick guess if there's a 'saved_model.pb'.
        contents = list(file_path.iterdir())
        if any(f.name == "saved_model.pb" for f in contents):
            return "tensorflow_directory"
        return "directory"

    # Single file
    size = file_path.stat().st_size
    if size < 4:
        return "unknown"

    # Read first bytes for format detection
    magic4 = read_magic_bytes(path, 4)
    magic8 = read_magic_bytes(path, 8)
    magic16 = read_magic_bytes(path, 16)

    # Check first 8 bytes for HDF5 magic
    hdf5_magic = b"\x89HDF\r\n\x1a\n"
    if magic8 == hdf5_magic:
        return "hdf5"

    ext = file_path.suffix.lower()

    # Check ZIP magic first (for .pt/.pth files that are actually zips)
    if magic4.startswith(b"PK"):
        return "zip"

    # Check pickle magic patterns
    pickle_magics = [
        b"\x80\x02",  # Protocol 2
        b"\x80\x03",  # Protocol 3
        b"\x80\x04",  # Protocol 4
        b"\x80\x05",  # Protocol 5
    ]
    if any(magic4.startswith(m) for m in pickle_magics):
        return "pickle"

    # For .bin files, do more sophisticated detection
    if ext == ".bin":
        # Check if it's a pickle file
        if any(magic4.startswith(m) for m in pickle_magics):
            return "pickle"
        # Check for safetensors format (starts with JSON header)
        if magic4[0:1] == b"{" or (size > 8 and b'"__metadata__"' in magic16):
            return "safetensors"

        # Check for ONNX format (protobuf)
        if magic4 == b"\x08\x01\x12\x00" or b"onnx" in magic16:
            return "onnx"

        # Otherwise, assume raw binary format (PyTorch weights)
        return "pytorch_binary"

    # Extension-based detection for non-.bin files
    # For .pt/.pth/.ckpt files, check if they're ZIP format first
    if ext in (".pt", ".pth", ".ckpt"):
        # These files can be either ZIP or pickle format
        if magic4.startswith(b"PK"):
            return "zip"
        # If not ZIP, assume pickle format
        return "pickle"
    if ext in (".pkl", ".pickle", ".dill"):
        return "pickle"
    if ext == ".h5":
        return "hdf5"
    if ext == ".pb":
        return "protobuf"
    if ext == ".safetensors":
        return "safetensors"
    if ext == ".onnx":
        return "onnx"
    if ext == ".npy":
        return "numpy"
    if ext == ".npz":
        return "zip"
    if ext == ".joblib":
        if magic4.startswith(b"PK"):
            return "zip"
        return "pickle"

    return "unknown"


def find_sharded_files(directory: str) -> list[str]:
    """
    Look for sharded model files like:
    pytorch_model-00001-of-00002.bin
    """
    dir_path = Path(directory)
    return sorted(
        [
            str(dir_path / fname)
            for fname in dir_path.iterdir()
            if fname.is_file()
            and re.match(r"pytorch_model-\d{5}-of-\d{5}\.bin", fname.name)
        ]
    )


=====================================
FILE: ./pyproject.toml
=====================================

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[project]
name = "modelaudit"
version = "0.1.3"
description = "Static scanning library for detecting malicious code, backdoors, and other security risks in ML model files"
authors = [{ name = "Ian Webster", email = "ian@promptfoo.dev" }]
license = { text = "MIT" }
readme = "README.md"
urls = { repository = "https://github.com/promptfoo/modelaudit", homepage = "https://github.com/promptfoo/modelaudit" }
keywords = ["ai", "ml", "security", "model-scanning", "pickle", "tensorflow", "pytorch"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Topic :: Security",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]
packages = [{ include = "modelaudit" }]
requires-python = ">=3.9,<4.0"
dependencies = [
    "click>=8.1.3",
    "yaspin>=2.3.0",
    "numpy>=1.19.0",
    "scipy>=1.5.0",
]

[tool.poetry.group.dev.dependencies]
pytest = ">=7.0.0"
coverage = "^7.2.7"
mypy = "^1.4.1"
ruff = ">=0.10.0"
types-PyYAML = "^6.0.12.20250516"
types-tensorflow = "^2.18.0.20250516"
pytest-cov = "^6.2.1"
dill = "^0.4.0"

[project.optional-dependencies]
tensorflow = ["tensorflow>=2.6"]
h5 = ["h5py>=3.1"]
pytorch = ["torch>=1.6"]
yaml = ["pyyaml>=6.0"]
safetensors = ["safetensors>=0.4.0"]
onnx = ["onnx>=1.12.0"]
joblib = ["joblib>=1.0.0"]
all = ["tensorflow", "h5py", "torch", "pyyaml", "safetensors", "onnx", "joblib"]

[project.scripts]
modelaudit = "modelaudit.cli:main"

[tool.ruff]
line-length = 88
target-version = "py39"

[tool.ruff.lint]
extend-select = ["I"]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]
"tests/**/*.py" = ["S101"]

[tool.ruff.format]
# Black-compatible formatting

[tool.mypy]
python_version = "3.9"
warn_return_any = false
warn_unused_configs = true
disallow_untyped_defs = false
disallow_incomplete_defs = false
ignore_missing_imports = true
no_implicit_optional = false


=====================================
FILE: ./setup-poetry.sh
=====================================

#!/bin/bash
set -e

# Colors for pretty output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}Setting up Poetry for modelaudit...${NC}"

# Check if Poetry is installed
if ! command -v poetry &> /dev/null; then
    echo -e "${BLUE}Poetry not found. Installing Poetry...${NC}"
    curl -sSL https://install.python-poetry.org | python3 -
    # Add Poetry to PATH (you may need to restart your terminal)
    export PATH="$HOME/.local/bin:$PATH"
else
    echo -e "${GREEN}Poetry is already installed.${NC}"
fi

# Create a new virtual environment with Poetry
echo -e "${BLUE}Creating virtual environment and installing dependencies...${NC}"
poetry install

# Optionally install dependencies for specific scanners
echo -e "${BLUE}Would you like to install optional dependencies? (y/n)${NC}"
read -n 1 choice
echo ""

if [[ $choice == "y" || $choice == "Y" ]]; then
    echo -e "${BLUE}Select extra dependencies to install:${NC}"
    echo "1) TensorFlow (for TF SavedModel scanning)"
    echo "2) h5py (for Keras H5 scanning)"
    echo "3) PyTorch (for PyTorch model scanning)"
    echo "4) All of the above"
    echo "0) None"
    read -n 1 extra_choice
    echo ""
    
    case $extra_choice in
        1)
            echo -e "${BLUE}Installing TensorFlow dependencies...${NC}"
            poetry install --extras "tensorflow"
            ;;
        2)
            echo -e "${BLUE}Installing h5py dependencies...${NC}"
            poetry install --extras "h5"
            ;;
        3)
            echo -e "${BLUE}Installing PyTorch dependencies...${NC}"
            poetry install --extras "pytorch"
            ;;
        4)
            echo -e "${BLUE}Installing all optional dependencies...${NC}"
            poetry install --extras "all"
            ;;
        *)
            echo -e "${GREEN}No extra dependencies selected.${NC}"
            ;;
    esac
fi

echo -e "${GREEN}Setup complete!${NC}"
echo -e "${BLUE}To activate the virtual environment, run:${NC}"
echo "  poetry shell"
echo -e "${BLUE}Or to run a command within the virtual environment:${NC}"
echo "  poetry run modelaudit scan /path/to/model" 


=====================================
FILE: ./setup.cfg
=====================================

[metadata]
description-file = README.md

[options]
packages = find:
include_package_data = True

[flake8]
max-line-length = 88
extend-ignore = E203, W503
exclude = .git,__pycache__,build,dist,.eggs,*.egg-info,.tox,.pytest_cache

