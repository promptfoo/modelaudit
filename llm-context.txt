# ModelAudit - LLM Context File
# Generated automatically - contains README and all source code
# This file provides complete context for AI assistants working with the ModelAudit codebase


## README.md

# ModelAudit

A security scanner for AI models. Quickly check your AIML models for potential security risks before deployment.

[![PyPI version](https://badge.fury.io/py/modelaudit.svg)](https://pypi.org/project/modelaudit/)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)

<img width="989" alt="image" src="https://www.promptfoo.dev/img/docs/modelaudit/modelaudit-result.png" />

## Table of Contents

- [ModelAudit](#modelaudit)
  - [Table of Contents](#table-of-contents)
  - [ðŸ” What It Does](#-what-it-does)
  - [ðŸš€ Quick Start](#-quick-start)
    - [Installation](#installation)
    - [Basic Usage](#basic-usage)
  - [âœ¨ Features](#-features)
    - [Core Capabilities](#core-capabilities)
    - [Reporting \& Integration](#reporting--integration)
    - [Security Detection](#security-detection)
  - [ðŸ›¡ï¸ Supported Model Formats](#ï¸-supported-model-formats)
    - [Weight Analysis](#weight-analysis)
  - [âš™ï¸ Advanced Usage](#ï¸-advanced-usage)
    - [Command Line Options](#command-line-options)
    - [Exit Codes](#exit-codes)
  - [ðŸ“‹ JSON Output Format](#-json-output-format)
  - [ðŸ”„ CI/CD Integration](#-cicd-integration)
    - [Basic Integration](#basic-integration)
    - [Platform Examples](#platform-examples)
  - [ðŸ”§ Troubleshooting](#-troubleshooting)
    - [Common Issues](#common-issues)
  - [âš ï¸ Limitations](#ï¸-limitations)
  - [ðŸ“ License](#-license)

## ðŸ” What It Does

ModelAudit scans ML model files for:

- **Malicious code execution** (e.g., `os.system` calls in pickled models)
- **Suspicious TensorFlow operations** (PyFunc, file I/O operations)
- **Potentially unsafe Keras Lambda layers** with arbitrary code execution
- **Dangerous pickle opcodes** (REDUCE, INST, OBJ, STACK_GLOBAL)
- **Custom ONNX operators** and external data integrity issues
- **Encoded payloads** and suspicious string patterns
- **Risky configurations** in model architectures
- **Suspicious patterns** in model manifests and configuration files
- **Models with blacklisted names** or content patterns
- **Malicious content in ZIP archives** including nested archives and zip bombs
- **Container-delivered models** in OCI/Docker layers and manifest files
- **GGUF/GGML file integrity** and tensor alignment validation
- **Anomalous weight patterns** that may indicate trojaned models (statistical analysis)
- **License compliance issues** including commercial use restrictions and AGPL obligations
- **Enhanced joblib/dill security** (format validation, compression bombs, embedded pickle analysis, bypass prevention)
- **NumPy array integrity issues** (malformed headers, dangerous dtypes)

## ðŸš€ Quick Start

### Installation

ModelAudit is available on [PyPI](https://pypi.org/project/modelaudit/) and requires **Python 3.9 or higher**.

**Basic installation:**

```bash
pip install modelaudit
```

**With optional dependencies for specific model formats:**

```bash
# For TensorFlow SavedModel scanning
pip install modelaudit[tensorflow]

# For Keras H5 model scanning
pip install modelaudit[h5]

# For PyTorch model scanning
pip install modelaudit[pytorch]

# For ONNX model scanning
pip install modelaudit[onnx]

# For TensorFlow Lite model scanning
pip install modelaudit[tflite]

# For YAML manifest scanning
pip install modelaudit[yaml]

# For SafeTensors model scanning
pip install modelaudit[safetensors]

# For enhanced pickle support (dill serialization with security validation)
pip install modelaudit[dill]

# For Joblib model scanning (includes scikit-learn integration)
pip install modelaudit[joblib]

# For Flax msgpack scanning
pip install modelaudit[flax]

# Install all optional dependencies
pip install modelaudit[all]
```

**Development installation:**

```bash
git clone https://github.com/promptfoo/modelaudit.git
cd modelaudit

# Using Rye (recommended)
rye sync --features all

# Or using pip
pip install -e .[all]
```

**Docker installation:**

```bash
# Pull from GitHub Container Registry
docker pull ghcr.io/promptfoo/modelaudit:latest

# Use specific variants
docker pull ghcr.io/promptfoo/modelaudit:latest-full        # All ML frameworks
docker pull ghcr.io/promptfoo/modelaudit:latest-tensorflow  # TensorFlow only

# Run with Docker
docker run --rm -v $(pwd):/data ghcr.io/promptfoo/modelaudit:latest scan /data/model.pkl
```

### Basic Usage

```bash
# Scan a single model
modelaudit scan model.pkl

# Scan an ONNX model
modelaudit scan model.onnx

# Scan multiple models (including enhanced dill/joblib support)
modelaudit scan model1.pkl model2.h5 model3.pt llama-model.gguf model4.joblib model5.dill model6.npy flax-checkpoint.msgpack

# Scan a directory
modelaudit scan ./models/

# Export results to JSON
modelaudit scan model.pkl --format json --output results.json

# Generate Software Bill of Materials (SBOM) with license information
modelaudit scan model.pkl --sbom sbom.json
```

**Example output:**

```bash
$ modelaudit scan suspicious_model.pkl

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ModelAudit Security Scanner
Scanning for potential security issues in ML model files
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Paths to scan: suspicious_model.pkl
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ“ Scanning suspicious_model.pkl

Active Scanner: pickle
Scan completed in 0.02 seconds
Files scanned: 1
Scanned 156 bytes
Issues found: 2 critical, 1 warnings

1. suspicious_model.pkl (pos 28): [CRITICAL] Suspicious module reference found: posix.system
2. suspicious_model.pkl (pos 52): [WARNING] Found REDUCE opcode - potential __reduce__ method execution

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ— Scan completed with findings
```

## âœ¨ Features

### Core Capabilities

- **Multiple Format Support**: PyTorch (.pt, .pth, .bin), TensorFlow (SavedModel, .pb), Keras (.h5, .hdf5, .keras), SafeTensors (.safetensors), GGUF/GGML (.gguf, .ggml), Pickle (.pkl, .pickle, .ckpt), Joblib (.joblib), NumPy (.npy, .npz), PMML (.pmml), ZIP archives (.zip), Manifests (.json, .yaml, .xml, etc.), Flax (.msgpack, .ckpt)
- **Automatic Format Detection**: Identifies model formats automatically
- **Deep Security Analysis**: Examines model internals, not just metadata
- **Recursive Archive Scanning**: Scans contents of ZIP files and nested archives
- **Batch Processing**: Scan multiple files and directories efficiently
- **Configurable Scanning**: Set timeouts, file size limits, custom blacklists

### Reporting & Integration

- **Multiple Output Formats**: Human-readable text and machine-readable JSON
- **SBOM Generation**: CycloneDX Software Bill of Materials with license metadata
- **Detailed Reporting**: Scan duration, files processed, bytes scanned, issue severity
- **Severity Levels**: CRITICAL, WARNING, INFO, DEBUG for flexible filtering
- **CI/CD Integration**: Clear exit codes for automated pipeline integration

### Security Detection

- **Code Execution**: Detects embedded Python code, eval/exec calls, system commands
- **Pickle Security**: Analyzes dangerous opcodes, suspicious imports, encoded payloads
- **Enhanced Dill/Joblib Analysis**: ML-aware scanning with format validation and bypass prevention
- **Model Integrity**: Checks for unexpected files, suspicious configurations
- **Archive Security**: Automatic Zip-Slip protection against directory traversal, zip bombs, malicious nested files
- **License Compliance**: Identifies commercial use restrictions, AGPL network obligations, unlicensed datasets
- **Pattern Matching**: Custom blacklist patterns for organizational policies

## ðŸ›¡ï¸ Supported Model Formats

ModelAudit provides specialized security scanners for different model formats:

| Format              | File Extensions                                                                                          | What We Check                                                                                                      |
| ------------------- | -------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
| **Pickle**          | `.pkl`, `.pickle`, `.dill`, `.bin`, `.pt`, `.pth`, `.ckpt`                                               | Malicious code execution, dangerous opcodes, suspicious imports                                                    |
| **PyTorch Zip**     | `.pt`, `.pth`                                                                                            | Embedded pickle analysis, suspicious files, custom patterns                                                        |
| **PyTorch Binary**  | `.bin`                                                                                                   | Binary tensor data analysis, embedded content                                                                      |
| **TensorFlow Lite** | `.tflite`                                                                                                | Extreme tensor shapes, custom ops, FlatBuffer integrity                                                            |
| **TensorFlow**      | SavedModel dirs, `.pb`                                                                                   | Suspicious operations, file I/O, Python execution                                                                  |
| **Keras**           | `.h5`, `.hdf5`, `.keras`                                                                                 | Lambda layers, custom objects, dangerous configurations                                                            |
| **ONNX**            | `.onnx`                                                                                                  | Custom operators, external data validation, tensor integrity                                                       |
| **SafeTensors**     | `.safetensors`                                                                                           | Metadata integrity, tensor validation                                                                              |
| **Flax**            | `.msgpack`                                                                                               | MessagePack integrity, suspicious code pattern detection, decompression bomb prevention, embedded content analysis |
| **GGUF/GGML**       | `.gguf`, `.ggml`                                                                                         | Header validation, tensor integrity, metadata security checks                                                      |
| **Joblib**          | `.joblib`                                                                                                | File format validation, compression bomb detection, embedded pickle analysis, ML-aware security filtering          |
| **NumPy**           | `.npy`, `.npz`                                                                                           | Array integrity, dangerous dtypes, dimension validation                                                            |
| **PMML**            | `.pmml`                                                                                                  | XML well-formedness, external entity checks, suspicious extensions                                                 |
| **ZIP Archives**    | `.zip`                                                                                                   | Recursive content scanning, zip bombs, directory traversal                                                         |
| **Manifests**       | `.json`, `.yaml`, `.yml`, `.xml`, `.toml`, `.ini`, `.cfg`, `.config`, `.manifest`, `.model`, `.metadata` | Suspicious keys, credential exposure, blacklisted patterns                                                         |

### Weight Analysis

ModelAudit can detect anomalous weight patterns that may indicate trojaned models using statistical analysis. This feature is disabled by default for large language models to avoid false positives.

## âš™ï¸ Advanced Usage

### Command Line Options

```bash
# Set maximum file size to scan (1GB limit)
modelaudit scan model.pkl --max-file-size 1073741824

# Stop scanning after a total of 5GB has been processed
modelaudit scan models/ --max-total-size 5368709120

# Add custom blacklist patterns
modelaudit scan model.pkl --blacklist "unsafe_model" --blacklist "malicious_net"

# Set scan timeout (5 minutes)
modelaudit scan large_model.pkl --timeout 300

# Generate SBOM with license information
modelaudit scan model.pkl --sbom sbom.json

# Verbose output for debugging
modelaudit scan model.pkl --verbose
```

### Exit Codes

ModelAudit uses different exit codes to indicate scan results:

- **0**: Success - No security issues found
- **1**: Security issues found (scan completed successfully)
- **2**: Errors occurred during scanning (e.g., file not found, scan failures)

## ðŸ“‹ JSON Output Format

When using `--format json`, ModelAudit outputs structured results:

```json
{
  "scanner_names": ["pickle"],
  "start_time": 1750168822.481906,
  "bytes_scanned": 74,
  "issues": [
    {
      "message": "Found REDUCE opcode - potential __reduce__ method execution",
      "severity": "warning",
      "location": "evil.pickle (pos 71)",
      "details": {
        "position": 71,
        "opcode": "REDUCE",
        "ml_context_confidence": 0.0
      },
      "timestamp": 1750168822.482304
    },
    {
      "message": "Suspicious module reference found: posix.system",
      "severity": "critical",
      "location": "evil.pickle (pos 28)",
      "details": {
        "module": "posix",
        "function": "system",
        "position": 28,
        "opcode": "STACK_GLOBAL",
        "ml_context_confidence": 0.0
      },
      "timestamp": 1750168822.482378
    }
  ],
  "has_errors": false,
  "files_scanned": 1,
  "duration": 0.0005328655242919922
}
```

Each issue includes a `message`, `severity` level (`critical`, `warning`, `info`, `debug`), `location`, and scanner-specific `details`.

## ðŸ”„ CI/CD Integration

ModelAudit is designed to integrate seamlessly into CI/CD pipelines with clear exit codes:

- **Exit Code 0**: No security issues found
- **Exit Code 1**: Security issues found (fails the build)
- **Exit Code 2**: Scan errors occurred (fails the build)

### Basic Integration

```bash
# Install ModelAudit
pip install modelaudit[all]

# Scan models and fail build if issues found
modelaudit scan models/ --format json --output scan-results.json

# Optional: Upload scan-results.json as build artifact
```

### Platform Examples

**GitHub Actions:**

```yaml
- name: Scan models
  run: |
    rye run modelaudit scan models/ --format json --output scan-results.json
    if [ $? -eq 1 ]; then
      echo "Security issues found in models!"
      exit 1
    fi
```

**GitLab CI:**

```yaml
model-security-scan:
  script:
    - pip install modelaudit[all]
    - modelaudit scan models/ --format json --output results.json
  artifacts:
    paths: [results.json]
```

**Jenkins:**

```groovy
sh 'pip install modelaudit[all]'
sh 'modelaudit scan models/ --format json --output results.json'
```

## ðŸ”§ Troubleshooting

### Common Issues

**Installation Problems:**

```bash
# If you get dependency conflicts
pip install --upgrade pip setuptools wheel
pip install modelaudit[all] --no-cache-dir

# Install with Rye (recommended)
rye sync --features all

# Or with pip
pip install -e .[all]

# If optional dependencies fail, install base package first
pip install modelaudit
pip install tensorflow h5py torch pyyaml safetensors onnx joblib  # Add what you need
```

**Large Models:**

```bash
# Increase file size limit and timeout for large models
modelaudit scan large_model.pt --max-file-size 5000000000 --timeout 600 --max-total-size 10000000000
```

**Testing:**

```bash
# Run all tests
rye run pytest

# Run with coverage
rye run pytest --cov=modelaudit

# Run specific test categories
rye run pytest tests/test_pickle_scanner.py -v
rye run pytest tests/test_integration.py -v

# Run tests with all optional dependencies
rye sync --features all
rye run pytest

# Run comprehensive migration test (tests everything including Docker)
./test_migration.sh
```

**Debug Mode:**

```bash
# Enable verbose output for troubleshooting
modelaudit scan model.pkl --verbose
```

**Development Commands:**

```bash
# Run linting and formatting with Ruff
rye run ruff check modelaudit/          # Check for linting issues
rye run ruff check --fix modelaudit/    # Fix auto-fixable issues
rye run ruff format modelaudit/         # Format code

# Type checking
rye run mypy modelaudit/

# Build package
rye build

# Publish (maintainers only)
rye publish
```

**Getting Help:**

- Use `--verbose` for detailed output
- Use `--format json` to see all details
- Check file permissions and format support
- Report issues on the [promptfoo GitHub repository](https://github.com/promptfoo/promptfoo/issues)

## âš ï¸ Limitations

ModelAudit is designed to find **obvious security risks** in model files, including direct code execution attempts, known dangerous patterns, malicious archive structures, and suspicious configurations.

**What it cannot detect:**

- Advanced adversarial attacks or subtle weight manipulation
- Heavily encoded/encrypted malicious payloads
- Runtime behavior that only triggers under specific conditions
- Model poisoning through careful data manipulation
- All possible license types or complex license arrangements

**Recommendations:**

- Use ModelAudit as one layer of your security strategy
- Review flagged issues manually - not all warnings indicate malicious intent
- Combine with other security practices like sandboxed execution and runtime monitoring
- Consult legal counsel for license compliance requirements beyond technical detection
- Implement automated scanning in CI/CD pipelines

## ðŸ“ License

This project is licensed under the [MIT License](https://opensource.org/licenses/MIT) - see the [LICENSE](LICENSE) file for details.



## Project Structure Overview

The ModelAudit project is organized as follows:

- `modelaudit/` - Main package directory
  - `__init__.py` - Package initialization and version info
  - `cli.py` - Command-line interface implementation
  - `core.py` - Core scanning logic and orchestration
  - `explanations.py` - Detailed explanations for findings
  - `license_checker.py` - License compliance checking
  - `sbom.py` - Software Bill of Materials generation
  - `suspicious_symbols.py` - Database of suspicious patterns
  - `auth/` - Authentication utilities
  - `name_policies/` - Model name policy enforcement
    - `blacklist.py` - Blacklist-based name filtering
  - `scanners/` - Individual format scanners
    - `base.py` - Base scanner interface and result classes
    - `*_scanner.py` - Format-specific scanners (pickle, pytorch, etc.)
  - `utils/` - Utility functions
    - `filetype.py` - File type detection and validation

## Source Code


### modelaudit/__init__.py

```python
"""ModelAudit package initialization.

This package uses the modern single-source version approach recommended by the
Python Packaging Authority (PyPA) as of 2025. The version is defined once in
pyproject.toml and accessed at runtime via importlib.metadata.
"""

from importlib.metadata import PackageNotFoundError, version

try:
    __version__ = version("modelaudit")
except PackageNotFoundError:
    # Package is not installed or in development mode
    __version__ = "unknown"

```


### modelaudit/name_policies/__init__.py

```python

```


### modelaudit/scanners/__init__.py

```python
from . import (
    base,
    flax_msgpack_scanner,
    gguf_scanner,
    joblib_scanner,
    keras_h5_scanner,
    manifest_scanner,
    numpy_scanner,
    oci_layer_scanner,
    onnx_scanner,
    pickle_scanner,
    pmml_scanner,
    pytorch_binary_scanner,
    pytorch_zip_scanner,
    safetensors_scanner,
    tf_savedmodel_scanner,
    tflite_scanner,
    weight_distribution_scanner,
    zip_scanner,
)

# Import scanner classes for direct use
from .base import BaseScanner, Issue, IssueSeverity, ScanResult
from .flax_msgpack_scanner import FlaxMsgpackScanner
from .gguf_scanner import GgufScanner
from .joblib_scanner import JoblibScanner
from .keras_h5_scanner import KerasH5Scanner
from .manifest_scanner import ManifestScanner
from .numpy_scanner import NumPyScanner
from .oci_layer_scanner import OciLayerScanner
from .onnx_scanner import OnnxScanner
from .pickle_scanner import PickleScanner
from .pmml_scanner import PmmlScanner
from .pytorch_binary_scanner import PyTorchBinaryScanner
from .pytorch_zip_scanner import PyTorchZipScanner
from .safetensors_scanner import SafeTensorsScanner
from .tf_savedmodel_scanner import TensorFlowSavedModelScanner
from .tflite_scanner import TFLiteScanner
from .weight_distribution_scanner import WeightDistributionScanner
from .zip_scanner import ZipScanner

# Create a registry of all available scanners
# Order matters - more specific scanners should come before generic ones
SCANNER_REGISTRY = [
    PickleScanner,
    PyTorchBinaryScanner,  # Must come before generic scanners for .bin files
    TensorFlowSavedModelScanner,
    KerasH5Scanner,
    OnnxScanner,
    PyTorchZipScanner,  # Must come before ZipScanner since .pt/.pth files are zip files
    GgufScanner,
    JoblibScanner,
    NumPyScanner,
    OciLayerScanner,
    ManifestScanner,
    PmmlScanner,
    WeightDistributionScanner,
    SafeTensorsScanner,
    FlaxMsgpackScanner,
    TFLiteScanner,
    ZipScanner,  # Generic zip scanner should be last
    # Add new scanners here as they are implemented
]

__all__ = [
    "BaseScanner",
    "GgufScanner",
    "Issue",
    "IssueSeverity",
    "JoblibScanner",
    "KerasH5Scanner",
    "ManifestScanner",
    "PmmlScanner",
    "NumPyScanner",
    "OciLayerScanner",
    "OnnxScanner",
    "PickleScanner",
    "PyTorchBinaryScanner",
    "PyTorchZipScanner",
    "SCANNER_REGISTRY",
    "SafeTensorsScanner",
    "FlaxMsgpackScanner",
    "TFLiteScanner",
    "ScanResult",
    "TensorFlowSavedModelScanner",
    "WeightDistributionScanner",
    "ZipScanner",
    "base",
    "gguf_scanner",
    "joblib_scanner",
    "keras_h5_scanner",
    "manifest_scanner",
    "numpy_scanner",
    "oci_layer_scanner",
    "onnx_scanner",
    "pickle_scanner",
    "pytorch_binary_scanner",
    "pytorch_zip_scanner",
    "safetensors_scanner",
    "flax_msgpack_scanner",
    "tflite_scanner",
    "pmml_scanner",
    "tf_savedmodel_scanner",
    "weight_distribution_scanner",
    "zip_scanner",
]

```


### modelaudit/utils/__init__.py

```python
import os
from pathlib import Path


def is_within_directory(base_dir: str, target: str) -> bool:
    """Return True if the target path is within the given base directory."""
    base_path = Path(base_dir).resolve()
    target_path = Path(target).resolve()
    try:
        return target_path.is_relative_to(base_path)
    except AttributeError:  # Python < 3.9
        try:
            return os.path.commonpath([target_path, base_path]) == str(base_path)
        except ValueError:
            return False


def sanitize_archive_path(entry_name: str, base_dir: str) -> tuple[str, bool]:
    """Return normalized path for archive entry and whether it stays within base.

    Parameters
    ----------
    entry_name: str
        Name of the entry in the archive.
    base_dir: str
        Intended extraction directory used for normalization.

    Returns
    -------
    tuple[str, bool]
        (resolved_path, is_safe) where ``is_safe`` is ``False`` if the entry
        would escape ``base_dir`` when extracted.
    """
    base_path = Path(base_dir).resolve()
    # Normalize separators
    entry = entry_name.replace("\\", "/")
    if entry.startswith("/") or (len(entry) > 1 and entry[1] == ":"):
        # Absolute paths are not allowed
        return str((base_path / entry.lstrip("/")).resolve()), False
    entry = entry.lstrip("/")
    resolved = (base_path / entry).resolve()
    try:
        is_safe = resolved.is_relative_to(base_path)
    except AttributeError:  # Python < 3.9
        try:
            is_safe = os.path.commonpath([resolved, base_path]) == str(base_path)
        except ValueError:  # Windows: different drives
            is_safe = False
    return str(resolved), is_safe

```


### modelaudit/core.py

```python
import builtins
import logging
import os
import time
from pathlib import Path
from threading import Lock
from typing import IO, Any, Callable, Optional, cast
from unittest.mock import patch

from modelaudit.license_checker import (
    check_commercial_use_warnings,
    collect_license_metadata,
)
from modelaudit.scanners import (
    SCANNER_REGISTRY,
    GgufScanner,
    KerasH5Scanner,
    NumPyScanner,
    OnnxScanner,
    PickleScanner,
    PyTorchBinaryScanner,
    PyTorchZipScanner,
    SafeTensorsScanner,
    TensorFlowSavedModelScanner,
    ZipScanner,
)
from modelaudit.scanners.base import IssueSeverity, ScanResult
from modelaudit.utils import is_within_directory
from modelaudit.utils.filetype import (
    detect_file_format,
    detect_format_from_extension,
    validate_file_type,
)

logger = logging.getLogger("modelaudit.core")

# Lock to ensure thread-safe monkey patching of builtins.open
_OPEN_PATCH_LOCK = Lock()


def validate_scan_config(config: dict[str, Any]) -> None:
    """Validate configuration parameters for scanning."""
    timeout = config.get("timeout")
    if timeout is not None:
        if not isinstance(timeout, int) or timeout <= 0:
            raise ValueError("timeout must be a positive integer")

    max_file_size = config.get("max_file_size")
    if max_file_size is not None:
        if not isinstance(max_file_size, int) or max_file_size < 0:
            raise ValueError("max_file_size must be a non-negative integer")

    chunk_size = config.get("chunk_size")
    if chunk_size is not None:
        if not isinstance(chunk_size, int) or chunk_size <= 0:
            raise ValueError("chunk_size must be a positive integer")


def scan_model_directory_or_file(
    path: str,
    blacklist_patterns: Optional[list[str]] = None,
    timeout: int = 300,
    max_file_size: int = 0,
    max_total_size: int = 0,
    progress_callback: Optional[Callable[[str, float], None]] = None,
    **kwargs,
) -> dict[str, Any]:
    """
    Scan a model file or directory for malicious content.

    Args:
        path: Path to the model file or directory
        blacklist_patterns: Additional blacklist patterns to check against model names
        timeout: Scan timeout in seconds
        max_file_size: Maximum file size to scan in bytes
        max_total_size: Maximum total bytes to scan across all files
        progress_callback: Optional callback function to report progress
                          (message, percentage)
        **kwargs: Additional arguments to pass to scanners

    Returns:
        Dictionary with scan results
    """
    # Start timer for timeout
    start_time = time.time()

    # Initialize results with proper type hints
    results: dict[str, Any] = {
        "start_time": start_time,
        "path": path,
        "bytes_scanned": 0,
        "issues": [],
        "success": True,
        "files_scanned": 0,
        "scanners": [],  # Track the scanners used
        "file_metadata": {},  # Per-file metadata
    }

    # Configure scan options
    config = {
        "blacklist_patterns": blacklist_patterns,
        "max_file_size": max_file_size,
        "max_total_size": max_total_size,
        "timeout": timeout,
        **kwargs,
    }

    validate_scan_config(config)

    try:
        # Check if path exists
        if not os.path.exists(path):
            raise FileNotFoundError(f"Path does not exist: {path}")

        # Check if path is readable
        if not os.access(path, os.R_OK):
            raise PermissionError(f"Path is not readable: {path}")

        # Check if path is a directory
        if os.path.isdir(path):
            if progress_callback:
                progress_callback(f"Scanning directory: {path}", 0.0)

            # Scan all files in the directory
            total_files = sum(1 for _ in Path(path).rglob("*") if _.is_file())
            processed_files = 0
            limit_reached = False

            base_dir = Path(path).resolve()
            for root, _, files in os.walk(path, followlinks=False):
                for file in files:
                    file_path = os.path.join(root, file)
                    resolved_file = Path(file_path).resolve()
                    if not is_within_directory(str(base_dir), str(resolved_file)):
                        issues_list = cast(list[dict[str, Any]], results["issues"])
                        issues_list.append(
                            {
                                "message": "Path traversal outside scanned directory",
                                "severity": IssueSeverity.CRITICAL.value,
                                "location": file_path,
                                "details": {"resolved_path": str(resolved_file)},
                            }
                        )
                        continue

                    # Check timeout
                    if time.time() - start_time > timeout:
                        raise TimeoutError(f"Scan timeout after {timeout} seconds")

                    # Update progress
                    if progress_callback and total_files > 0:
                        processed_files += 1
                        progress_callback(
                            f"Scanning file {processed_files}/{total_files}: {file}",
                            processed_files / total_files * 100,
                        )

                    # Scan the file
                    try:
                        file_result = scan_file(file_path, config)
                        # Use cast to help mypy understand the types
                        results["bytes_scanned"] = (
                            cast(int, results["bytes_scanned"])
                            + file_result.bytes_scanned
                        )
                        results["files_scanned"] = (
                            cast(int, results["files_scanned"]) + 1
                        )  # Increment file count

                        # Track scanner name
                        scanner_name = file_result.scanner_name
                        scanners_list = cast(list[str], results["scanners"])
                        if scanner_name and scanner_name not in scanners_list:
                            scanners_list.append(scanner_name)

                        # Add issues from file scan
                        issues_list = cast(list[dict[str, Any]], results["issues"])
                        for issue in file_result.issues:
                            issues_list.append(issue.to_dict())

                        # Save metadata for SBOM generation
                        file_meta = cast(dict[str, Any], results["file_metadata"])
                        # Merge scanner metadata with license metadata
                        license_metadata = collect_license_metadata(file_path)
                        combined_metadata = {**file_result.metadata, **license_metadata}
                        file_meta[file_path] = combined_metadata

                        if (
                            max_total_size > 0
                            and cast(int, results["bytes_scanned"]) > max_total_size
                        ):
                            issues_list.append(
                                {
                                    "message": f"Total scan size limit exceeded: {results['bytes_scanned']} bytes (max: {max_total_size})",
                                    "severity": IssueSeverity.WARNING.value,
                                    "location": file_path,
                                    "details": {"max_total_size": max_total_size},
                                }
                            )
                            limit_reached = True
                            break
                    except Exception as e:
                        logger.warning(f"Error scanning file {file_path}: {str(e)}")
                        # Add as an issue
                        issues_list = cast(list[dict[str, Any]], results["issues"])
                        issues_list.append(
                            {
                                "message": f"Error scanning file: {str(e)}",
                                "severity": IssueSeverity.WARNING.value,
                                "location": file_path,
                                "details": {"exception_type": type(e).__name__},
                            },
                        )
                if limit_reached:
                    break
            # Stop scanning if size limit reached
            if limit_reached:
                pass
        else:
            # Scan a single file
            if progress_callback:
                progress_callback(f"Scanning file: {path}", 0.0)

            # Get file size for progress reporting
            file_size = os.path.getsize(path)
            results["files_scanned"] = 1  # Single file scan

            # Create a wrapper for the file to report progress
            if progress_callback is not None and file_size > 0:
                original_builtins_open = builtins.open

                def progress_open(
                    file_path: str,
                    mode: str = "r",
                    *args: Any,
                    **kwargs: Any,
                ) -> IO[Any]:
                    file = original_builtins_open(file_path, mode, *args, **kwargs)
                    file_pos = 0

                    # Override read method to report progress
                    original_read = file.read

                    def progress_read(size: int = -1) -> Any:
                        nonlocal file_pos
                        data = original_read(size)
                        if isinstance(data, (str, bytes)):
                            file_pos += len(data)
                        if progress_callback is not None:
                            progress_callback(
                                f"Reading file: {os.path.basename(file_path)}",
                                min(file_pos / file_size * 100, 100),
                            )
                        return data

                    file.read = progress_read  # type: ignore[method-assign]
                    return file

                with _OPEN_PATCH_LOCK, patch("builtins.open", progress_open):
                    file_result = scan_file(path, config)
            else:
                file_result = scan_file(path, config)

            results["bytes_scanned"] = (
                cast(int, results["bytes_scanned"]) + file_result.bytes_scanned
            )

            # Track scanner name
            scanner_name = file_result.scanner_name
            scanners_list = cast(list[str], results["scanners"])
            if scanner_name and scanner_name not in scanners_list:
                scanners_list.append(scanner_name)

            # Add issues from file scan
            issues_list = cast(list[dict[str, Any]], results["issues"])
            for issue in file_result.issues:
                issues_list.append(issue.to_dict())

            # Save metadata for SBOM generation
            file_meta = cast(dict[str, Any], results["file_metadata"])
            # Merge scanner metadata with license metadata
            license_metadata = collect_license_metadata(path)
            combined_metadata = {**file_result.metadata, **license_metadata}
            file_meta[path] = combined_metadata

            if (
                max_total_size > 0
                and cast(int, results["bytes_scanned"]) > max_total_size
            ):
                issues_list.append(
                    {
                        "message": f"Total scan size limit exceeded: {results['bytes_scanned']} bytes (max: {max_total_size})",
                        "severity": IssueSeverity.WARNING.value,
                        "location": path,
                        "details": {"max_total_size": max_total_size},
                    }
                )

            if progress_callback:
                progress_callback(f"Completed scanning: {path}", 100.0)

    except Exception as e:
        logger.exception(f"Error during scan: {str(e)}")
        results["success"] = False
        issue_dict = {
            "message": f"Error during scan: {str(e)}",
            "severity": IssueSeverity.WARNING.value,
            "details": {"exception_type": type(e).__name__},
        }
        issues_list = cast(list[dict[str, Any]], results["issues"])
        issues_list.append(issue_dict)

    # Add final timing information
    results["finish_time"] = time.time()
    results["duration"] = cast(float, results["finish_time"]) - cast(
        float,
        results["start_time"],
    )

    # Add license warnings if any
    try:
        license_warnings = check_commercial_use_warnings(results)
        issues_list = cast(list[dict[str, Any]], results["issues"])
        for warning in license_warnings:
            # Convert license warnings to issues
            issue_dict = {
                "message": warning["message"],
                "severity": warning["severity"],
                "location": "",  # License warnings are generally project-wide
                "details": warning.get("details", {}),
                "type": warning["type"],
            }
            issues_list.append(issue_dict)
    except Exception as e:
        logger.warning(f"Error checking license warnings: {str(e)}")

    # Determine if there were operational scan errors vs security findings
    # has_errors should only be True for operational errors (scanner crashes,
    # file not found, etc.) not for security findings detected in models
    operational_error_indicators = [
        # Scanner execution errors
        "Error during scan",
        "Error checking file size",
        "Error scanning file",
        "Scanner crashed",
        "Scan timeout",
        # File system errors
        "Path does not exist",
        "Path is not readable",
        "Permission denied",
        "File not found",
        # Dependency/environment errors
        "not installed, cannot scan",
        "Missing dependency",
        "Import error",
        "Module not found",
        # File format/corruption errors
        "not a valid",
        "Invalid file format",
        "Corrupted file",
        "Bad file signature",
        "Unable to parse",
        # Resource/system errors
        "Out of memory",
        "Disk space",
        "Too many open files",
    ]

    issues_list = cast(list[dict[str, Any]], results["issues"])
    results["has_errors"] = (
        any(
            any(
                indicator in issue.get("message", "")
                for indicator in operational_error_indicators
            )
            for issue in issues_list
            if isinstance(issue, dict)
            and issue.get("severity") == IssueSeverity.CRITICAL.value
        )
        or not results["success"]
    )

    return results


def determine_exit_code(results: dict[str, Any]) -> int:
    """
    Determine the appropriate exit code based on scan results.

    Exit codes:
    - 0: Success, no security issues found
    - 1: Security issues found (scan completed successfully)
    - 2: Operational errors occurred during scanning

    Args:
        results: Dictionary with scan results

    Returns:
        Exit code (0, 1, or 2)
    """
    # Check for operational errors first (highest priority)
    if results.get("has_errors", False):
        return 2

    # Check for any security findings (warnings, errors, or info issues)
    issues = results.get("issues", [])
    if issues:
        # Filter out DEBUG level issues for exit code determination
        non_debug_issues = [
            issue
            for issue in issues
            if isinstance(issue, dict) and issue.get("severity") != "debug"
        ]
        if non_debug_issues:
            return 1

    # No issues found
    return 0


def scan_file(path: str, config: dict[str, Any] = None) -> ScanResult:
    """
    Scan a single file with the appropriate scanner.

    Args:
        path: Path to the file to scan
        config: Optional scanner configuration

    Returns:
        ScanResult object with the scan results
    """
    if config is None:
        config = {}
    validate_scan_config(config)

    # Check file size first
    max_file_size = config.get("max_file_size", 0)  # Default unlimited
    try:
        file_size = os.path.getsize(path)
        if max_file_size > 0 and file_size > max_file_size:
            sr = ScanResult(scanner_name="size_check")
            sr.add_issue(
                f"File too large to scan: {file_size} bytes (max: {max_file_size})",
                severity=IssueSeverity.WARNING,
                details={
                    "file_size": file_size,
                    "max_file_size": max_file_size,
                    "path": path,
                },
            )
            return sr
    except OSError as e:
        sr = ScanResult(scanner_name="error")
        sr.add_issue(
            f"Error checking file size: {e}",
            severity=IssueSeverity.WARNING,
            details={"error": str(e), "path": path},
        )
        return sr

    logger.info(f"Scanning file: {path}")

    header_format = detect_file_format(path)
    ext_format = detect_format_from_extension(path)
    ext = os.path.splitext(path)[1].lower()

    # Validate file type consistency as a security check
    file_type_valid = validate_file_type(path)
    discrepancy_msg = None

    if not file_type_valid:
        # File type validation failed - this is a security concern
        discrepancy_msg = f"File type validation failed: extension indicates {ext_format} but magic bytes indicate {header_format}. This could indicate file spoofing or corruption."
        logger.warning(discrepancy_msg)
    elif (
        header_format != ext_format
        and header_format != "unknown"
        and ext_format != "unknown"
    ):
        # Don't warn about common PyTorch .bin files that are ZIP format internally
        # This is expected behavior for torch.save()
        if not (
            ext_format == "pytorch_binary" and header_format == "zip" and ext == ".bin"
        ):
            discrepancy_msg = f"File extension indicates {ext_format} but header indicates {header_format}."
            logger.warning(discrepancy_msg)

    # Prefer scanner based on header format
    preferred_scanner: Optional[type] = None

    # Special handling for PyTorch files that are ZIP-based
    if header_format == "zip" and ext in [".pt", ".pth"]:
        preferred_scanner = PyTorchZipScanner
    elif header_format == "zip" and ext == ".bin":
        # PyTorch .bin files saved with torch.save() are ZIP format internally
        # Use PickleScanner which can handle both pickle and ZIP-based PyTorch files
        preferred_scanner = PickleScanner
    else:
        preferred_scanner = {
            "pickle": PickleScanner,
            "pytorch_binary": PyTorchBinaryScanner,
            "hdf5": KerasH5Scanner,
            "safetensors": SafeTensorsScanner,
            "tensorflow_directory": TensorFlowSavedModelScanner,
            "protobuf": TensorFlowSavedModelScanner,
            "zip": ZipScanner,
            "onnx": OnnxScanner,
            "gguf": GgufScanner,
            "ggml": GgufScanner,
            "numpy": NumPyScanner,
        }.get(header_format)

    result: Optional[ScanResult]
    if preferred_scanner and preferred_scanner.can_handle(path):
        logger.debug(
            f"Using {preferred_scanner.name} scanner for {path} based on header"
        )
        scanner = preferred_scanner(config=config)  # type: ignore[abstract]
        result = scanner.scan(path)
    else:
        result = None
        for scanner_class in SCANNER_REGISTRY:
            if scanner_class.can_handle(path):
                logger.debug(f"Using {scanner_class.name} scanner for {path}")
                scanner = scanner_class(config=config)  # type: ignore[abstract]
                result = scanner.scan(path)
                break

        if result is None:
            format_ = header_format
            sr = ScanResult(scanner_name="unknown")
            sr.add_issue(
                f"Unknown or unhandled format: {format_}",
                severity=IssueSeverity.DEBUG,
                details={"format": format_, "path": path},
            )
            result = sr

    if discrepancy_msg:
        # Determine severity based on whether it's a validation failure or just a discrepancy
        severity = IssueSeverity.WARNING if not file_type_valid else IssueSeverity.DEBUG
        result.add_issue(
            discrepancy_msg + " Using header-based detection.",
            severity=severity,
            location=path,
            details={
                "extension_format": ext_format,
                "header_format": header_format,
                "file_type_validation_failed": not file_type_valid,
            },
        )

    return result


def merge_scan_result(
    results: dict[str, Any],
    scan_result: ScanResult,
) -> dict[str, Any]:
    """
    Merge a ScanResult object into the results dictionary.

    Args:
        results: The existing results dictionary
        scan_result: The ScanResult object to merge

    Returns:
        The updated results dictionary
    """
    # Convert scan_result to dict if it's a ScanResult object
    if isinstance(scan_result, ScanResult):
        scan_dict = scan_result.to_dict()
    else:
        scan_dict = scan_result

    # Merge issues
    issues_list = cast(list[dict[str, Any]], results["issues"])
    for issue in scan_dict.get("issues", []):
        issues_list.append(issue)

    # Update bytes scanned
    results["bytes_scanned"] = cast(int, results["bytes_scanned"]) + scan_dict.get(
        "bytes_scanned",
        0,
    )

    # Update scanner info if not already set
    if "scanner_name" not in results and "scanner" in scan_dict:
        results["scanner_name"] = scan_dict["scanner"]

    # Set success to False if any scan failed
    if not scan_dict.get("success", True):
        results["success"] = False

    return results

```


### modelaudit/cli.py

```python
import json
import logging
import os
import sys
import time
from typing import Any

import click
from yaspin import yaspin
from yaspin.spinners import Spinners

from . import __version__
from .core import determine_exit_code, scan_model_directory_or_file

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger("modelaudit")


@click.group()
@click.version_option(__version__)
def cli():
    """Static scanner for ML models"""
    pass


@cli.command("scan")
@click.argument("paths", nargs=-1, type=click.Path(exists=True), required=True)
@click.option(
    "--blacklist",
    "-b",
    multiple=True,
    help="Additional blacklist patterns to check against model names",
)
@click.option(
    "--format",
    "-f",
    type=click.Choice(["text", "json"]),
    default="text",
    help="Output format [default: text]",
)
@click.option(
    "--output",
    "-o",
    type=click.Path(),
    help="Output file path (prints to stdout if not specified)",
)
@click.option(
    "--sbom",
    type=click.Path(),
    help="Write CycloneDX SBOM to the specified file",
)
@click.option(
    "--timeout",
    "-t",
    type=int,
    default=300,
    help="Scan timeout in seconds [default: 300]",
)
@click.option("--verbose", "-v", is_flag=True, help="Enable verbose output")
@click.option(
    "--max-file-size",
    type=int,
    default=0,
    help="Maximum file size to scan in bytes [default: unlimited]",
)
@click.option(
    "--max-total-size",
    type=int,
    default=0,
    help="Maximum total bytes to scan before stopping [default: unlimited]",
)
def scan_command(
    paths,
    blacklist,
    format,
    output,
    sbom,
    timeout,
    verbose,
    max_file_size,
    max_total_size,
):
    """Scan files or directories for malicious content.

    \b
    Usage:
        modelaudit scan /path/to/model1 /path/to/model2 ...

    You can specify additional blacklist patterns with ``--blacklist`` or ``-b``:

        modelaudit scan /path/to/model1 /path/to/model2 -b llama -b alpaca

    \b
    Advanced options:
        --format, -f       Output format (text or json)
        --output, -o       Write results to a file instead of stdout
        --sbom             Write CycloneDX SBOM to file
        --timeout, -t      Set scan timeout in seconds
        --verbose, -v      Show detailed information during scanning
        --max-file-size    Maximum file size to scan in bytes
        --max-total-size   Maximum total bytes to scan before stopping

    \b
    Exit codes:
        0 - Success, no security issues found
        1 - Security issues found (scan completed successfully)
        2 - Errors occurred during scanning
    """
    # Print a nice header if not in JSON mode and not writing to a file
    if format == "text" and not output:
        header = [
            "â”€" * 80,
            click.style("ModelAudit Security Scanner", fg="blue", bold=True),
            click.style(
                "Scanning for potential security issues in ML model files",
                fg="cyan",
            ),
            "â”€" * 80,
        ]
        click.echo("\n".join(header))
        click.echo(f"Paths to scan: {click.style(', '.join(paths), fg='green')}")
        if blacklist:
            click.echo(
                f"Additional blacklist patterns: "
                f"{click.style(', '.join(blacklist), fg='yellow')}",
            )
        click.echo("â”€" * 80)
        click.echo("")

    # Set logging level based on verbosity
    if verbose:
        logger.setLevel(logging.DEBUG)

    # Aggregated results
    aggregated_results = {
        "scanner_names": [],  # Track all scanner names used
        "start_time": time.time(),
        "bytes_scanned": 0,
        "issues": [],
        "has_errors": False,
        "files_scanned": 0,
    }

    # Scan each path
    for path in paths:
        # Early exit for common non-model file extensions
        if os.path.isfile(path):
            _, ext = os.path.splitext(path)
            ext = ext.lower()
            if ext in (
                ".md",
                ".txt",
                ".py",
                ".js",
                ".html",
                ".css",
                ".json",
                ".yaml",
                ".yml",
            ):
                if verbose:
                    logger.info(f"Skipping non-model file: {path}")
                click.echo(f"Skipping non-model file: {path}")
                continue

        # Show progress indicator if in text mode and not writing to a file
        spinner = None
        if format == "text" and not output:
            spinner_text = f"Scanning {click.style(path, fg='cyan')}"
            spinner = yaspin(Spinners.dots, text=spinner_text)
            spinner.start()

        # Perform the scan with the specified options
        try:
            # Define progress callback if using spinner
            progress_callback = None
            if spinner:

                def update_progress(message, percentage):
                    spinner.text = f"{message} ({percentage:.1f}%)"

                progress_callback = update_progress

            # Run the scan with progress reporting
            results = scan_model_directory_or_file(
                path,
                blacklist_patterns=list(blacklist) if blacklist else None,
                timeout=timeout,
                max_file_size=max_file_size,
                max_total_size=max_total_size,
                progress_callback=progress_callback,
            )

            # Aggregate results
            aggregated_results["bytes_scanned"] += results.get("bytes_scanned", 0)
            aggregated_results["issues"].extend(results.get("issues", []))
            aggregated_results["files_scanned"] += results.get(
                "files_scanned",
                1,
            )  # Count each file scanned
            if results.get("has_errors", False):
                aggregated_results["has_errors"] = True

            # Track scanner names
            for scanner in results.get("scanners", []):
                if (
                    scanner
                    and scanner not in aggregated_results["scanner_names"]
                    and scanner != "unknown"
                ):
                    aggregated_results["scanner_names"].append(scanner)

            # Show completion status if in text mode and not writing to a file
            if spinner:
                if results.get("issues", []):
                    # Filter out DEBUG severity issues when not in verbose mode
                    visible_issues = [
                        issue
                        for issue in results.get("issues", [])
                        if verbose
                        or not isinstance(issue, dict)
                        or issue.get("severity") != "debug"
                    ]
                    issue_count = len(visible_issues)
                    spinner.text = f"Scanned {click.style(path, fg='cyan')}"
                    if issue_count > 0:
                        spinner.ok(
                            click.style(
                                f"âœ“ Found {issue_count} issues!",
                                fg="yellow",
                                bold=True,
                            ),
                        )
                    else:
                        spinner.ok(click.style("âœ“", fg="green", bold=True))
                else:
                    spinner.text = f"Scanned {click.style(path, fg='cyan')}"
                    spinner.ok(click.style("âœ“", fg="green", bold=True))

        except Exception as e:
            # Show error if in text mode and not writing to a file
            if spinner:
                spinner.text = f"Error scanning {click.style(path, fg='cyan')}"
                spinner.fail(click.style("âœ—", fg="red", bold=True))

            logger.error(f"Error during scan of {path}: {str(e)}", exc_info=verbose)
            click.echo(f"Error scanning {path}: {str(e)}", err=True)
            aggregated_results["has_errors"] = True

    # Calculate total duration
    aggregated_results["duration"] = time.time() - aggregated_results["start_time"]

    # Format the output
    if format == "json":
        output_data = aggregated_results
        output_text = json.dumps(output_data, indent=2)
    else:
        # Text format
        output_text = format_text_output(aggregated_results, verbose)

    # Generate SBOM if requested
    if sbom:
        from .sbom import generate_sbom

        sbom_text = generate_sbom(paths, aggregated_results)
        with open(sbom, "w") as f:
            f.write(sbom_text)

    # Send output to the specified destination
    if output:
        with open(output, "w") as f:
            f.write(output_text)
        click.echo(f"Results written to {output}")
    else:
        # Add a separator line between debug output and scan results
        if format == "text":
            click.echo("\n" + "â”€" * 80)
        click.echo(output_text)

    # Exit with appropriate error code based on scan results
    exit_code = determine_exit_code(aggregated_results)
    sys.exit(exit_code)


def format_text_output(results: dict[str, Any], verbose: bool = False) -> str:
    """Format scan results as human-readable text with colors"""
    output_lines = []

    # Add summary information with styling
    if "scanner_names" in results and results["scanner_names"]:
        scanner_names = results["scanner_names"]
        if len(scanner_names) == 1:
            output_lines.append(
                click.style(
                    f"Active Scanner: {scanner_names[0]}", fg="blue", bold=True
                ),
            )
        else:
            output_lines.append(
                click.style(
                    f"Active Scanners: {', '.join(scanner_names)}",
                    fg="blue",
                    bold=True,
                ),
            )
    if "duration" in results:
        duration = results["duration"]
        if duration < 0.01:
            # For very fast scans, show more precision
            output_lines.append(
                click.style(
                    f"Scan completed in {duration:.3f} seconds",
                    fg="cyan",
                ),
            )
        else:
            output_lines.append(
                click.style(
                    f"Scan completed in {duration:.2f} seconds",
                    fg="cyan",
                ),
            )
    if "files_scanned" in results:
        output_lines.append(
            click.style(f"Files scanned: {results['files_scanned']}", fg="cyan"),
        )
    if "bytes_scanned" in results:
        # Format bytes in a more readable way
        bytes_scanned = results["bytes_scanned"]
        if bytes_scanned >= 1024 * 1024 * 1024:
            size_str = f"{bytes_scanned / (1024 * 1024 * 1024):.2f} GB"
        elif bytes_scanned >= 1024 * 1024:
            size_str = f"{bytes_scanned / (1024 * 1024):.2f} MB"
        elif bytes_scanned >= 1024:
            size_str = f"{bytes_scanned / 1024:.2f} KB"
        else:
            size_str = f"{bytes_scanned} bytes"
        output_lines.append(click.style(f"Scanned {size_str}", fg="cyan"))

    # Add issue details with color-coded severity
    issues = results.get("issues", [])
    # Filter out DEBUG severity issues when not in verbose mode
    visible_issues = [
        issue
        for issue in issues
        if verbose or not isinstance(issue, dict) or issue.get("severity") != "debug"
    ]

    if visible_issues:
        # Count issues by severity (excluding DEBUG when not in verbose mode)
        error_count = sum(
            1
            for issue in visible_issues
            if isinstance(issue, dict) and issue.get("severity") == "critical"
        )
        warning_count = sum(
            1
            for issue in visible_issues
            if isinstance(issue, dict) and issue.get("severity") == "warning"
        )
        info_count = sum(
            1
            for issue in visible_issues
            if isinstance(issue, dict) and issue.get("severity") == "info"
        )
        debug_count = sum(
            1
            for issue in issues
            if isinstance(issue, dict) and issue.get("severity") == "debug"
        )

        # Only show debug count in verbose mode
        issue_summary = []
        if error_count:
            issue_summary.append(
                click.style(f"{error_count} critical", fg="red", bold=True),
            )
        if warning_count:
            issue_summary.append(click.style(f"{warning_count} warnings", fg="yellow"))
        if info_count:
            issue_summary.append(click.style(f"{info_count} info", fg="blue"))
        if verbose and debug_count:
            issue_summary.append(click.style(f"{debug_count} debug", fg="cyan"))

        if issue_summary:
            output_lines.append(
                click.style("Issues found: ", fg="white") + ", ".join(issue_summary),
            )

        # Only display visible issues
        for i, issue in enumerate(visible_issues, 1):
            severity = issue.get("severity", "warning").lower()

            # Skip debug issues if verbose is not enabled
            if severity == "debug" and not verbose:
                continue

            message = issue.get("message", "Unknown issue")
            location = issue.get("location", "")

            # Color-code based on severity
            if severity == "critical":
                severity_style = click.style("[CRITICAL]", fg="red", bold=True)
            elif severity == "warning":
                severity_style = click.style("[WARNING]", fg="yellow")
            elif severity == "info":
                severity_style = click.style("[INFO]", fg="blue")
            elif severity == "debug":
                severity_style = click.style("[DEBUG]", fg="bright_black")

            # Format the issue line
            issue_num = click.style(f"{i}.", fg="white", bold=True)
            if location:
                location_str = click.style(f"{location}", fg="cyan", bold=True)
                output_lines.append(
                    f"{issue_num} {location_str}: {severity_style} {message}",
                )
            else:
                output_lines.append(f"{issue_num} {severity_style} {message}")

            # Add "Why" explanation if available
            why = issue.get("why")
            if why:
                # Indent the explanation and style it
                why_label = click.style("   Why:", fg="magenta", bold=True)
                why_text = click.style(f" {why}", fg="bright_white")
                output_lines.append(f"{why_label}{why_text}")

            # Add a small separator between issues for readability
            if i < len(visible_issues):
                output_lines.append("")
    else:
        output_lines.append(
            "\n" + click.style("âœ“ No issues found", fg="green", bold=True),
        )

    # Add a footer
    output_lines.append("â”€" * 80)
    if visible_issues:
        if any(
            isinstance(issue, dict) and issue.get("severity") == "critical"
            for issue in visible_issues
        ):
            status = click.style("âœ— Scan completed with findings", fg="red", bold=True)
        elif any(
            isinstance(issue, dict) and issue.get("severity") == "warning"
            for issue in visible_issues
        ):
            status = click.style(
                "âš  Scan completed with warnings",
                fg="yellow",
                bold=True,
            )
        else:
            # Only info/debug issues
            status = click.style("âœ“ Scan completed successfully", fg="green", bold=True)
    else:
        status = click.style("âœ“ Scan completed successfully", fg="green", bold=True)
    output_lines.append(status)

    return "\n".join(output_lines)


def main():
    cli()

```


### modelaudit/scanners/base.py

```python
import json
import logging
import os
import time
from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, ClassVar, Optional

# Configure logging
logger = logging.getLogger("modelaudit.scanners")


class IssueSeverity(Enum):
    """Enum for issue severity levels"""

    DEBUG = "debug"  # Debug information
    INFO = "info"  # Informational, not a security concern
    WARNING = "warning"  # Potential issue, needs review
    CRITICAL = "critical"  # Definite security concern


class Issue:
    """Represents a single issue found during scanning"""

    def __init__(
        self,
        message: str,
        severity: IssueSeverity = IssueSeverity.WARNING,
        location: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
        why: Optional[str] = None,
    ):
        self.message = message
        self.severity = severity
        self.location = location  # File position, line number, etc.
        self.details = details or {}
        self.why = why  # Explanation of why this is a security concern
        self.timestamp = time.time()

    def to_dict(self) -> dict[str, Any]:
        """Convert the issue to a dictionary for serialization"""
        result = {
            "message": self.message,
            "severity": self.severity.value,
            "location": self.location,
            "details": self.details,
            "timestamp": self.timestamp,
        }
        if self.why:
            result["why"] = self.why
        return result

    def __str__(self) -> str:
        """String representation of the issue"""
        prefix = f"[{self.severity.value.upper()}]"
        if self.location:
            prefix += f" ({self.location})"
        return f"{prefix}: {self.message}"


class ScanResult:
    """Collects and manages issues found during scanning"""

    def __init__(self, scanner_name: str = "unknown"):
        self.scanner_name = scanner_name
        self.issues: list[Issue] = []
        self.start_time = time.time()
        self.end_time: Optional[float] = None
        self.bytes_scanned: int = 0
        self.success: bool = True
        self.metadata: dict[str, Any] = {}

    def add_issue(
        self,
        message: str,
        severity: IssueSeverity = IssueSeverity.WARNING,
        location: Optional[str] = None,
        details: Optional[dict[str, Any]] = None,
        why: Optional[str] = None,
    ) -> None:
        """Add an issue to the result"""
        issue = Issue(message, severity, location, details, why)
        self.issues.append(issue)
        log_level = (
            logging.CRITICAL
            if severity == IssueSeverity.CRITICAL
            else (
                logging.WARNING
                if severity == IssueSeverity.WARNING
                else (logging.INFO if severity == IssueSeverity.INFO else logging.DEBUG)
            )
        )
        logger.log(log_level, str(issue))

    def merge(self, other: "ScanResult") -> None:
        """Merge another scan result into this one"""
        self.issues.extend(other.issues)
        self.bytes_scanned += other.bytes_scanned
        # Merge metadata dictionaries
        for key, value in other.metadata.items():
            if (
                key in self.metadata
                and isinstance(self.metadata[key], dict)
                and isinstance(value, dict)
            ):
                self.metadata[key].update(value)
            else:
                self.metadata[key] = value

    def finish(self, success: bool = True) -> None:
        """Mark the scan as finished"""
        self.end_time = time.time()
        self.success = success

    @property
    def duration(self) -> float:
        """Return the duration of the scan in seconds"""
        if self.end_time is None:
            return time.time() - self.start_time
        return self.end_time - self.start_time

    @property
    def has_errors(self) -> bool:
        """Return True if there are any critical-level issues"""
        return any(issue.severity == IssueSeverity.CRITICAL for issue in self.issues)

    @property
    def has_warnings(self) -> bool:
        """Return True if there are any warning-level issues"""
        return any(issue.severity == IssueSeverity.WARNING for issue in self.issues)

    def to_dict(self) -> dict[str, Any]:
        """Convert the scan result to a dictionary for serialization"""
        return {
            "scanner": self.scanner_name,
            "success": self.success,
            "duration": self.duration,
            "bytes_scanned": self.bytes_scanned,
            "issues": [issue.to_dict() for issue in self.issues],
            "metadata": self.metadata,
            "has_errors": self.has_errors,
            "has_warnings": self.has_warnings,
        }

    def to_json(self, indent: int = 2) -> str:
        """Convert the scan result to a JSON string"""
        return json.dumps(self.to_dict(), indent=indent)

    def summary(self) -> str:
        """Return a human-readable summary of the scan result"""
        error_count = sum(
            1 for issue in self.issues if issue.severity == IssueSeverity.CRITICAL
        )
        warning_count = sum(
            1 for issue in self.issues if issue.severity == IssueSeverity.WARNING
        )
        info_count = sum(
            1 for issue in self.issues if issue.severity == IssueSeverity.INFO
        )

        result = []
        result.append(f"Scan completed in {self.duration:.2f}s")
        result.append(
            f"Scanned {self.bytes_scanned} bytes with scanner '{self.scanner_name}'",
        )
        result.append(
            f"Found {len(self.issues)} issues ({error_count} critical, "
            f"{warning_count} warnings, {info_count} info)",
        )

        # If there are any issues, show them
        if self.issues:
            result.append("\nIssues:")
            for issue in self.issues:
                result.append(f"  {issue}")

        return "\n".join(result)

    def __str__(self) -> str:
        """String representation of the scan result"""
        return self.summary()


class BaseScanner(ABC):
    """Base class for all scanners"""

    name: ClassVar[str] = "base"
    description: ClassVar[str] = "Base scanner class"
    supported_extensions: ClassVar[list[str]] = []

    def __init__(self, config: Optional[dict[str, Any]] = None):
        """Initialize the scanner with configuration"""
        self.config = config or {}
        self.timeout = self.config.get("timeout", 300)  # Default 5 minutes
        self.current_file_path = ""  # Track the current file being scanned
        self.chunk_size = self.config.get(
            "chunk_size",
            10 * 1024 * 1024,
        )  # Default: 10MB chunks
        self._path_validation_result: Optional[ScanResult] = None

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Return True if this scanner can handle the file at the given path"""
        # Basic implementation checks file extension
        # Subclasses should override for more sophisticated detection
        file_ext = os.path.splitext(path)[1].lower()
        return file_ext in cls.supported_extensions

    @abstractmethod
    def scan(self, path: str) -> ScanResult:
        """Scan the model file or directory at the given path"""
        pass

    def _create_result(self) -> ScanResult:
        """Create a new ScanResult instance for this scanner"""
        result = ScanResult(scanner_name=self.name)

        # Automatically merge any stored path validation warnings
        if hasattr(self, "_path_validation_result") and self._path_validation_result:
            result.merge(self._path_validation_result)
            # Clear the stored result to avoid duplicate merging
            self._path_validation_result = None

        return result

    def _check_path(self, path: str) -> Optional[ScanResult]:
        """Common path checks and validation

        Returns:
            None if path is valid or has only warnings, otherwise a ScanResult with critical errors
        """
        result = self._create_result()

        # Check if path exists
        if not os.path.exists(path):
            result.add_issue(
                f"Path does not exist: {path}",
                severity=IssueSeverity.CRITICAL,
                details={"path": path},
            )
            result.finish(success=False)
            return result

        # Check if path is readable
        if not os.access(path, os.R_OK):
            result.add_issue(
                f"Path is not readable: {path}",
                severity=IssueSeverity.CRITICAL,
                details={"path": path},
            )
            result.finish(success=False)
            return result

        # Validate file type consistency for files (security check)
        if os.path.isfile(path):
            try:
                from modelaudit.utils.filetype import (
                    detect_file_format_from_magic,
                    detect_format_from_extension,
                    validate_file_type,
                )

                if not validate_file_type(path):
                    header_format = detect_file_format_from_magic(path)
                    ext_format = detect_format_from_extension(path)
                    result.add_issue(
                        f"File type validation failed: extension indicates {ext_format} but magic bytes indicate {header_format}. This could indicate file spoofing, corruption, or a security threat.",
                        severity=IssueSeverity.WARNING,  # Warning level to allow scan to continue
                        location=path,
                        details={
                            "header_format": header_format,
                            "extension_format": ext_format,
                            "security_check": "file_type_validation",
                        },
                    )
            except Exception as e:
                # Don't fail the scan if file type validation has an error
                result.add_issue(
                    f"File type validation error: {str(e)}",
                    severity=IssueSeverity.DEBUG,
                    location=path,
                    details={"exception": str(e), "exception_type": type(e).__name__},
                )

        # Store validation warnings for the scanner to merge later
        self._path_validation_result = result if result.issues else None

        # Only return result for CRITICAL issues that should stop the scan
        critical_issues = [
            issue for issue in result.issues if issue.severity == IssueSeverity.CRITICAL
        ]
        if critical_issues:
            return result

        return None  # Path is valid, scanner should continue and merge warnings if any

    def get_file_size(self, path: str) -> int:
        """Get the size of a file in bytes."""
        try:
            return os.path.getsize(path) if os.path.isfile(path) else 0
        except OSError:
            # If the file becomes inaccessible during scanning, treat the size
            # as zero rather than raising an exception.
            return 0

```


### modelaudit/utils/filetype.py

```python
import re
from pathlib import Path


def is_zipfile(path: str) -> bool:
    """Check if file is a ZIP by reading the signature."""
    file_path = Path(path)
    if not file_path.is_file():
        return False
    try:
        with file_path.open("rb") as f:
            signature = f.read(4)
        return signature.startswith(b"PK")
    except OSError:
        return False


def read_magic_bytes(path: str, num_bytes: int = 8) -> bytes:
    with Path(path).open("rb") as f:
        return f.read(num_bytes)


def detect_file_format_from_magic(path: str) -> str:
    """Detect file format solely from magic bytes."""
    file_path = Path(path)
    if file_path.is_dir():
        if (file_path / "saved_model.pb").exists():
            return "tensorflow_directory"
        return "directory"

    if not file_path.is_file():
        return "unknown"

    size = file_path.stat().st_size
    if size < 4:
        return "unknown"

    magic4 = read_magic_bytes(path, 4)
    magic8 = read_magic_bytes(path, 8)
    magic16 = read_magic_bytes(path, 16)

    hdf5_magic = b"\x89HDF\r\n\x1a\n"
    if magic8 == hdf5_magic:
        return "hdf5"

    # NumPy magic check
    numpy_magic = b"\x93NUMPY"
    if magic8.startswith(numpy_magic):
        return "numpy"

    if magic4 == b"GGUF":
        return "gguf"
    if magic4 == b"GGML":
        return "ggml"

    if magic4.startswith(b"PK"):
        return "zip"

    pickle_magics = [b"\x80\x02", b"\x80\x03", b"\x80\x04", b"\x80\x05"]
    if any(magic4.startswith(m) for m in pickle_magics):
        return "pickle"

    # SafeTensors format check: 8-byte length header + JSON metadata
    if size >= 12:  # Minimum: 8 bytes length + some JSON
        try:
            import struct

            # Read 8 bytes as little-endian u64 for JSON length
            json_length = struct.unpack("<Q", magic8)[0]
            # Sanity check: JSON length should be reasonable
            if 0 < json_length < size and json_length < 1024 * 1024:  # Max 1MB JSON
                # Read some bytes after the header to check for JSON
                with open(path, "rb") as f:
                    f.seek(8)  # Skip the 8-byte header
                    json_start = f.read(min(32, json_length))
                    if json_start.startswith(b"{") and b'"' in json_start:
                        return "safetensors"
        except (struct.error, OSError):
            pass

    # Fallback: check if it starts with JSON (for old safetensors or other JSON formats)
    if magic4[0:1] == b"{" or (size > 8 and b'"__metadata__"' in magic16):
        return "safetensors"

    if magic4 == b"\x08\x01\x12\x00" or b"onnx" in magic16:
        return "onnx"

    return "unknown"


def detect_file_format(path: str) -> str:
    """
    Attempt to identify the format:
    - TensorFlow SavedModel (directory with saved_model.pb)
    - Keras HDF5 (.h5 file with HDF5 magic bytes)
    - PyTorch ZIP (.pt/.pth file that's a ZIP)
    - Pickle (.pkl/.pickle or other files with pickle magic)
    - PyTorch binary (.bin files with various formats)
    - GGUF/GGML files with magic bytes
    - If extension indicates pickle/pt/h5/pb, etc.
    """
    file_path = Path(path)
    if file_path.is_dir():
        # We'll let the caller handle directory logic.
        # But we do a quick guess if there's a 'saved_model.pb'.
        contents = list(file_path.iterdir())
        if any(f.name == "saved_model.pb" for f in contents):
            return "tensorflow_directory"
        return "directory"

    # Single file
    size = file_path.stat().st_size
    if size < 4:
        return "unknown"

    # Read first bytes for format detection
    magic4 = read_magic_bytes(path, 4)
    magic8 = read_magic_bytes(path, 8)
    magic16 = read_magic_bytes(path, 16)

    # Check first 8 bytes for HDF5 magic
    hdf5_magic = b"\x89HDF\r\n\x1a\n"
    if magic8 == hdf5_magic:
        return "hdf5"

    # Check for GGUF/GGML magic bytes
    if magic4 == b"GGUF":
        return "gguf"
    if magic4 == b"GGML":
        return "ggml"

    ext = file_path.suffix.lower()

    # Check ZIP magic first (for .pt/.pth files that are actually zips)
    if magic4.startswith(b"PK"):
        return "zip"

    # Check pickle magic patterns
    pickle_magics = [
        b"\x80\x02",  # Protocol 2
        b"\x80\x03",  # Protocol 3
        b"\x80\x04",  # Protocol 4
        b"\x80\x05",  # Protocol 5
    ]
    if any(magic4.startswith(m) for m in pickle_magics):
        return "pickle"

    # For .bin files, do more sophisticated detection
    if ext == ".bin":
        # Check if it's a pickle file
        if any(magic4.startswith(m) for m in pickle_magics):
            return "pickle"
        # Check for safetensors format (starts with JSON header)
        if magic4[0:1] == b"{" or (size > 8 and b'"__metadata__"' in magic16):
            return "safetensors"

        # Check for ONNX format (protobuf)
        if magic4 == b"\x08\x01\x12\x00" or b"onnx" in magic16:
            return "onnx"

        # Otherwise, assume raw binary format (PyTorch weights)
        return "pytorch_binary"

    # Extension-based detection for non-.bin files
    # For .pt/.pth/.ckpt files, check if they're ZIP format first
    if ext in (".pt", ".pth", ".ckpt"):
        # These files can be either ZIP or pickle format
        if magic4.startswith(b"PK"):
            return "zip"
        # If not ZIP, assume pickle format
        return "pickle"
    if ext in (".pkl", ".pickle", ".dill"):
        return "pickle"
    if ext == ".h5":
        return "hdf5"
    if ext == ".pb":
        return "protobuf"
    if ext == ".tflite":
        return "tflite"
    if ext == ".safetensors":
        return "safetensors"
    if ext == ".msgpack":
        return "flax_msgpack"
    if ext == ".onnx":
        return "onnx"
    if ext in (".gguf", ".ggml"):
        # Check magic bytes first for accuracy
        if magic4 == b"GGUF":
            return "gguf"
        elif magic4 == b"GGML":
            return "ggml"
        # Fall back to extension-based detection
        return "gguf" if ext == ".gguf" else "ggml"
    if ext == ".npy":
        return "numpy"
    if ext == ".npz":
        return "zip"
    if ext == ".joblib":
        if magic4.startswith(b"PK"):
            return "zip"
        return "pickle"

    return "unknown"


def find_sharded_files(directory: str) -> list[str]:
    """
    Look for sharded model files like:
    pytorch_model-00001-of-00002.bin
    """
    dir_path = Path(directory)
    return sorted(
        [
            str(dir_path / fname)
            for fname in dir_path.iterdir()
            if fname.is_file()
            and re.match(r"pytorch_model-\d{5}-of-\d{5}\.bin", fname.name)
        ]
    )


EXTENSION_FORMAT_MAP = {
    ".pt": "pickle",
    ".pth": "pickle",
    ".ckpt": "pickle",
    ".pkl": "pickle",
    ".pickle": "pickle",
    ".dill": "pickle",
    ".h5": "hdf5",
    ".hdf5": "hdf5",
    ".keras": "hdf5",
    ".pb": "protobuf",
    ".safetensors": "safetensors",
    ".onnx": "onnx",
    ".bin": "pytorch_binary",
    ".zip": "zip",
    ".gguf": "gguf",
    ".ggml": "ggml",
    ".npy": "numpy",
    ".npz": "zip",
    ".joblib": "pickle",  # joblib can be either zip or pickle format
}


def detect_format_from_extension(path: str) -> str:
    """Return a format string based solely on the file extension."""
    file_path = Path(path)
    if file_path.is_dir():
        if (file_path / "saved_model.pb").exists():
            return "tensorflow_directory"
        return "directory"
    return EXTENSION_FORMAT_MAP.get(file_path.suffix.lower(), "unknown")


def validate_file_type(path: str) -> bool:
    """Validate that a file's magic bytes match its extension-based format."""
    try:
        header_format = detect_file_format_from_magic(path)
        ext_format = detect_format_from_extension(path)

        # If extension format is unknown, we can't validate - assume valid
        if ext_format == "unknown":
            return True

        # Small files (< 4 bytes) are always valid - can't determine magic bytes reliably
        file_path = Path(path)
        if file_path.is_file() and file_path.stat().st_size < 4:
            return True

        # Handle special cases where different formats are compatible first
        # before doing the unknown header check

        # Pickle files can be stored in various ways
        if ext_format == "pickle" and header_format in {"pickle", "zip"}:
            return True

        # PyTorch binary files are flexible in format
        if ext_format == "pytorch_binary" and header_format in {
            "pytorch_binary",
            "pickle",
            "zip",
            "unknown",  # .bin files can contain arbitrary binary data
        }:
            return True

        # TensorFlow protobuf files (.pb extension)
        if ext_format == "protobuf" and header_format in {"protobuf", "unknown"}:
            return True

        # ZIP files can have various extensions (.zip, .pt, .pth, .ckpt when they're torch.save() files)
        if header_format == "zip" and ext_format in {"zip", "pickle", "pytorch_binary"}:
            return True

        # HDF5 files should always match
        if ext_format == "hdf5":
            return header_format == "hdf5"

        # SafeTensors files should always match
        if ext_format == "safetensors":
            return header_format == "safetensors"

        # GGUF/GGML files should match their format
        if ext_format in {"gguf", "ggml"}:
            return header_format == ext_format

        # ONNX files (Protocol Buffer format - difficult to detect reliably)
        if ext_format == "onnx":
            return header_format in {"onnx", "unknown"}

        # NumPy files should match
        if ext_format == "numpy":
            return header_format == "numpy"

        # Flax msgpack files (less strict validation)
        if ext_format == "flax_msgpack":
            return True  # Hard to validate msgpack format reliably

        # TensorFlow directories are special case
        if ext_format == "tensorflow_directory":
            return header_format == "tensorflow_directory"

        # TensorFlow Lite files
        if ext_format == "tflite":
            return True  # TFLite format can be complex to validate

        # If header format is unknown but extension is known, this might be suspicious
        # unless the file is very small or empty (checked after format-specific rules)
        if header_format == "unknown":
            file_path = Path(path)
            if file_path.is_file() and file_path.stat().st_size >= 4:
                return False
            return True  # Small files are acceptable

        # Default: exact match required
        return header_format == ext_format

    except Exception:
        # If validation fails due to error, assume valid to avoid breaking scans
        return True

```


### modelaudit/explanations.py

```python
"""
Security issue explanations for ModelAudit.

This module provides centralized, security-team-friendly explanations
for common security issues found in ML model files.
"""

from typing import Optional

# Common explanations for dangerous imports and modules
DANGEROUS_IMPORTS = {
    "os": "The 'os' module provides direct access to operating system functions, allowing execution of arbitrary system commands, file system manipulation, and environment variable access. Malicious models can use this to compromise the host system, steal data, or install malware.",
    "posix": "The 'posix' module provides direct access to POSIX system calls on Unix-like systems. Like the 'os' module, it can execute arbitrary system commands and manipulate the file system. The 'posix.system' function is equivalent to 'os.system' and poses the same security risks.",
    "sys": "The 'sys' module provides access to interpreter internals and system-specific parameters. It can be used to modify the Python runtime, access command-line arguments, or manipulate the module import system to load malicious code.",
    "subprocess": "The 'subprocess' module allows spawning new processes and executing system commands. This is a critical security risk as it enables arbitrary command execution on the host system.",
    "eval": "The 'eval' function executes arbitrary Python code from strings. This is extremely dangerous as it allows dynamic code execution, potentially running any malicious code embedded in the model.",
    "exec": "The 'exec' function executes arbitrary Python statements from strings. Like eval, this enables unrestricted code execution and is a severe security risk.",
    "__import__": "The '__import__' function dynamically imports modules at runtime. Attackers can use this to load malicious modules or bypass import restrictions.",
    "importlib": "The 'importlib' module provides programmatic module importing capabilities. It can be used to dynamically load malicious code or bypass security controls.",
    "pickle": "Nested pickle operations (pickle.load/loads within a pickle) can indicate attempts to obfuscate malicious payloads or create multi-stage attacks.",
    "base64": "Base64 encoding/decoding functions are often used to obfuscate malicious payloads, making them harder to detect through static analysis.",
    "socket": "The 'socket' module enables network communication. Malicious models can use this to exfiltrate data, download additional payloads, or establish command & control channels.",
    "ctypes": "The 'ctypes' module provides low-level system access through foreign function interfaces. It can bypass Python's safety features and directly manipulate memory or call system libraries.",
    "pty": "The 'pty' module provides pseudo-terminal utilities. The 'spawn' function can be used to create interactive shells, potentially giving attackers remote access.",
    "platform": "Functions like 'platform.system' or 'platform.popen' can be used for system reconnaissance or command execution.",
    "shutil": "The 'shutil' module provides high-level file operations. Functions like 'rmtree' can recursively delete directories, potentially causing data loss.",
    "tempfile": "Unsafe temp file creation (like 'mktemp') can lead to race conditions and security vulnerabilities.",
    "runpy": "The 'runpy' module executes Python modules as scripts, potentially running malicious code embedded in the model.",
    "operator.attrgetter": "The 'attrgetter' function can be used to access object attributes dynamically, potentially bypassing access controls or reaching sensitive data.",
    "builtins": "Direct access to builtin functions can be used to bypass restrictions or access dangerous functionality like eval/exec.",
    "dill": "The 'dill' module extends pickle's capabilities to serialize almost any Python object, including lambda functions and code objects. This significantly increases the attack surface for code execution.",
}

# Explanations for dangerous pickle opcodes
DANGEROUS_OPCODES = {
    "REDUCE": "The REDUCE opcode calls a callable with arguments, effectively executing arbitrary Python functions. This is the primary mechanism for pickle-based code execution attacks through __reduce__ methods.",
    "INST": "The INST opcode instantiates objects by calling their class constructor. Malicious classes can execute code in __init__ methods during unpickling.",
    "OBJ": "The OBJ opcode creates class instances. Like INST, this can trigger code execution through object initialization.",
    "NEWOBJ": "The NEWOBJ opcode creates new-style class instances. It can execute initialization code and is commonly used in pickle exploits.",
    "NEWOBJ_EX": "The NEWOBJ_EX opcode is an extended version of NEWOBJ with additional capabilities for creating objects, potentially executing initialization code.",
    "BUILD": "The BUILD opcode updates object state and can trigger code execution through __setstate__ or __setattr__ methods.",
    "STACK_GLOBAL": "The STACK_GLOBAL opcode imports modules and retrieves attributes dynamically. Outside ML contexts, this often indicates attempts to access dangerous functionality.",
    "GLOBAL": "The GLOBAL opcode imports and accesses module attributes. When referencing dangerous modules, this indicates potential security risks.",
}

# Explanations for specific patterns and behaviors
PATTERN_EXPLANATIONS = {
    "base64_payload": "Base64-encoded data in models often conceals malicious payloads. Legitimate ML models rarely need encoded strings unless handling specific data formats.",
    "hex_encoded": "Hexadecimal-encoded strings (\\x00 format) can hide malicious code or data. This obfuscation technique is commonly used to evade detection.",
    "lambda_layer": "Lambda layers in Keras/TensorFlow can contain arbitrary Python code that executes during model inference. Unlike standard layers, these can perform system operations beyond tensor computations.",
    "executable_in_zip": "Executable files (.exe, .sh, .bat, etc.) within model archives are highly suspicious. ML models should only contain weights and configuration, not executables.",
    "dissimilar_weights": "Weight vectors that are completely dissimilar to others in the same layer may indicate injected malicious data masquerading as model parameters.",
    "outlier_neurons": "Neurons with weight distributions far outside the normal range might encode hidden functionality or backdoors rather than learned features.",
    "blacklisted_name": "This model name appears on security blacklists, indicating known malicious models or naming patterns associated with attacks.",
    "manifest_name_mismatch": "Model names in manifests that don't match expected patterns may indicate tampered or malicious models trying to impersonate legitimate ones.",
    "encoded_strings": "Encoded or obfuscated strings in model files often hide malicious payloads or commands from security scanners.",
    "pickle_size_limit": "Extremely large pickle files may indicate embedded malicious data or attempts to cause resource exhaustion.",
    "nested_pickle": "Pickle operations within pickled data (nested pickling) is often used to create multi-stage exploits or hide malicious payloads.",
    "torch_legacy": "Legacy PyTorch formats may have unpatched vulnerabilities. The _use_new_zipfile_serialization=False flag indicates use of the older, less secure format.",
}


# Function to get explanation for a security issue
def get_explanation(category: str, specific_item: str = None) -> Optional[str]:
    """
    Get a security explanation for a given category and item.

    Args:
        category: The category of security issue ('import', 'opcode', 'pattern')
        specific_item: The specific item (e.g., 'os', 'REDUCE', 'base64_payload')

    Returns:
        A security-team-friendly explanation, or None if not found
    """
    if category == "import" and specific_item in DANGEROUS_IMPORTS:
        return DANGEROUS_IMPORTS[specific_item]
    elif category == "opcode" and specific_item in DANGEROUS_OPCODES:
        return DANGEROUS_OPCODES[specific_item]
    elif category == "pattern" and specific_item in PATTERN_EXPLANATIONS:
        return PATTERN_EXPLANATIONS[specific_item]

    return None


# Convenience functions for common use cases
def get_import_explanation(module_name: str) -> Optional[str]:
    """Get explanation for a dangerous import/module."""
    # Handle module.function format (e.g., "os.system")
    base_module = module_name.split(".")[0]
    return get_explanation("import", base_module)


def get_opcode_explanation(opcode_name: str) -> Optional[str]:
    """Get explanation for a dangerous pickle opcode."""
    return get_explanation("opcode", opcode_name)


def get_pattern_explanation(pattern_name: str) -> Optional[str]:
    """Get explanation for a suspicious pattern."""
    return get_explanation("pattern", pattern_name)

```


### modelaudit/license_checker.py

```python
import os
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional


@dataclass
class LicenseInfo:
    """Information about a detected license."""

    spdx_id: Optional[str] = None
    name: Optional[str] = None
    commercial_allowed: Optional[bool] = None
    source: str = "unknown"  # Where the license was detected from
    confidence: float = 0.0  # Confidence score (0.0 to 1.0)
    text: Optional[str] = None


@dataclass
class CopyrightInfo:
    """Information about detected copyright notices."""

    holder: str
    year: Optional[str] = None
    text: str = ""


# Common license patterns with SPDX IDs and commercial use status
LICENSE_PATTERNS = {
    # Permissive licenses (commercial-friendly)
    r"MIT\s+License|License\s*:\s*MIT|SPDX-License-Identifier:\s*MIT": {
        "spdx_id": "MIT",
        "name": "MIT License",
        "commercial_allowed": True,
    },
    r"Apache\s+License,?\s*Version\s+2\.0|Apache-2\.0|SPDX-License-Identifier:\s*Apache-2\.0": {
        "spdx_id": "Apache-2.0",
        "name": "Apache License 2.0",
        "commercial_allowed": True,
    },
    r"BSD\s+3-Clause|3-Clause\s+BSD|SPDX-License-Identifier:\s*BSD-3-Clause": {
        "spdx_id": "BSD-3-Clause",
        "name": "BSD 3-Clause License",
        "commercial_allowed": True,
    },
    r"BSD\s+2-Clause|2-Clause\s+BSD|SPDX-License-Identifier:\s*BSD-2-Clause": {
        "spdx_id": "BSD-2-Clause",
        "name": "BSD 2-Clause License",
        "commercial_allowed": True,
    },
    # Copyleft licenses (require careful consideration)
    r"GNU\s+General\s+Public\s+License.*version\s+3|GPL-3\.0|GPLv3|SPDX-License-Identifier:\s*GPL-3\.0": {
        "spdx_id": "GPL-3.0",
        "name": "GNU General Public License v3.0",
        "commercial_allowed": True,  # But with obligations
    },
    r"GNU\s+General\s+Public\s+License.*version\s+2|GPL-2\.0|GPLv2|SPDX-License-Identifier:\s*GPL-2\.0": {
        "spdx_id": "GPL-2.0",
        "name": "GNU General Public License v2.0",
        "commercial_allowed": True,  # But with obligations
    },
    r"GNU\s+Affero\s+General\s+Public\s+License|AGPL-3\.0|AGPLv3|SPDX-License-Identifier:\s*AGPL-3\.0": {
        "spdx_id": "AGPL-3.0",
        "name": "GNU Affero General Public License v3.0",
        "commercial_allowed": True,  # But with strong network use obligations
    },
    r"GNU\s+Lesser\s+General\s+Public\s+License|LGPL-2\.1|LGPL-3\.0|LGPLv[23]|SPDX-License-Identifier:\s*LGPL-[23]\.": {
        "spdx_id": "LGPL-2.1+",
        "name": "GNU Lesser General Public License",
        "commercial_allowed": True,  # But with linking obligations
    },
    # Creative Commons licenses
    r"Creative\s+Commons.*Attribution.*4\.0|CC\s+BY\s+4\.0|CC-BY-4\.0": {
        "spdx_id": "CC-BY-4.0",
        "name": "Creative Commons Attribution 4.0",
        "commercial_allowed": True,
    },
    r"Creative\s+Commons.*Attribution.*ShareAlike.*4\.0|CC\s+BY-SA\s+4\.0|CC-BY-SA-4\.0": {
        "spdx_id": "CC-BY-SA-4.0",
        "name": "Creative Commons Attribution ShareAlike 4.0",
        "commercial_allowed": True,  # But with share-alike obligations
    },
    r"Creative\s+Commons.*Attribution.*NonCommercial|CC\s+BY-NC|CC-BY-NC": {
        "spdx_id": "CC-BY-NC-4.0",
        "name": "Creative Commons Attribution NonCommercial",
        "commercial_allowed": False,
    },
    # Common dataset licenses
    r"Open\s+Data\s+Commons.*Open\s+Database\s+License|ODbL-1\.0": {
        "spdx_id": "ODbL-1.0",
        "name": "Open Data Commons Open Database License",
        "commercial_allowed": True,  # But with share-alike obligations
    },
    r"Open\s+Data\s+Commons.*Public\s+Domain\s+Dedication|PDDL-1\.0": {
        "spdx_id": "PDDL-1.0",
        "name": "Open Data Commons Public Domain Dedication",
        "commercial_allowed": True,
    },
}

# Copyright notice patterns
COPYRIGHT_PATTERNS = [
    r"Copyright\s+(?:\(c\)\s*)?(\d{4}(?:-\d{4})?)\s+(.+?)(?:\n|$)",
    r"\(c\)\s*(\d{4}(?:-\d{4})?)\s+(.+?)(?:\n|$)",
    r"Â©\s*(\d{4}(?:-\d{4})?)\s+(.+?)(?:\n|$)",
]

# Patterns indicating unlicensed or problematic licensing
UNLICENSED_INDICATORS = [
    r"all\s+rights\s+reserved",
    r"proprietary",
    r"confidential",
    r"internal\s+use\s+only",
]

# File extensions that commonly contain license information
LICENSE_FILES = {
    "license",
    "license.txt",
    "license.md",
    "licence",
    "licence.txt",
    "licence.md",
    "copying",
    "copying.txt",
    "copyright",
    "copyright.txt",
    "notice",
    "notice.txt",
    "legal",
    "legal.txt",
    "terms",
    "terms.txt",
}

# Dataset file patterns that often lack proper licensing
DATASET_EXTENSIONS = {
    ".csv",
    ".json",
    ".jsonl",
    ".parquet",
    ".tsv",
    ".pkl",
    ".npy",
    ".npz",
}

# Model file patterns
MODEL_EXTENSIONS = {
    ".pkl",
    ".joblib",
    ".pt",
    ".pth",
    ".onnx",
    ".pb",
    ".h5",
    ".keras",
    ".safetensors",
}


def scan_for_license_headers(file_path: str, max_lines: int = 50) -> List[LicenseInfo]:
    """
    Scan a file's header for license information.

    Args:
        file_path: Path to the file to scan
        max_lines: Maximum number of lines to scan from the beginning

    Returns:
        List of detected license information
    """
    licenses: List[LicenseInfo] = []

    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = ""
            for i, line in enumerate(f):
                if i >= max_lines:
                    break
                content += line
    except (OSError, UnicodeDecodeError):
        # Try reading as binary for files that might not be text
        try:
            with open(file_path, "rb") as f:
                binary_content = f.read(1024 * 10)  # Read first 10KB
                content = binary_content.decode("utf-8", errors="ignore")
        except Exception:
            return licenses

    # Search for license patterns
    for pattern, info in LICENSE_PATTERNS.items():
        matches = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE)
        if matches:
            license_info = LicenseInfo(
                spdx_id=str(info["spdx_id"]) if info["spdx_id"] else None,
                name=str(info["name"]) if info["name"] else None,
                commercial_allowed=info["commercial_allowed"]
                if isinstance(info["commercial_allowed"], bool)
                else None,
                source="file_header",
                confidence=0.8,  # High confidence for explicit patterns
            )
            licenses.append(license_info)

    return licenses


def extract_copyright_notices(
    file_path: str, max_lines: int = 50
) -> List[CopyrightInfo]:
    """
    Extract copyright notices from a file.

    Args:
        file_path: Path to the file to scan
        max_lines: Maximum number of lines to scan

    Returns:
        List of copyright information found
    """
    copyrights: List[CopyrightInfo] = []

    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = ""
            for i, line in enumerate(f):
                if i >= max_lines:
                    break
                content += line
    except Exception:
        return copyrights

    # Search for copyright patterns
    for pattern in COPYRIGHT_PATTERNS:
        matches = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE)
        for match in matches:
            if len(match) >= 2:
                year = match[0].strip()
                holder = match[1].strip()
                copyright_info = CopyrightInfo(
                    holder=holder, year=year, text=f"Copyright {year} {holder}"
                )
                copyrights.append(copyright_info)

    return copyrights


def find_license_files(directory: str) -> List[str]:
    """
    Find license files in a directory.

    Args:
        directory: Directory to search

    Returns:
        List of paths to license files
    """
    license_files: List[str] = []

    if not os.path.isdir(directory):
        return license_files

    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.lower() in LICENSE_FILES:
                license_files.append(os.path.join(root, file))

        # Don't recurse too deep
        if len(Path(root).parts) - len(Path(directory).parts) > 2:
            dirs.clear()

    return license_files


def detect_unlicensed_datasets(file_paths: List[str]) -> List[str]:
    """
    Detect dataset files that may lack proper licensing.

    Args:
        file_paths: List of file paths to check

    Returns:
        List of file paths that appear to be unlicensed datasets
    """
    unlicensed = []

    # Check if this looks like an ML model directory
    is_ml_model_dir = _is_ml_model_directory(file_paths)

    for file_path in file_paths:
        ext = Path(file_path).suffix.lower()
        filename = Path(file_path).name.lower()

        if ext in DATASET_EXTENSIONS:
            # Skip model files in ML model directories
            if is_ml_model_dir and ext in MODEL_EXTENSIONS:
                continue

            # Skip common ML configuration files in ML model directories
            if is_ml_model_dir and _is_ml_config_file(filename):
                continue

            # Check if there's a nearby license file
            dir_path = Path(file_path).parent
            try:
                existing_files = {
                    f.name.lower() for f in dir_path.iterdir() if f.is_file()
                }
                has_license = bool(LICENSE_FILES & existing_files)
            except OSError:
                has_license = False

            if not has_license:
                # Check if the file itself contains license info
                licenses = scan_for_license_headers(file_path, max_lines=10)
                if not licenses:
                    unlicensed.append(file_path)

    return unlicensed


def _is_ml_config_file(filename: str) -> bool:
    """
    Determine if a filename represents an ML configuration file.

    Args:
        filename: Lowercase filename to check

    Returns:
        True if this appears to be an ML configuration file
    """
    ml_config_patterns = {
        "config.json",
        "model.json",
        "tokenizer_config.json",
        "tokenizer.json",
        "vocab.json",
        "merges.txt",
        "generation_config.json",
        "preprocessor_config.json",
        "model_config.json",
        "training_args.json",
        "optimizer.json",
        "scheduler.json",
    }

    return filename in ml_config_patterns


def _is_ml_model_directory(file_paths: List[str]) -> bool:
    """
    Determine if the file paths represent an ML model directory.

    Args:
        file_paths: List of file paths to analyze

    Returns:
        True if this appears to be an ML model directory
    """
    if not file_paths:
        return False

    # Get all filenames
    filenames = [Path(fp).name.lower() for fp in file_paths]

    # Common ML model directory indicators
    ml_indicators = {
        "config.json",
        "model.json",
        "tokenizer_config.json",
        "model.safetensors",
        "pytorch_model.bin",
        "tf_model.h5",
        "model.onnx",
        "model.pb",
        "saved_model.pb",
    }

    # Check if we have typical ML model files
    has_ml_files = any(filename in ml_indicators for filename in filenames)

    # Check for model weight files with typical patterns
    has_weight_files = any(
        "model" in filename
        and any(ext in filename for ext in [".bin", ".h5", ".safetensors"])
        for filename in filenames
    )

    # Check for config files
    has_config_files = any(
        "config" in filename and filename.endswith(".json") for filename in filenames
    )

    return has_ml_files or (has_weight_files and has_config_files)


def detect_agpl_components(scan_results: Dict[str, Any]) -> List[str]:
    """
    Detect components that use AGPL licensing.

    Args:
        scan_results: Scan results dictionary

    Returns:
        List of file paths with AGPL licensing
    """
    agpl_files = []

    file_metadata = scan_results.get("file_metadata", {})
    for file_path, metadata in file_metadata.items():
        licenses = metadata.get("license_info", [])
        for license_info in licenses:
            if isinstance(license_info, dict):
                spdx_id = license_info.get("spdx_id", "")
                if "AGPL" in spdx_id:
                    agpl_files.append(file_path)
            elif hasattr(license_info, "spdx_id"):
                if license_info.spdx_id and "AGPL" in license_info.spdx_id:
                    agpl_files.append(file_path)

    return agpl_files


def check_commercial_use_warnings(scan_results: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Check for common license warnings related to commercial use.

    Args:
        scan_results: Scan results dictionary

    Returns:
        List of warning dictionaries
    """
    warnings = []

    # Check for AGPL components
    agpl_files = detect_agpl_components(scan_results)
    if agpl_files:
        warnings.append(
            {
                "type": "license_warning",
                "severity": "warning",
                "message": f"AGPL-licensed components detected ({len(agpl_files)} files). Review network use restrictions for SaaS deployment.",
                "details": {
                    "files": agpl_files[:5],  # Show first 5 files
                    "total_count": len(agpl_files),
                    "license_type": "AGPL",
                    "impact": "Requires source code disclosure for network services",
                },
            }
        )

    # Check for datasets with unspecified licenses
    # Only warn if we have multiple files or files that are clearly datasets
    all_files = list(scan_results.get("file_metadata", {}).keys())
    unlicensed_datasets = detect_unlicensed_datasets(all_files)

    # Filter out single files that might be tests or simple examples
    significant_unlicensed_datasets = []
    if len(all_files) > 1:  # Multiple files - likely a project
        significant_unlicensed_datasets = unlicensed_datasets
    else:
        # Single file case - only warn if it's clearly a substantial dataset
        for file_path in unlicensed_datasets:
            try:
                file_size = os.path.getsize(file_path)
                # Only warn about single files that are substantial (>100KB)
                if file_size > 100 * 1024:
                    significant_unlicensed_datasets.append(file_path)
            except OSError:
                pass

    if significant_unlicensed_datasets:
        warnings.append(
            {
                "type": "license_warning",
                "severity": "warning",
                "message": f"Datasets with unspecified licenses detected ({len(significant_unlicensed_datasets)} files). Verify data usage rights.",
                "details": {
                    "files": significant_unlicensed_datasets[:5],  # Show first 5 files
                    "total_count": len(significant_unlicensed_datasets),
                    "impact": "May restrict commercial use or redistribution",
                },
            }
        )

    # Check for non-commercial licenses
    nc_files = []
    file_metadata = scan_results.get("file_metadata", {})
    for file_path, metadata in file_metadata.items():
        licenses = metadata.get("license_info", [])
        for license_info in licenses:
            commercial_allowed = None
            if isinstance(license_info, dict):
                commercial_allowed = license_info.get("commercial_allowed")
            elif hasattr(license_info, "commercial_allowed"):
                commercial_allowed = license_info.commercial_allowed

            if commercial_allowed is False:
                nc_files.append(file_path)
                break

    if nc_files:
        warnings.append(
            {
                "type": "license_warning",
                "severity": "warning",
                "message": f"Non-commercial licensed components detected ({len(nc_files)} files). These cannot be used commercially.",
                "details": {
                    "files": nc_files[:5],
                    "total_count": len(nc_files),
                    "impact": "Prohibited for commercial use",
                },
            }
        )

    # Check for strong copyleft licenses mixed with proprietary code
    copyleft_files = []
    for file_path, metadata in file_metadata.items():
        licenses = metadata.get("license_info", [])
        for license_info in licenses:
            spdx_id = None
            if isinstance(license_info, dict):
                spdx_id = license_info.get("spdx_id")
            elif hasattr(license_info, "spdx_id"):
                spdx_id = license_info.spdx_id

            if spdx_id and any(gpl in spdx_id for gpl in ["GPL-", "AGPL-"]):
                copyleft_files.append(file_path)
                break

    if copyleft_files:
        warnings.append(
            {
                "type": "license_warning",
                "severity": "info",
                "message": f"Strong copyleft licensed components detected ({len(copyleft_files)} files). May require derivative works to be open-sourced.",
                "details": {
                    "files": copyleft_files[:5],
                    "total_count": len(copyleft_files),
                    "impact": "May require making derivative works available under the same license",
                },
            }
        )

    return warnings


def collect_license_metadata(file_path: str) -> Dict[str, Any]:
    """
    Collect comprehensive license metadata for a file.

    Args:
        file_path: Path to the file to analyze

    Returns:
        Dictionary containing license metadata
    """
    metadata = {
        "license_info": [],
        "copyright_notices": [],
        "license_files_nearby": [],
        "is_dataset": False,
        "is_model": False,
    }

    # Detect file type
    ext = Path(file_path).suffix.lower()
    metadata["is_dataset"] = ext in DATASET_EXTENSIONS
    metadata["is_model"] = ext in MODEL_EXTENSIONS

    # Scan for license headers
    licenses = scan_for_license_headers(file_path)
    metadata["license_info"] = [
        {
            "spdx_id": lic.spdx_id,
            "name": lic.name,
            "commercial_allowed": lic.commercial_allowed,
            "source": lic.source,
            "confidence": lic.confidence,
        }
        for lic in licenses
    ]

    # Extract copyright notices
    copyrights = extract_copyright_notices(file_path)
    metadata["copyright_notices"] = [
        {
            "holder": cr.holder,
            "year": cr.year,
            "text": cr.text,
        }
        for cr in copyrights
    ]

    # Find nearby license files
    if os.path.isfile(file_path):
        dir_path = str(Path(file_path).parent)
        nearby_licenses = find_license_files(dir_path)
        metadata["license_files_nearby"] = nearby_licenses

    return metadata

```


### modelaudit/name_policies/blacklist.py

```python
from typing import Optional

BLACKLIST_PATTERNS = [
    # Examples of patterns you might want to blacklist
    "malicious",
    "unsafe",
    # Add more patterns as needed
]


def check_model_name_policies(
    model_name: str,
    additional_patterns: Optional[list[str]] = None,
) -> tuple[bool, str]:
    """
    Return (blocked:boolean, reason:str) if model_name matches any pattern in
    the blacklist.

    Args:
        model_name: The name of the model to check
        additional_patterns: Optional list of additional patterns to check against
    """
    name_lower = model_name.lower()

    # Combine default patterns with any additional patterns
    patterns = list(BLACKLIST_PATTERNS)
    if additional_patterns:
        patterns.extend(additional_patterns)

    for pattern in patterns:
        if pattern.lower() in name_lower:
            return True, f"Model name matched blacklist pattern: {pattern}"
    return False, ""

```


### modelaudit/sbom.py

```python
import hashlib
import os
from typing import Any, Iterable

from cyclonedx.model import HashType, Property
from cyclonedx.model.bom import Bom
from cyclonedx.model.component import Component, ComponentType
from cyclonedx.model.license import LicenseExpression
from cyclonedx.output import OutputFormat, SchemaVersion, make_outputter


def _file_sha256(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def _component_for_file(
    path: str, metadata: dict[str, Any], issues: Iterable[dict[str, Any]]
) -> Component:
    size = os.path.getsize(path)
    sha256 = _file_sha256(path)
    props = [Property(name="size", value=str(size))]

    # Compute risk score based on issues related to this file
    score = 0
    for issue in issues:
        if issue.get("location") == path:
            severity = issue.get("severity")
            if severity == "error":
                score += 5
            elif severity == "warning":
                score += 2
            elif severity == "info":
                score += 1
    if score > 10:
        score = 10
    props.append(Property(name="risk_score", value=str(score)))

    # Enhanced license handling
    license_expressions = []
    if isinstance(metadata, dict):
        # Collect all license identifiers
        license_identifiers = []

        # Check for legacy license field
        legacy_license = metadata.get("license")
        if legacy_license:
            license_identifiers.append(str(legacy_license))

        # Check for new license metadata
        detected_licenses = metadata.get("license_info", [])
        for lic in detected_licenses:
            if isinstance(lic, dict) and lic.get("spdx_id"):
                license_identifiers.append(str(lic["spdx_id"]))
            elif isinstance(lic, dict) and lic.get("name"):
                license_identifiers.append(str(lic["name"]))

        # Create a single license expression to comply with CycloneDX
        if license_identifiers:
            # Remove duplicates while preserving order
            unique_licenses = []
            seen = set()
            for lic_id in license_identifiers:
                if lic_id not in seen:
                    unique_licenses.append(lic_id)
                    seen.add(lic_id)

            if len(unique_licenses) == 1:
                license_expressions.append(LicenseExpression(unique_licenses[0]))
            else:
                # Create compound license expression for multiple licenses
                compound_expression = " OR ".join(unique_licenses)
                license_expressions.append(LicenseExpression(compound_expression))

        # Add license-related properties
        if metadata.get("is_dataset"):
            props.append(Property(name="is_dataset", value="true"))
        if metadata.get("is_model"):
            props.append(Property(name="is_model", value="true"))

        # Add copyright information
        copyrights = metadata.get("copyright_notices", [])
        if copyrights:
            copyright_holders = [
                cr.get("holder", "") for cr in copyrights if isinstance(cr, dict)
            ]
            if copyright_holders:
                props.append(
                    Property(
                        name="copyright_holders", value=", ".join(copyright_holders)
                    )
                )

        # Add license files information
        license_files = metadata.get("license_files_nearby", [])
        if license_files:
            props.append(
                Property(name="license_files_found", value=str(len(license_files)))
            )

    component = Component(
        name=os.path.basename(path),
        bom_ref=path,
        type=ComponentType.FILE,
        hashes=[HashType.from_hashlib_alg("sha256", sha256)],
        properties=props,
    )

    if license_expressions:
        component.licenses = license_expressions

    return component


def generate_sbom(paths: Iterable[str], results: dict[str, Any]) -> str:
    bom = Bom()
    issues = results.get("issues", [])
    file_meta: dict[str, Any] = results.get("file_metadata", {})

    for input_path in paths:
        if os.path.isdir(input_path):
            for root, _, files in os.walk(input_path):
                for f in files:
                    fp = os.path.join(root, f)
                    meta = file_meta.get(fp, {})
                    component = _component_for_file(fp, meta, issues)
                    bom.components.add(component)
        else:
            meta = file_meta.get(input_path, {})
            component = _component_for_file(input_path, meta, issues)
            bom.components.add(component)

    outputter = make_outputter(bom, OutputFormat.JSON, SchemaVersion.V1_5)
    return outputter.output_as_string(indent=2)

```


### modelaudit/scanners/flax_msgpack_scanner.py

```python
from __future__ import annotations

import os
import re
from typing import Any, Dict, Optional

try:
    import msgpack  # type: ignore

    HAS_MSGPACK = True
except Exception:  # pragma: no cover - optional dependency missing
    HAS_MSGPACK = False

from .base import BaseScanner, IssueSeverity, ScanResult


class FlaxMsgpackScanner(BaseScanner):
    """Scanner for Flax msgpack checkpoint files with security threat detection."""

    name = "flax_msgpack"
    description = (
        "Scans Flax/JAX msgpack checkpoints for security threats and integrity issues"
    )
    supported_extensions = [".msgpack"]  # Removed .ckpt to avoid conflicts with PyTorch

    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        super().__init__(config)
        self.max_blob_bytes = self.config.get(
            "max_blob_bytes", 50 * 1024 * 1024
        )  # 50MB reasonable for model weights
        self.max_recursion_depth = self.config.get("max_recursion_depth", 100)
        self.max_items_per_container = self.config.get("max_items_per_container", 10000)
        self.suspicious_patterns = self.config.get(
            "suspicious_patterns",
            [
                r"__reduce__",
                r"__getstate__",
                r"__setstate__",
                r"eval\s*\(",
                r"exec\s*\(",
                r"subprocess",
                r"os\.system",
                r"import\s+os",
                r"import\s+subprocess",
                r"__import__",
                r"compile\s*\(",
                r"pickle\.loads",
                r"marshal\.loads",
                r"base64\.decode",
            ],
        )
        self.suspicious_keys = self.config.get(
            "suspicious_keys",
            {
                "__class__",
                "__module__",
                "__reduce__",
                "__getstate__",
                "__setstate__",
                "__dict__",
                "__code__",
                "__globals__",
                "__builtins__",
                "__import__",
            },
        )

    @classmethod
    def can_handle(cls, path: str) -> bool:
        if not os.path.isfile(path):
            return False
        ext = os.path.splitext(path)[1].lower()
        if ext in cls.supported_extensions and HAS_MSGPACK:
            return True
        return False

    def _check_suspicious_strings(
        self, value: str, location: str, result: ScanResult
    ) -> None:
        """Check string values for suspicious patterns that might indicate code injection."""
        for pattern in self.suspicious_patterns:
            if re.search(pattern, value, re.IGNORECASE):
                result.add_issue(
                    f"Suspicious code pattern detected: {pattern}",
                    severity=IssueSeverity.CRITICAL,
                    location=location,
                    details={
                        "pattern": pattern,
                        "sample": value[:200] + "..." if len(value) > 200 else value,
                        "full_length": len(value),
                    },
                )

    def _check_suspicious_keys(
        self, key: str, location: str, result: ScanResult
    ) -> None:
        """Check dictionary keys for suspicious names that might indicate serialization attacks."""
        if key in self.suspicious_keys:
            result.add_issue(
                f"Suspicious object attribute detected: {key}",
                severity=IssueSeverity.CRITICAL,
                location=location,
                details={"suspicious_key": key},
            )

    def _analyze_content(
        self, value: Any, location: str, result: ScanResult, depth: int = 0
    ) -> None:
        """Recursively analyze msgpack content for security threats and anomalies."""
        if depth > self.max_recursion_depth:
            result.add_issue(
                f"Maximum recursion depth exceeded: {depth}",
                severity=IssueSeverity.CRITICAL,
                location=location,
                details={"depth": depth, "max_allowed": self.max_recursion_depth},
            )
            return

        if isinstance(value, (bytes, bytearray)):
            size = len(value)
            if size > self.max_blob_bytes:
                result.add_issue(
                    f"Suspiciously large binary blob: {size:,} bytes",
                    severity=IssueSeverity.INFO,
                    location=location,
                    details={"size": size, "max_allowed": self.max_blob_bytes},
                )

            # Check for embedded executable content in binary data
            try:
                # Try to decode as UTF-8 to check for hidden text
                decoded = value.decode("utf-8", errors="ignore")
                if len(decoded) > 50:  # Only check substantial text
                    self._check_suspicious_strings(
                        decoded, f"{location}[decoded_binary]", result
                    )
            except Exception:  # pragma: no cover - encoding edge cases
                pass

        elif isinstance(value, str):
            # Check for suspicious string patterns
            self._check_suspicious_strings(value, location, result)

            # Check for very long strings that might be attacks
            if len(value) > 100000:  # 100KB string
                result.add_issue(
                    f"Extremely long string found: {len(value):,} characters",
                    severity=IssueSeverity.INFO,
                    location=location,
                    details={"length": len(value)},
                )

        elif isinstance(value, dict):
            if len(value) > self.max_items_per_container:
                result.add_issue(
                    f"Dictionary with excessive items: {len(value):,}",
                    severity=IssueSeverity.INFO,
                    location=location,
                    details={
                        "item_count": len(value),
                        "max_allowed": self.max_items_per_container,
                    },
                )

            for k, v in value.items():
                key_str = str(k)
                self._check_suspicious_keys(key_str, f"{location}/{key_str}", result)

                # Check if key itself contains suspicious patterns
                if isinstance(k, str):
                    self._check_suspicious_strings(k, f"{location}[key:{k}]", result)

                self._analyze_content(v, f"{location}/{key_str}", result, depth + 1)

        elif isinstance(value, (list, tuple)):
            if len(value) > self.max_items_per_container:
                result.add_issue(
                    f"Array with excessive items: {len(value):,}",
                    severity=IssueSeverity.INFO,
                    location=location,
                    details={
                        "item_count": len(value),
                        "max_allowed": self.max_items_per_container,
                    },
                )

            for i, v in enumerate(value):
                self._analyze_content(v, f"{location}[{i}]", result, depth + 1)

        elif isinstance(value, (int, float)):
            # Check for suspicious numerical values that might indicate attacks
            if isinstance(value, int) and abs(value) > 2**63:
                result.add_issue(
                    f"Extremely large integer value: {value}",
                    severity=IssueSeverity.INFO,
                    location=location,
                    details={"value": value},
                )

    def _validate_flax_structure(self, obj: Any, result: ScanResult) -> None:
        """Validate that the msgpack structure looks like a legitimate Flax checkpoint."""
        if not isinstance(obj, dict):
            result.add_issue(
                f"Unexpected top-level type: {type(obj).__name__} (expected dict)",
                severity=IssueSeverity.WARNING,
                location="root",
                details={"actual_type": type(obj).__name__},
            )
            return

        # Check for common Flax checkpoint patterns
        expected_keys = {"params", "state", "opt_state", "model_state", "step", "epoch"}
        found_keys = set(obj.keys()) if isinstance(obj, dict) else set()

        if not any(key in found_keys for key in expected_keys):
            result.add_issue(
                "No standard Flax checkpoint keys found - may not be a legitimate model",
                severity=IssueSeverity.INFO,
                location="root",
                details={
                    "found_keys": list(found_keys)[:20],  # Limit output
                    "expected_any_of": list(expected_keys),
                },
            )

        # Check for non-standard keys that might be suspicious
        suspicious_top_level = (
            found_keys - expected_keys - {"metadata", "config", "hyperparams"}
        )
        if suspicious_top_level:
            result.add_issue(
                f"Unusual top-level keys found: {suspicious_top_level}",
                severity=IssueSeverity.INFO,
                location="root",
                details={"unusual_keys": list(suspicious_top_level)},
            )

    def scan(self, path: str) -> ScanResult:
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        if not HAS_MSGPACK:
            result.add_issue(
                "msgpack library not installed - cannot analyze Flax checkpoints",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"required_package": "msgpack"},
            )
            result.finish(success=False)
            return result

        try:
            self.current_file_path = path

            # Read entire file to check for trailing data
            with open(path, "rb") as f:
                file_data = f.read()

            # Try to unpack and detect trailing data
            try:
                obj = msgpack.unpackb(file_data, raw=False, strict_map_key=False)
                # If we get here, the entire file was valid msgpack - no trailing data
            except msgpack.exceptions.ExtraData:
                # This means there's extra data after valid msgpack
                result.add_issue(
                    "Extra trailing data found after msgpack content",
                    severity=IssueSeverity.WARNING,
                    location=path,
                )
                # Unpack just the first object
                unpacker = msgpack.Unpacker(None, raw=False, strict_map_key=False)
                unpacker.feed(file_data)
                obj = unpacker.unpack()
            except (
                msgpack.exceptions.UnpackException,
                msgpack.exceptions.OutOfData,
            ) as e:
                result.add_issue(
                    f"Invalid msgpack format: {str(e)}",
                    severity=IssueSeverity.CRITICAL,
                    location=path,
                    details={"msgpack_error": str(e)},
                )
                result.finish(success=False)
                return result

            # Record metadata
            result.metadata["top_level_type"] = type(obj).__name__
            if isinstance(obj, dict):
                result.metadata["top_level_keys"] = list(obj.keys())[
                    :50
                ]  # Limit for large dicts
                result.metadata["key_count"] = len(obj.keys())

            # Validate Flax structure
            self._validate_flax_structure(obj, result)

            # Perform deep security analysis
            self._analyze_content(obj, "root", result)

            result.bytes_scanned = file_size
        except MemoryError:
            result.add_issue(
                "File too large to process safely - potential memory exhaustion attack",
                severity=IssueSeverity.CRITICAL,
                location=path,
            )
            result.finish(success=False)
            return result
        except Exception as e:
            result.add_issue(
                f"Unexpected error processing Flax msgpack file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"error_type": type(e).__name__, "error_message": str(e)},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

```


### modelaudit/scanners/gguf_scanner.py

```python
"""GGUF/GGML scanner that combines comprehensive parsing with security checks."""

from __future__ import annotations

import os
import struct
from typing import Any, BinaryIO, Dict, Optional

from .base import BaseScanner, IssueSeverity, ScanResult

# Map ggml_type enum to (block_size, type_size) for comprehensive validation
# Values derived from ggml source
_GGML_TYPE_INFO = {
    0: (1, 4),  # F32
    1: (1, 2),  # F16
    2: (32, 18),  # Q4_0
    3: (32, 20),  # Q4_1
    6: (32, 22),  # Q5_0
    7: (32, 24),  # Q5_1
    8: (32, 34),  # Q8_0
    9: (32, 36),  # Q8_1
    10: (256, 84),  # Q2_K
    11: (256, 110),  # Q3_K
    12: (256, 144),  # Q4_K
    13: (256, 176),  # Q5_K
    14: (256, 210),  # Q6_K
    15: (256, 292),  # Q8_K
}

# Type sizes for metadata parsing
_TYPE_SIZES = {
    0: 1,  # UINT8
    1: 1,  # INT8
    2: 2,  # UINT16
    3: 2,  # INT16
    4: 4,  # UINT32
    5: 4,  # INT32
    6: 4,  # FLOAT32
    7: 1,  # BOOL
    8: 8,  # STRING
    9: 0,  # ARRAY (variable size)
    10: 8,  # UINT64
    11: 8,  # INT64
    12: 8,  # FLOAT64
}


class GgufScanner(BaseScanner):
    """Scanner for GGUF/GGML model files with comprehensive parsing and security checks."""

    name = "gguf"
    description = (
        "Validates GGUF/GGML model file headers, metadata, and tensor integrity"
    )
    supported_extensions = [".gguf", ".ggml"]

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self.max_uncompressed = self.config.get(
            "max_uncompressed", 2 * 1024 * 1024 * 1024
        )

    @classmethod
    def can_handle(cls, path: str) -> bool:
        if not os.path.isfile(path):
            return False

        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False

        try:
            with open(path, "rb") as f:
                magic = f.read(4)
            return magic in (b"GGUF", b"GGML")
        except Exception:
            return False

    def scan(self, path: str) -> ScanResult:
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            with open(path, "rb") as f:
                magic = f.read(4)
                if magic == b"GGUF":
                    self._scan_gguf(f, file_size, result)
                elif magic == b"GGML":
                    self._scan_ggml(f, file_size, magic, result)
                else:
                    result.add_issue(
                        f"Unrecognized file format: {magic!r}",
                        IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result
        except Exception as e:
            result.add_issue(
                f"Error scanning GGUF/GGML file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(
            success=not any(i.severity == IssueSeverity.CRITICAL for i in result.issues)
        )
        return result

    def _read_string(self, f: BinaryIO, max_length: int = 1024 * 1024) -> str:
        """Read a string with length checking for security."""
        (length,) = struct.unpack("<Q", f.read(8))
        if length > max_length:
            raise ValueError(f"String length {length} exceeds maximum {max_length}")
        data = f.read(length)
        if len(data) != length:
            raise ValueError("Unexpected end of file while reading string")
        return data.decode("utf-8", "ignore")

    def _scan_gguf(self, f: BinaryIO, file_size: int, result: ScanResult) -> None:
        """Comprehensive GGUF file scanning with security checks."""
        # Read header
        version = struct.unpack("<I", f.read(4))[0]
        n_tensors = struct.unpack("<Q", f.read(8))[0]
        n_kv = struct.unpack("<Q", f.read(8))[0]

        result.metadata.update(
            {
                "format": "gguf",
                "version": version,
                "n_tensors": n_tensors,
                "n_kv": n_kv,
            }
        )

        # Security checks on header values
        if n_kv > 1_000_000:
            result.add_issue(
                f"GGUF header appears invalid (declared {n_kv} KV entries)",
                severity=IssueSeverity.CRITICAL,
            )
            return

        if n_tensors > 100_000:
            result.add_issue(
                f"GGUF header appears invalid (declared {n_tensors} tensors)",
                severity=IssueSeverity.CRITICAL,
            )
            return

        if file_size < 24:
            result.add_issue(
                "File too small to contain GGUF metadata",
                severity=IssueSeverity.CRITICAL,
            )
            return

        # Parse metadata with security checks
        metadata: Dict[str, Any] = {}
        try:
            for i in range(min(n_kv, 10000)):  # Limit to prevent DoS
                key = self._read_string(f)

                # Security check for suspicious keys
                if any(x in key for x in ("../", "..\\", "/", "\\")):
                    result.add_issue(
                        f"Suspicious metadata key with path traversal: {key}",
                        severity=IssueSeverity.WARNING,
                    )

                (value_type,) = struct.unpack("<I", f.read(4))
                value = self._read_value(f, value_type)
                metadata[key] = value

                # Security check for suspicious values
                if isinstance(value, str) and any(
                    p in value for p in ("/", "\\", ";", "&&", "|", "`")
                ):
                    result.add_issue(
                        f"Suspicious metadata value for key '{key}': {value}",
                        severity=IssueSeverity.INFO,
                    )

            result.metadata["metadata"] = metadata
        except Exception as e:
            result.add_issue(
                f"GGUF metadata parse error: {e}",
                severity=IssueSeverity.CRITICAL,
            )
            return

        # Validate alignment
        alignment = metadata.get("general.alignment", 32)
        if alignment < 8 or alignment % 8 != 0 or alignment > 1024:
            result.add_issue(
                f"Invalid alignment value: {alignment}",
                IssueSeverity.WARNING,
            )

        # Align to tensor data
        current = f.tell()
        pad = (alignment - (current % alignment)) % alignment
        if pad:
            f.seek(pad, os.SEEK_CUR)

        # Parse tensor information
        tensors = []
        try:
            for i in range(min(n_tensors, 10000)):  # Limit to prevent DoS
                t_name = self._read_string(f)
                (nd,) = struct.unpack("<I", f.read(4))

                # Hard limit on dimensions to prevent DoS attacks
                if nd > 1000:  # Extremely large dimension count - skip this tensor
                    result.add_issue(
                        f"Tensor {t_name} has excessive dimensions ({nd}), skipping for security",
                        IssueSeverity.CRITICAL,
                    )
                    # Skip the rest of this tensor's data to prevent DoS
                    f.seek(nd * 8 + 4 + 8, os.SEEK_CUR)  # Skip dims + type + offset
                    continue

                if nd > 8:  # Reasonable limit for tensor dimensions
                    result.add_issue(
                        f"Tensor {t_name} has suspicious number of dimensions: {nd}",
                        IssueSeverity.WARNING,
                    )

                dims = [struct.unpack("<Q", f.read(8))[0] for _ in range(nd)]
                (t_type,) = struct.unpack("<I", f.read(4))
                (offset,) = struct.unpack("<Q", f.read(8))

                tensors.append(
                    {
                        "name": t_name,
                        "dims": dims,
                        "type": t_type,
                        "offset": offset,
                    }
                )

            result.metadata["tensors"] = [
                {"name": t["name"], "type": t["type"], "dims": t["dims"]}
                for t in tensors
            ]
        except Exception as e:
            result.add_issue(
                f"GGUF tensor parse error: {e}",
                severity=IssueSeverity.CRITICAL,
            )
            return

        # Validate tensor sizes and offsets
        for idx, tensor in enumerate(tensors):
            try:
                nelements = 1
                has_invalid_dimension = False
                for d in tensor["dims"]:
                    if d <= 0 or d > 2**31:
                        result.add_issue(
                            f"Tensor {tensor['name']} has invalid dimension: {d}",
                            IssueSeverity.WARNING,
                        )
                        has_invalid_dimension = True
                        break
                    nelements *= d

                # Skip tensor validation if any dimension is invalid
                if has_invalid_dimension:
                    continue

                # Check for extremely large tensors using correct size calculation
                # For quantized types, use the actual type information
                info = _GGML_TYPE_INFO.get(tensor["type"])
                if info:
                    # For quantized types, calculate based on block and type size
                    blck, ts = info
                    estimated_size = ((nelements + blck - 1) // blck) * ts
                else:
                    # Fallback for unknown types - assume 4 bytes per element
                    estimated_size = nelements * 4

                if estimated_size > self.max_uncompressed:
                    result.add_issue(
                        f"Tensor {tensor['name']} estimated size ({estimated_size}) exceeds limit",
                        IssueSeverity.CRITICAL,
                    )

                # Validate tensor type and size
                info = _GGML_TYPE_INFO.get(tensor["type"])
                if info:
                    blck, ts = info
                    if nelements % blck != 0:
                        result.add_issue(
                            f"Tensor {tensor['name']} not aligned to block size {blck}",
                            IssueSeverity.WARNING,
                        )

                    expected = ((nelements + blck - 1) // blck) * ts
                    next_offset = (
                        tensors[idx + 1]["offset"]
                        if idx + 1 < len(tensors)
                        else file_size
                    )
                    actual = next_offset - tensor["offset"]

                    if expected != actual:
                        result.add_issue(
                            f"Size mismatch for tensor {tensor['name']}",
                            IssueSeverity.CRITICAL,
                            details={"expected": expected, "actual": actual},
                        )
            except (OverflowError, ValueError) as e:
                result.add_issue(
                    f"Error validating tensor {tensor['name']}: {e}",
                    IssueSeverity.WARNING,
                )

        result.bytes_scanned = f.tell()

    def _scan_ggml(
        self, f: BinaryIO, file_size: int, magic: bytes, result: ScanResult
    ) -> None:
        """Basic GGML file validation with security checks."""
        result.metadata["format"] = "ggml"
        result.metadata["magic"] = magic.decode("ascii", "ignore")

        if file_size < 32:
            result.add_issue(
                "File too small to be valid GGML",
                severity=IssueSeverity.CRITICAL,
            )
            return

        # Basic heuristic validation
        try:
            version_bytes = f.read(4)
            if len(version_bytes) < 4:
                result.add_issue(
                    "Truncated GGML header",
                    severity=IssueSeverity.CRITICAL,
                )
                return

            version = struct.unpack("<I", version_bytes)[0]
            result.metadata["version"] = version

            if version > 10000:  # Reasonable upper bound
                result.add_issue(
                    f"Suspicious GGML version: {version}",
                    severity=IssueSeverity.WARNING,
                )
        except Exception as e:
            result.add_issue(
                f"Error parsing GGML header: {e}",
                severity=IssueSeverity.CRITICAL,
            )

        result.bytes_scanned = file_size

    def _read_value(self, f: BinaryIO, vtype: int) -> Any:
        """Read a value of the specified type with security checks."""
        if vtype == 0:  # UINT8
            return struct.unpack("<B", f.read(1))[0]
        elif vtype == 1:  # INT8
            return struct.unpack("<b", f.read(1))[0]
        elif vtype == 2:  # UINT16
            return struct.unpack("<H", f.read(2))[0]
        elif vtype == 3:  # INT16
            return struct.unpack("<h", f.read(2))[0]
        elif vtype == 4:  # UINT32
            return struct.unpack("<I", f.read(4))[0]
        elif vtype == 5:  # INT32
            return struct.unpack("<i", f.read(4))[0]
        elif vtype == 6:  # FLOAT32
            return struct.unpack("<f", f.read(4))[0]
        elif vtype == 7:  # BOOL
            return struct.unpack("<B", f.read(1))[0] != 0
        elif vtype == 8:  # STRING
            return self._read_string(f)
        elif vtype == 9:  # ARRAY
            subtype = struct.unpack("<I", f.read(4))[0]
            (count,) = struct.unpack("<Q", f.read(8))
            if count > 10000:  # Prevent DoS
                raise ValueError(f"Array too large: {count} elements")
            return [self._read_value(f, subtype) for _ in range(count)]
        elif vtype == 10:  # UINT64
            return struct.unpack("<Q", f.read(8))[0]
        elif vtype == 11:  # INT64
            return struct.unpack("<q", f.read(8))[0]
        elif vtype == 12:  # FLOAT64
            return struct.unpack("<d", f.read(8))[0]
        else:
            raise ValueError(f"Unknown metadata type {vtype}")

```


### modelaudit/scanners/joblib_scanner.py

```python
from __future__ import annotations

import io
import lzma
import os
import zlib
from typing import Any, Optional

from ..utils.filetype import read_magic_bytes
from .base import BaseScanner, IssueSeverity, ScanResult
from .pickle_scanner import PickleScanner


class JoblibScanner(BaseScanner):
    """Scanner for joblib serialized files."""

    name = "joblib"
    description = "Scans joblib files by decompressing and analyzing embedded pickle"
    supported_extensions = [".joblib"]

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        self.pickle_scanner = PickleScanner(config)
        # Security limits
        self.max_decompression_ratio = self.config.get("max_decompression_ratio", 100.0)
        self.max_decompressed_size = self.config.get(
            "max_decompressed_size", 100 * 1024 * 1024
        )  # 100MB
        self.max_file_read_size = self.config.get(
            "max_file_read_size", 100 * 1024 * 1024
        )  # 100MB
        self.chunk_size = self.config.get("chunk_size", 8192)  # 8KB chunks

    @classmethod
    def can_handle(cls, path: str) -> bool:
        if not os.path.isfile(path):
            return False
        ext = os.path.splitext(path)[1].lower()
        if ext != ".joblib":
            return False
        return True

    def _read_file_safely(self, path: str) -> bytes:
        """Read file in chunks with size validation"""
        data = b""
        file_size = self.get_file_size(path)

        if file_size > self.max_file_read_size:
            raise ValueError(
                f"File too large: {file_size} bytes (max: {self.max_file_read_size})"
            )

        with open(path, "rb") as f:
            while True:
                chunk = f.read(self.chunk_size)
                if not chunk:
                    break
                data += chunk
                if len(data) > self.max_file_read_size:
                    raise ValueError(f"File read exceeds limit: {len(data)} bytes")
        return data

    def _safe_decompress(self, data: bytes) -> bytes:
        """Safely decompress data with bomb protection"""
        compressed_size = len(data)

        # Try zlib first
        decompressed = None
        try:
            decompressed = zlib.decompress(data)
        except Exception:
            # Try lzma
            try:
                decompressed = lzma.decompress(data)
            except Exception as e:
                raise ValueError(f"Unable to decompress joblib file: {e}")

        # Check decompression ratio for compression bomb detection
        if compressed_size > 0:
            ratio = len(decompressed) / compressed_size
            if ratio > self.max_decompression_ratio:
                raise ValueError(
                    f"Suspicious compression ratio: {ratio:.1f}x "
                    f"(max: {self.max_decompression_ratio}x) - possible compression bomb"
                )

        # Check absolute decompressed size
        if len(decompressed) > self.max_decompressed_size:
            raise ValueError(
                f"Decompressed size too large: {len(decompressed)} bytes "
                f"(max: {self.max_decompressed_size})"
            )

        return decompressed

    def scan(self, path: str) -> ScanResult:
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            self.current_file_path = path
            magic = read_magic_bytes(path, 4)
            data = self._read_file_safely(path)

            if magic.startswith(b"PK"):
                # Treat as zip archive
                from .zip_scanner import ZipScanner

                zip_scanner = ZipScanner(self.config)
                sub_result = zip_scanner.scan(path)
                result.merge(sub_result)
                result.bytes_scanned = sub_result.bytes_scanned
                result.metadata.update(sub_result.metadata)
                result.finish(success=sub_result.success)
                return result

            if magic.startswith(b"\x80"):
                with io.BytesIO(data) as file_like:
                    sub_result = self.pickle_scanner._scan_pickle_bytes(
                        file_like, len(data)
                    )
                result.merge(sub_result)
                result.bytes_scanned = len(data)
            else:
                # Try safe decompression
                try:
                    decompressed = self._safe_decompress(data)
                except ValueError as e:
                    result.add_issue(
                        str(e),
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                        details={"security_check": "compression_bomb_detection"},
                    )
                    result.finish(success=False)
                    return result
                except Exception as e:
                    result.add_issue(
                        f"Error decompressing joblib file: {e}",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result
                with io.BytesIO(decompressed) as file_like:
                    sub_result = self.pickle_scanner._scan_pickle_bytes(
                        file_like, len(decompressed)
                    )
                result.merge(sub_result)
                result.bytes_scanned = len(decompressed)
        except Exception as e:  # pragma: no cover
            result.add_issue(
                f"Error scanning joblib file: {e}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

```


### modelaudit/scanners/keras_h5_scanner.py

```python
import json
import os
from typing import Any, Optional

from modelaudit.suspicious_symbols import (
    SUSPICIOUS_CONFIG_PROPERTIES,
    SUSPICIOUS_LAYER_TYPES,
)

from ..explanations import get_pattern_explanation
from .base import BaseScanner, IssueSeverity, ScanResult

# Try to import h5py, but handle the case where it's not installed
try:
    import h5py

    HAS_H5PY = True
except ImportError:
    HAS_H5PY = False


class KerasH5Scanner(BaseScanner):
    """Scanner for Keras H5 model files"""

    name = "keras_h5"
    description = "Scans Keras H5 model files for suspicious layer configurations"
    supported_extensions = [".h5", ".hdf5", ".keras"]

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        # Additional scanner-specific configuration
        self.suspicious_layer_types = dict(SUSPICIOUS_LAYER_TYPES)
        if config and "suspicious_layer_types" in config:
            self.suspicious_layer_types.update(config["suspicious_layer_types"])

        self.suspicious_config_props = list(SUSPICIOUS_CONFIG_PROPERTIES)
        if config and "suspicious_config_properties" in config:
            self.suspicious_config_props.extend(config["suspicious_config_properties"])

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not HAS_H5PY:
            return False

        if not os.path.isfile(path):
            return False

        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False

        # Try to open as HDF5 file
        try:
            with h5py.File(path, "r") as _:
                return True
        except Exception:
            return False

    def scan(self, path: str) -> ScanResult:
        """Scan a Keras model file for suspicious configurations"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        # Check if h5py is installed
        if not HAS_H5PY:
            result = self._create_result()
            result.add_issue(
                "h5py not installed, cannot scan Keras H5 files. Install with "
                "'pip install modelaudit[h5]'.",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"path": path},
            )
            result.finish(success=False)
            return result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            # Store the file path for use in issue locations
            self.current_file_path = path

            with h5py.File(path, "r") as f:
                result.bytes_scanned = file_size

                # Check if this is a Keras model file
                if "model_config" not in f.attrs:
                    # Check if this might be a TensorFlow SavedModel H5 file instead
                    # Look for common TensorFlow H5 structure patterns
                    is_tensorflow_h5 = any(
                        key.startswith(
                            ("model_weights", "optimizer_weights", "variables")
                        )
                        for key in f.keys()
                    )

                    if is_tensorflow_h5:
                        result.add_issue(
                            "File appears to be a TensorFlow H5 model, not Keras format "
                            "(no model_config attribute)",
                            severity=IssueSeverity.DEBUG,  # Reduced severity - this is expected
                            location=self.current_file_path,
                        )
                    else:
                        result.add_issue(
                            "File does not appear to be a Keras model "
                            "(no model_config attribute)",
                            severity=IssueSeverity.DEBUG,  # Reduced severity - not necessarily suspicious
                            location=self.current_file_path,
                        )
                    result.finish(success=True)  # Still success, just not a Keras file
                    return result

                # Parse model config
                model_config_str = f.attrs["model_config"]
                model_config = json.loads(model_config_str)

                # Scan model configuration
                self._scan_model_config(model_config, result)

                # Check for custom objects in the model
                if "custom_objects" in f.attrs:
                    result.add_issue(
                        "Model contains custom objects which could contain "
                        "arbitrary code",
                        severity=IssueSeverity.WARNING,
                        location=f"{self.current_file_path} (model_config)",
                        details={"custom_objects": list(f.attrs["custom_objects"])},
                    )

                # Check for custom metrics
                if "training_config" in f.attrs:
                    training_config = json.loads(f.attrs["training_config"])
                    if "metrics" in training_config:
                        for metric in training_config["metrics"]:
                            if isinstance(metric, dict) and metric.get(
                                "class_name",
                            ) not in [
                                "Accuracy",
                                "CategoricalAccuracy",
                                "BinaryAccuracy",
                            ]:
                                result.add_issue(
                                    f"Model contains custom metric: "
                                    f"{metric.get('class_name', 'unknown')}",
                                    severity=IssueSeverity.WARNING,
                                    location=f"{self.current_file_path} (metrics)",
                                    details={"metric": metric},
                                )

        except Exception as e:
            result.add_issue(
                f"Error scanning Keras H5 file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _scan_model_config(
        self,
        model_config: dict[str, Any],
        result: ScanResult,
    ) -> None:
        """Scan the model configuration for suspicious elements"""
        if not isinstance(model_config, dict):
            result.add_issue(
                "Invalid model configuration format",
                severity=IssueSeverity.WARNING,
                location=self.current_file_path,
            )
            return

        # Check model class name
        model_class = model_config.get("class_name", "")
        result.metadata["model_class"] = model_class

        # Collect all layers
        layers = []
        if "config" in model_config and "layers" in model_config["config"]:
            layers = model_config["config"]["layers"]

        # Count of each layer type
        layer_counts: dict[str, int] = {}

        # Check each layer
        for layer in layers:
            layer_class = layer.get("class_name", "")

            # Update layer count
            if layer_class in layer_counts:
                layer_counts[layer_class] += 1
            else:
                layer_counts[layer_class] = 1

            # Check for suspicious layer types
            if layer_class in self.suspicious_layer_types:
                result.add_issue(
                    f"Suspicious layer type found: {layer_class}",
                    severity=IssueSeverity.CRITICAL,
                    location=self.current_file_path,
                    details={
                        "layer_class": layer_class,
                        "description": self.suspicious_layer_types[layer_class],
                        "layer_config": layer.get("config", {}),
                    },
                    why=get_pattern_explanation("lambda_layer")
                    if layer_class == "Lambda"
                    else None,
                )

            # Check layer configuration for suspicious strings
            self._check_config_for_suspicious_strings(
                layer.get("config", {}),
                result,
                layer_class,
            )

            # If there are nested models, scan them recursively
            if (
                layer_class == "Model"
                and "config" in layer
                and "layers" in layer["config"]
            ):
                self._scan_model_config(layer, result)

        # Add layer counts to metadata
        result.metadata["layer_counts"] = layer_counts

    def _check_config_for_suspicious_strings(
        self,
        config: dict[str, Any],
        result: ScanResult,
        context: str = "",
    ) -> None:
        """Recursively check a configuration dictionary for suspicious strings"""
        if not isinstance(config, dict):
            return

        # Check all string values in the config
        for key, value in config.items():
            if isinstance(value, str):
                # Check for suspicious strings
                for suspicious_term in self.suspicious_config_props:
                    if suspicious_term in value.lower():
                        result.add_issue(
                            f"Suspicious configuration string found in {context}: "
                            f"'{suspicious_term}'",
                            severity=IssueSeverity.INFO,
                            location=f"{self.current_file_path} ({context})",
                            details={
                                "suspicious_term": suspicious_term,
                                "context": context,
                            },
                        )
            elif isinstance(value, dict):
                # Recursively check nested dictionaries
                self._check_config_for_suspicious_strings(
                    value,
                    result,
                    f"{context}.{key}",
                )
            elif isinstance(value, list):
                # Check each item in the list
                for i, item in enumerate(value):
                    if isinstance(item, dict):
                        self._check_config_for_suspicious_strings(
                            item,
                            result,
                            f"{context}.{key}[{i}]",
                        )

```


### modelaudit/scanners/manifest_scanner.py

```python
import json
import os
from typing import Any, Optional

from modelaudit.suspicious_symbols import SUSPICIOUS_CONFIG_PATTERNS

from .base import BaseScanner, IssueSeverity, ScanResult, logger

# Try to import the name policies module
try:
    from modelaudit.name_policies.blacklist import check_model_name_policies

    HAS_NAME_POLICIES = True
except ImportError:
    HAS_NAME_POLICIES = False

    # Create a placeholder function when the module is not available
    def check_model_name_policies(
        model_name: str,
        additional_patterns: Optional[list[str]] = None,
    ) -> tuple[bool, str]:
        return False, ""


# Try to import yaml, but handle the case where it's not installed
try:
    import yaml

    HAS_YAML = True
except ImportError:
    HAS_YAML = False

# Common manifest and config file formats
MANIFEST_EXTENSIONS = [
    ".json",
    ".yaml",
    ".yml",
    ".xml",
    ".toml",
    ".ini",
    ".cfg",
    ".config",
    ".manifest",
    ".model",
    ".metadata",
]

# Keys that might contain model names
MODEL_NAME_KEYS = [
    "name",
    "model_name",
    "model",
    "model_id",
    "id",
    "title",
    "artifact_name",
    "artifact_id",
    "package_name",
]

# Pre-compute lowercase versions for faster checks
MODEL_NAME_KEYS_LOWER = [key.lower() for key in MODEL_NAME_KEYS]


class ManifestScanner(BaseScanner):
    """Scanner for model manifest and configuration files"""

    name = "manifest"
    description = (
        "Scans model manifest and configuration files for suspicious content "
        "and blacklisted names"
    )
    supported_extensions = MANIFEST_EXTENSIONS

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        # Get blacklist patterns from config
        self.blacklist_patterns = self.config.get("blacklist_patterns", [])

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not os.path.isfile(path):
            return False

        filename = os.path.basename(path).lower()

        # Whitelist: Only scan files that are unique to AI/ML models
        aiml_specific_patterns = [
            # HuggingFace/Transformers specific configuration files
            "config.json",  # Model architecture config (when in ML model context)
            "generation_config.json",  # Text generation parameters
            "preprocessor_config.json",  # Data preprocessing config
            "feature_extractor_config.json",  # Feature extraction config
            "image_processor_config.json",  # Image processing config
            "scheduler_config.json",  # Learning rate scheduler config
            # Model metadata and manifest files specific to ML
            "model_index.json",  # Diffusion model index
            "model_card.json",  # Model card metadata
            "pytorch_model.bin.index.json",  # PyTorch model shard index
            "model.safetensors.index.json",  # SafeTensors model index
            "tf_model.h5.index.json",  # TensorFlow model index
            # ML-specific execution and deployment configs
            "inference_config.json",  # Model inference configuration
            "deployment_config.json",  # Model deployment configuration
            "serving_config.json",  # Model serving configuration
            # ONNX model specific
            "onnx_config.json",  # ONNX export configuration
            # Custom model configs that might contain execution parameters
            "custom_config.json",  # Custom model configurations
            "runtime_config.json",  # Runtime execution parameters
        ]

        # Check if filename matches any AI/ML specific pattern
        if any(pattern in filename for pattern in aiml_specific_patterns):
            return True

        # Additional check: files with "config" in name that are in ML model context
        # (but exclude tokenizer configs and general software configs)
        if (
            "config" in filename
            and "tokenizer" not in filename
            and filename
            not in [
                "config.py",
                "config.yaml",
                "config.yml",
                "config.ini",
                "config.cfg",
            ]
        ):
            # Only if it's likely an ML model config
            # (has model-related terms in path or specific extensions)
            path_lower = path.lower()
            if any(
                ml_term in path_lower
                for ml_term in ["model", "checkpoint", "huggingface", "transformers"]
            ) or os.path.splitext(path)[1].lower() in [".json"]:
                return True

        return False

    def scan(self, path: str) -> ScanResult:
        """Scan a manifest or configuration file"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            # Store the file path for use in issue locations
            self.current_file_path = path

            # First, check the raw file content for blacklisted terms
            self._check_file_for_blacklist(path, result)

            # Parse the file based on its extension
            ext = os.path.splitext(path)[1].lower()
            content = self._parse_file(path, ext, result)

            if content:
                result.bytes_scanned = file_size

                # Extract license information if present
                license_info = self._extract_license_info(content)
                if license_info:
                    result.metadata["license"] = license_info

                # Check for suspicious configuration patterns
                self._check_suspicious_patterns(content, result)

            else:
                result.add_issue(
                    f"Unable to parse file as a manifest or configuration: {path}",
                    severity=IssueSeverity.DEBUG,
                    location=path,
                )

        except Exception as e:
            result.add_issue(
                f"Error scanning manifest file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _check_file_for_blacklist(self, path: str, result: ScanResult) -> None:
        """Check the entire file content for blacklisted terms"""
        if not self.blacklist_patterns:
            return

        try:
            with open(path, encoding="utf-8") as f:
                content = (
                    f.read().lower()
                )  # Convert to lowercase for case-insensitive matching

                for pattern in self.blacklist_patterns:
                    pattern_lower = pattern.lower()
                    if pattern_lower in content:
                        result.add_issue(
                            f"Blacklisted term '{pattern}' found in file",
                            severity=IssueSeverity.CRITICAL,
                            location=self.current_file_path,
                            details={"blacklisted_term": pattern, "file_path": path},
                            why="This term matches a user-defined blacklist pattern. Organizations use blacklists to identify models or configurations that violate security policies or contain known malicious indicators.",
                        )
        except Exception as e:
            result.add_issue(
                f"Error checking file for blacklist: {str(e)}",
                severity=IssueSeverity.WARNING,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )

    def _parse_file(
        self, path: str, ext: str, result: Optional[ScanResult] = None
    ) -> Optional[dict[str, Any]]:
        """Parse the file based on its extension"""
        try:
            with open(path, encoding="utf-8") as f:
                content = f.read()

                # Try JSON format first
                if ext in [
                    ".json",
                    ".manifest",
                    ".model",
                    ".metadata",
                ] or content.strip().startswith(("{", "[")):
                    return json.loads(content)

                # Try YAML format if available
                if HAS_YAML and (
                    ext in [".yaml", ".yml"] or content.strip().startswith("---")
                ):
                    return yaml.safe_load(content)

                # For other formats, try JSON and then YAML if available
                try:
                    return json.loads(content)
                except json.JSONDecodeError:
                    if HAS_YAML:
                        try:
                            return yaml.safe_load(content)
                        except Exception:
                            pass

        except Exception as e:
            # Log the error but don't raise, as we want to continue scanning
            logger.warning(f"Error parsing file {path}: {str(e)}")
            if result is not None:
                result.add_issue(
                    f"Error parsing file: {path}",
                    severity=IssueSeverity.DEBUG,
                    location=path,
                    details={"exception": str(e), "exception_type": type(e).__name__},
                )

        return None

    def _extract_license_info(self, content: dict[str, Any]) -> Optional[str]:
        """Return license string if found in manifest content"""
        if not isinstance(content, dict):
            return None

        potential_keys = ["license", "licence", "licenses"]
        for key in potential_keys:
            if key in content:
                value = content[key]
                if isinstance(value, str):
                    return value
                if isinstance(value, list) and value:
                    first = value[0]
                    if isinstance(first, str):
                        return first

        return None

    def _check_suspicious_patterns(
        self,
        content: dict[str, Any],
        result: ScanResult,
    ) -> None:
        """Smart pattern detection with value analysis and ML context awareness"""

        # STEP 1: Detect ML context for smart filtering
        ml_context = self._detect_ml_context(content)

        def check_dict(d, prefix=""):
            if not isinstance(d, dict):
                return

            for key, value in d.items():
                key_lower = key.lower()
                full_key = f"{prefix}.{key}" if prefix else key

                # STEP 1.5: Check for blacklisted model names (integrated from original)
                if key_lower in MODEL_NAME_KEYS_LOWER:
                    blocked, reason = check_model_name_policies(
                        str(value), self.blacklist_patterns
                    )
                    if blocked:
                        result.add_issue(
                            f"Model name blocked by policy: {value}",
                            severity=IssueSeverity.CRITICAL,
                            location=self.current_file_path,
                            details={
                                "model_name": str(value),
                                "reason": reason,
                                "key": full_key,
                            },
                        )

                # STEP 2: Value-based analysis - check for actually dangerous content
                if self._is_actually_dangerous_value(key, value):
                    result.add_issue(
                        f"Dangerous configuration content: {full_key}",
                        severity=IssueSeverity.CRITICAL,
                        location=self.current_file_path,
                        details={
                            "key": full_key,
                            "analysis": "value_based",
                            "danger": "executable_content",
                            "value": self._format_value(value),
                        },
                    )
                    # Don't continue here - still check for patterns and recurse

                # STEP 3: Smart pattern matching
                matches = self._find_suspicious_matches(key_lower)
                if matches:
                    # STEP 4: Context-aware filtering
                    if not self._should_ignore_in_context(
                        key, value, matches, ml_context
                    ):
                        # STEP 5: Report with context-aware severity
                        severity = self._get_context_aware_severity(matches, ml_context)
                        why = None
                        if severity == IssueSeverity.INFO:
                            if "file_access" in matches and "network_access" in matches:
                                why = "File and network access patterns in ML model configurations are common for loading datasets and downloading resources. They are flagged for awareness but are typically benign in ML contexts."
                            elif "file_access" in matches:
                                why = "File access patterns in ML model configurations often indicate dataset paths or model checkpoints. This is flagged for awareness but is typical in ML workflows."
                            elif "network_access" in matches:
                                why = "Network access patterns in ML model configurations may indicate remote model repositories or dataset URLs. This is common in ML pipelines but worth reviewing."

                        result.add_issue(
                            f"Suspicious configuration pattern: {full_key} "
                            f"(category: {', '.join(matches)})",
                            severity=severity,
                            location=self.current_file_path,
                            details={
                                "key": full_key,
                                "value": self._format_value(value),
                                "categories": matches,
                                "ml_context": ml_context,
                                "analysis": "pattern_based",
                            },
                            why=why,
                        )

                # ALWAYS recursively check nested structures,
                # regardless of pattern matches
                if isinstance(value, dict):
                    check_dict(value, full_key)
                elif isinstance(value, list):
                    for i, item in enumerate(value):
                        if isinstance(item, dict):
                            check_dict(item, f"{full_key}[{i}]")

        check_dict(content)

    def _is_actually_dangerous_value(self, key: str, value: Any) -> bool:
        """Check if value content is actually dangerous executable code"""
        if not isinstance(value, str):
            return False

        value_lower = value.lower().strip()

        # Look for ACTUAL executable content patterns
        dangerous_patterns = [
            "import os",
            "subprocess.",
            "eval(",
            "exec(",
            "os.system",
            "__import__",
            "runpy",
            "shell=true",
            "rm -rf",
            "/bin/sh",
            "cmd.exe",
            # Add more specific patterns
            "exec('",
            'exec("',
            "eval('",
            'eval("',
        ]

        return any(pattern in value_lower for pattern in dangerous_patterns)

    def _detect_ml_context(self, content: dict[str, Any]) -> dict[str, Any]:
        """Detect ML model context to adjust sensitivity"""
        indicators = {
            "framework": None,
            "model_type": None,
            "confidence": 0,
            "is_tokenizer": False,
            "is_model_config": False,
        }

        # Framework detection patterns
        framework_patterns = {
            "huggingface": [
                "tokenizer_class",
                "transformers_version",
                "model_type",
                "architectures",
                "auto_map",
                "_name_or_path",
            ],
            "pytorch": [
                "torch",
                "state_dict",
                "pytorch_model",
                "model.pt",
                "torch_dtype",
            ],
            "tensorflow": [
                "tensorflow",
                "saved_model",
                "model.h5",
                "tf_version",
                "keras",
            ],
            "sklearn": ["sklearn", "pickle_module", "scikit"],
        }

        # Model type indicators
        tokenizer_indicators = [
            "tokenizer_class",
            "added_tokens_decoder",
            "model_input_names",
            "special_tokens_map",
            "bos_token",
            "eos_token",
            "pad_token",
        ]

        model_config_indicators = [
            "hidden_size",
            "num_attention_heads",
            "num_hidden_layers",
            "vocab_size",
            "max_position_embeddings",
            "architectures",
        ]

        def check_indicators(d):
            if not isinstance(d, dict):
                return

            for key, val in d.items():
                key_str = str(key).lower()
                val_str = str(val).lower()

                # Check framework patterns
                for framework, patterns in framework_patterns.items():
                    if any(
                        pattern in key_str or pattern in val_str for pattern in patterns
                    ):
                        indicators["framework"] = framework
                        indicators["confidence"] += 1

                # Check tokenizer indicators
                if any(indicator in key_str for indicator in tokenizer_indicators):
                    indicators["is_tokenizer"] = True
                    indicators["confidence"] += 1

                # Check model config indicators
                if any(indicator in key_str for indicator in model_config_indicators):
                    indicators["is_model_config"] = True
                    indicators["confidence"] += 1

                # Recursive check
                if isinstance(val, dict):
                    check_indicators(val)

        check_indicators(content)
        return indicators

    def _find_suspicious_matches(self, key_lower: str) -> list[str]:
        """Find all categories that match this key"""
        matches = []
        for category, patterns in SUSPICIOUS_CONFIG_PATTERNS.items():
            if any(pattern in key_lower for pattern in patterns):
                matches.append(category)
        return matches

    def _should_ignore_in_context(
        self, key: str, value: Any, matches: list[str], ml_context: dict
    ) -> bool:
        """Context-aware ignore logic combining smart patterns with value analysis"""
        key_lower = key.lower()

        # Special case for HuggingFace patterns - check this FIRST before other logic
        if ml_context.get("framework") == "huggingface" or "_name_or_path" in key_lower:
            huggingface_safe_patterns = [
                "_name_or_path",
                "name_or_path",
                "model_input_names",
                "model_output_names",
                "transformers_version",
                "torch_dtype",
                "architectures",
            ]
            if any(pattern in key_lower for pattern in huggingface_safe_patterns):
                return True

        # High-confidence ML context gets more lenient treatment
        if ml_context.get("confidence", 0) >= 2:
            # File access patterns in ML context
            if "file_access" in matches:
                # First check if this is an actual file path - never ignore those
                if key_lower.endswith(
                    ("_dir", "_path", "_file")
                ) and self._is_file_path_value(value):
                    return False  # Don't ignore actual file paths

                # Common ML config patterns that aren't actual file access
                safe_ml_patterns = [
                    "_input",
                    "input_",
                    "_output",
                    "output_",
                    "_size",
                    "_dim",
                    "hidden_",
                    "attention_",
                    "embedding_",
                    "_token_",
                    "vocab_",
                    "_names",
                    "model_input_names",
                    "model_output_names",
                ]

                if any(pattern in key_lower for pattern in safe_ml_patterns):
                    return True

            # Credentials in ML context
            if "credentials" in matches:
                # Token IDs and model tokens are not credentials in ML context
                if any(
                    pattern in key_lower
                    for pattern in ["_token_id", "token_id_", "_token", "token_type"]
                ):
                    return True

        # Special case for tokenizer configs
        if ml_context.get("is_tokenizer"):
            tokenizer_safe_keys = [
                "added_tokens_decoder",
                "model_input_names",
                "special_tokens_map",
                "tokenizer_class",
                "model_max_length",
            ]
            if any(safe_key in key_lower for safe_key in tokenizer_safe_keys):
                return True

        return False

    def _is_file_path_value(self, value: Any) -> bool:
        """Check if value appears to be an actual file system path"""
        if not isinstance(value, str):
            return False

        # Absolute paths
        if value.startswith(("/", "\\", "C:", "D:")):
            return True

        # Relative paths with separators
        if "/" in value or "\\" in value:
            return True

        # File extensions that suggest actual files (using endswith for better matching)
        file_extensions = [
            ".json",
            ".h5",
            ".pt",
            ".onnx",
            ".pkl",
            ".model",
            ".txt",
            ".log",
            ".csv",
            ".xml",
            ".yaml",
            ".yml",
            ".py",
            ".js",
            ".html",
            ".css",
            ".sql",
            ".md",
        ]
        if any(value.lower().endswith(ext) for ext in file_extensions):
            return True

        # Common path indicators
        if any(
            indicator in value.lower()
            for indicator in ["/tmp", "/var", "/data", "/home", "/etc", "c:\\", "d:\\"]
        ):
            return True

        return False

    def _get_context_aware_severity(
        self, matches: list[str], ml_context: dict
    ) -> IssueSeverity:
        """Determine severity based on context and match types"""
        # Execution patterns are always ERROR
        if "execution" in matches:
            return IssueSeverity.CRITICAL

        # In high-confidence ML context, downgrade some warnings
        if ml_context.get("confidence", 0) >= 2:
            # In ML context, file_access and network_access are less concerning
            if all(match in ["file_access", "network_access"] for match in matches):
                return IssueSeverity.INFO

        # Credentials are high priority
        if "credentials" in matches:
            return IssueSeverity.WARNING

        return IssueSeverity.WARNING

    def _format_value(self, value: Any) -> str:
        """Format a value for display, truncating if necessary"""
        str_value = str(value)
        if len(str_value) > 100:
            return str_value[:100] + "..."
        return str_value

```


### modelaudit/scanners/numpy_scanner.py

```python
from __future__ import annotations

import sys

import numpy.lib.format as fmt

from .base import BaseScanner, IssueSeverity, ScanResult


class NumPyScanner(BaseScanner):
    """Scanner for NumPy binary files (.npy)."""

    name = "numpy"
    description = "Scans NumPy .npy files for integrity issues"
    supported_extensions = [".npy"]

    def __init__(self, config=None):
        super().__init__(config)
        # Security limits
        self.max_array_bytes = self.config.get(
            "max_array_bytes", 1024 * 1024 * 1024
        )  # 1GB
        self.max_dimensions = self.config.get("max_dimensions", 32)
        self.max_dimension_size = self.config.get("max_dimension_size", 100_000_000)
        self.max_itemsize = self.config.get("max_itemsize", 1024)  # 1KB per element

    def _validate_array_dimensions(self, shape: tuple) -> None:
        """Validate array dimensions for security"""
        # Check number of dimensions
        if len(shape) > self.max_dimensions:
            raise ValueError(
                f"Too many dimensions: {len(shape)} (max: {self.max_dimensions})"
            )

        # Check individual dimension sizes
        for i, dim in enumerate(shape):
            if dim < 0:
                raise ValueError(f"Negative dimension at index {i}: {dim}")
            if dim > self.max_dimension_size:
                raise ValueError(
                    f"Dimension {i} too large: {dim} (max: {self.max_dimension_size})"
                )

    def _validate_dtype(self, dtype) -> None:
        """Validate numpy dtype for security"""
        # Check for problematic data types
        dangerous_names = ["object"]
        dangerous_kinds = ["O", "V"]  # Object and Void kinds

        if dtype.name in dangerous_names or dtype.kind in dangerous_kinds:
            raise ValueError(
                f"Dangerous dtype not allowed: {dtype.name} (kind: {dtype.kind})"
            )

        # Check for extremely large item sizes
        if dtype.itemsize > self.max_itemsize:
            raise ValueError(
                f"Itemsize too large: {dtype.itemsize} bytes (max: {self.max_itemsize})"
            )

    def _calculate_safe_array_size(self, shape: tuple, dtype) -> int:
        """Calculate array size with overflow protection"""
        total_elements = 1
        max_elements = sys.maxsize // max(dtype.itemsize, 1)

        for dim in shape:
            # Check for overflow before multiplication
            if total_elements > max_elements // max(dim, 1):
                raise ValueError(
                    f"Array size would overflow: shape={shape}, dtype={dtype}"
                )

            total_elements *= dim

        total_bytes = total_elements * dtype.itemsize

        if total_bytes > self.max_array_bytes:
            raise ValueError(
                f"Array too large: {total_bytes} bytes "
                f"(max: {self.max_array_bytes}) for shape={shape}, dtype={dtype}"
            )

        return total_bytes

    def scan(self, path: str) -> ScanResult:
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            self.current_file_path = path
            with open(path, "rb") as f:
                # Verify magic string
                magic = f.read(6)
                if magic != b"\x93NUMPY":
                    result.add_issue(
                        "Invalid NumPy file magic",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result
                f.seek(0)
                major, minor = fmt.read_magic(f)
                if (major, minor) == (1, 0):
                    shape, fortran, dtype = fmt.read_array_header_1_0(f)
                elif (major, minor) == (2, 0):
                    shape, fortran, dtype = fmt.read_array_header_2_0(f)
                else:
                    shape, fortran, dtype = fmt._read_array_header(  # type: ignore[attr-defined]
                        f, version=(major, minor)
                    )
                data_offset = f.tell()

                # Validate array dimensions and dtype for security
                try:
                    self._validate_array_dimensions(shape)
                    self._validate_dtype(dtype)
                    expected_data_size = self._calculate_safe_array_size(shape, dtype)
                    expected_size = data_offset + expected_data_size
                except ValueError as e:
                    result.add_issue(
                        f"Array validation failed: {e}",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                        details={
                            "security_check": "array_validation",
                            "shape": shape,
                            "dtype": str(dtype),
                        },
                    )
                    result.finish(success=False)
                    return result

                if file_size != expected_size:
                    result.add_issue(
                        "File size does not match header information",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                        details={
                            "expected_size": expected_size,
                            "actual_size": file_size,
                            "shape": shape,
                            "dtype": str(dtype),
                        },
                    )

                # Note: Dimension validation is now handled in _validate_array_dimensions
                # which is called earlier and has configurable limits

                result.bytes_scanned = file_size
                result.metadata.update(
                    {"shape": shape, "dtype": str(dtype), "fortran_order": fortran}
                )
        except Exception as e:  # pragma: no cover - unexpected errors
            result.add_issue(
                f"Error scanning NumPy file: {e}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

```


### modelaudit/scanners/oci_layer_scanner.py

```python
import json
import os
import tarfile
import tempfile
from typing import Any

from ..utils import sanitize_archive_path
from .base import BaseScanner, IssueSeverity, ScanResult

# Try to import yaml for YAML manifests
try:
    import yaml  # type: ignore

    HAS_YAML = True
except Exception:
    HAS_YAML = False


class OciLayerScanner(BaseScanner):
    """Scanner for OCI/Artifactory manifest files with .tar.gz layers."""

    name = "oci_layer"
    description = "Scans container manifests and embedded layers for model files"
    supported_extensions = [".manifest"]

    @classmethod
    def can_handle(cls, path: str) -> bool:
        if not os.path.isfile(path):
            return False
        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False
        # Quick check for .tar.gz references to avoid conflicts with ManifestScanner
        try:
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                snippet = f.read(2048)
            return ".tar.gz" in snippet
        except Exception:
            return False

    def scan(self, path: str) -> ScanResult:
        path_check = self._check_path(path)
        if path_check:
            return path_check

        result = self._create_result()
        manifest_data: Any = None

        try:
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                text = f.read()
            try:
                manifest_data = json.loads(text)
            except Exception:
                if HAS_YAML:
                    manifest_data = yaml.safe_load(text)
                else:
                    raise
        except Exception as e:
            result.add_issue(
                f"Error parsing manifest: {e}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        # Find layer paths ending with .tar.gz
        layer_paths: list[str] = []

        def _search(obj: Any) -> None:
            if isinstance(obj, dict):
                for v in obj.values():
                    _search(v)
            elif isinstance(obj, list):
                for item in obj:
                    _search(item)
            elif isinstance(obj, str) and obj.endswith(".tar.gz"):
                layer_paths.append(obj)

        _search(manifest_data)

        manifest_dir = os.path.dirname(path)

        for layer_ref in layer_paths:
            if os.path.isabs(layer_ref):
                layer_path = layer_ref
                is_safe = True
            else:
                layer_path, is_safe = sanitize_archive_path(layer_ref, manifest_dir)

            if not is_safe:
                result.add_issue(
                    f"Layer reference {layer_ref} attempted path traversal outside manifest directory",
                    severity=IssueSeverity.CRITICAL,
                    location=f"{path}:{layer_ref}",
                    details={"layer": layer_ref},
                )
                continue

            if not os.path.exists(layer_path):
                result.add_issue(
                    f"Layer not found: {layer_ref}",
                    severity=IssueSeverity.WARNING,
                    location=f"{path}:{layer_ref}",
                )
                continue
            try:
                from . import SCANNER_REGISTRY

                with tarfile.open(layer_path, "r:gz") as tar:
                    for member in tar:
                        if not member.isfile():
                            continue
                        name = member.name
                        _, ext = os.path.splitext(name)
                        if not any(s.can_handle(name) for s in SCANNER_REGISTRY):
                            continue
                        fileobj = tar.extractfile(member)
                        if fileobj is None:
                            continue
                        with tempfile.NamedTemporaryFile(
                            suffix=ext, delete=False
                        ) as tmp:
                            tmp.write(fileobj.read())
                            tmp_path = tmp.name
                        fileobj.close()
                        try:
                            from .. import core

                            file_result = core.scan_file(tmp_path, self.config)
                            for issue in file_result.issues:
                                if issue.location:
                                    issue.location = (
                                        f"{path}:{layer_ref}:{name} {issue.location}"
                                    )
                                else:
                                    issue.location = f"{path}:{layer_ref}:{name}"
                                if issue.details is None:
                                    issue.details = {}
                                issue.details["layer"] = layer_ref
                            result.merge(file_result)
                        finally:
                            os.unlink(tmp_path)
            except Exception as e:
                result.add_issue(
                    f"Error processing layer {layer_ref}: {e}",
                    severity=IssueSeverity.WARNING,
                    location=f"{path}:{layer_ref}",
                    details={"exception_type": type(e).__name__},
                )

        result.finish(success=True)
        return result

```


### modelaudit/scanners/onnx_scanner.py

```python
import os
from pathlib import Path
from typing import Any

from .base import BaseScanner, IssueSeverity, ScanResult

try:
    import numpy as np
    import onnx
    from onnx import mapping

    HAS_ONNX = True
except Exception:
    HAS_ONNX = False


class OnnxScanner(BaseScanner):
    """Scanner for ONNX model files."""

    name = "onnx"
    description = "Scans ONNX models for custom operators and integrity issues"
    supported_extensions = [".onnx"]

    @classmethod
    def can_handle(cls, path: str) -> bool:
        if not HAS_ONNX:
            return False
        if not os.path.isfile(path):
            return False
        return os.path.splitext(path)[1].lower() in cls.supported_extensions

    def scan(self, path: str) -> ScanResult:
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        if not HAS_ONNX:
            result.add_issue(
                "onnx package not installed, cannot scan ONNX files.",
                severity=IssueSeverity.CRITICAL,
                location=path,
            )
            result.finish(success=False)
            return result

        try:
            model = onnx.load(path, load_external_data=False)
            result.bytes_scanned = file_size
        except Exception as e:  # pragma: no cover - unexpected parse errors
            result.add_issue(
                f"Error parsing ONNX model: {e}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.metadata.update(
            {
                "ir_version": model.ir_version,
                "producer_name": model.producer_name,
                "node_count": len(model.graph.node),
            }
        )

        self._check_custom_ops(model, path, result)
        self._check_external_data(model, path, result)
        self._check_tensor_sizes(model, path, result)

        result.finish(success=True)
        return result

    def _check_custom_ops(self, model: Any, path: str, result: ScanResult) -> None:
        custom_domains = set()
        for node in model.graph.node:
            if node.domain and node.domain not in ("", "ai.onnx"):
                custom_domains.add(node.domain)
                result.add_issue(
                    f"Model uses custom operator domain '{node.domain}'",
                    severity=IssueSeverity.WARNING,
                    location=f"{path} (node: {node.name})",
                    details={"op_type": node.op_type, "domain": node.domain},
                )
        if custom_domains:
            result.metadata["custom_domains"] = sorted(custom_domains)

    def _check_external_data(self, model: Any, path: str, result: ScanResult) -> None:
        model_dir = Path(path).resolve().parent
        for tensor in model.graph.initializer:
            if tensor.data_location == onnx.TensorProto.EXTERNAL:
                info = {entry.key: entry.value for entry in tensor.external_data}
                location = info.get("location")
                if location is None:
                    result.add_issue(
                        f"Tensor '{tensor.name}' uses external data without location",
                        severity=IssueSeverity.WARNING,
                        location=path,
                        details={"tensor": tensor.name},
                    )
                    continue
                external_path = (model_dir / location).resolve()
                if not external_path.exists():
                    result.add_issue(
                        f"External data file not found for tensor '{tensor.name}'",
                        severity=IssueSeverity.CRITICAL,
                        location=str(external_path),
                        details={"tensor": tensor.name, "file": location},
                    )
                elif not str(external_path).startswith(str(model_dir)):
                    result.add_issue(
                        f"External data file outside model directory for tensor '{tensor.name}'",
                        severity=IssueSeverity.CRITICAL,
                        location=str(external_path),
                        details={"tensor": tensor.name, "file": location},
                    )
                else:
                    self._validate_external_size(tensor, external_path, result)

    def _validate_external_size(
        self, tensor: Any, external_path: Path, result: ScanResult
    ) -> None:
        try:
            dtype = np.dtype(mapping.TENSOR_TYPE_TO_NP_TYPE[tensor.data_type])
            num_elem = 1
            for d in tensor.dims:
                num_elem *= d
            expected_size = int(num_elem) * int(dtype.itemsize)
            actual_size = external_path.stat().st_size
            if actual_size < expected_size:
                result.add_issue(
                    "External data file size mismatch",
                    severity=IssueSeverity.CRITICAL,
                    location=str(external_path),
                    details={
                        "tensor": tensor.name,
                        "expected_size": expected_size,
                        "actual_size": actual_size,
                    },
                )
        except Exception as e:
            result.add_issue(
                f"Could not validate external data size: {e}",
                severity=IssueSeverity.DEBUG,
                location=str(external_path),
            )

    def _check_tensor_sizes(self, model: Any, path: str, result: ScanResult) -> None:
        for tensor in model.graph.initializer:
            if tensor.data_location == onnx.TensorProto.EXTERNAL:
                continue
            if tensor.raw_data:
                try:
                    dtype = np.dtype(mapping.TENSOR_TYPE_TO_NP_TYPE[tensor.data_type])
                    num_elem = 1
                    for d in tensor.dims:
                        num_elem *= d
                    expected_size = int(num_elem) * int(dtype.itemsize)
                    actual_size = len(tensor.raw_data)
                    if actual_size < expected_size:
                        result.add_issue(
                            f"Tensor '{tensor.name}' data appears truncated",
                            severity=IssueSeverity.CRITICAL,
                            location=f"{path} (tensor: {tensor.name})",
                            details={
                                "expected_size": expected_size,
                                "actual_size": actual_size,
                            },
                        )
                except Exception as e:
                    result.add_issue(
                        f"Could not validate tensor '{tensor.name}': {e}",
                        severity=IssueSeverity.DEBUG,
                        location=path,
                    )

```


### modelaudit/scanners/pickle_scanner.py

```python
import logging
import os
import pickletools
import struct
import time
from typing import Any, BinaryIO, Dict, List, Optional, Union

from modelaudit.suspicious_symbols import (
    SUSPICIOUS_GLOBALS,
    SUSPICIOUS_STRING_PATTERNS,
)

from ..explanations import (
    get_import_explanation,
    get_opcode_explanation,
    get_pattern_explanation,
)
from ..suspicious_symbols import DANGEROUS_OPCODES
from .base import BaseScanner, IssueSeverity, ScanResult

logger = logging.getLogger(__name__)
# ============================================================================
# SMART DETECTION SYSTEM - ML Context Awareness
# ============================================================================

# ML Framework Detection Patterns
ML_FRAMEWORK_PATTERNS: Dict[str, Dict[str, Union[List[str], float]]] = {
    "pytorch": {
        "modules": [
            "torch",
            "torchvision",
            "torch.nn",
            "torch.optim",
            "torch.utils",
            "_pickle",
        ],
        "classes": [
            "OrderedDict",
            "Parameter",
            "Module",
            "Linear",
            "Conv2d",
            "BatchNorm2d",
            "ReLU",
            "MaxPool2d",
            "AdaptiveAvgPool2d",
            "Sequential",
            "ModuleList",
        ],
        "patterns": [r"torch\..*", r"_pickle\..*", r"collections\.OrderedDict"],
        "confidence_boost": 0.8,
    },
    "yolo": {
        "modules": ["ultralytics", "yolo", "models"],
        "classes": ["YOLO", "YOLOv8", "Detect", "C2f", "Conv", "Bottleneck", "SPPF"],
        "patterns": [
            r"yolo.*",
            r"ultralytics\..*",
            r".*\.detect",
            r".*\.backbone",
            r".*\.head",
        ],
        "confidence_boost": 0.9,
    },
    "tensorflow": {
        "modules": ["tensorflow", "keras", "tf"],
        "classes": ["Model", "Layer", "Dense", "Conv2D", "Flatten"],
        "patterns": [r"tensorflow\..*", r"keras\..*"],
        "confidence_boost": 0.8,
    },
    "sklearn": {
        "modules": ["sklearn", "joblib"],
        "classes": ["Pipeline", "StandardScaler", "PCA"],
        "patterns": [r"sklearn\..*", r"joblib\..*"],
        "confidence_boost": 0.7,
    },
    "huggingface": {
        "modules": ["transformers", "tokenizers"],
        "classes": ["AutoModel", "AutoTokenizer", "BertModel", "GPT2Model"],
        "patterns": [r"transformers\..*", r"tokenizers\..*"],
        "confidence_boost": 0.8,
    },
}

# Safe ML-specific global patterns
ML_SAFE_GLOBALS: Dict[str, List[str]] = {
    # PyTorch safe patterns
    "torch": ["*"],  # All torch functions are generally safe
    "torch.nn": ["*"],
    "torch.optim": ["*"],
    "torch.utils": ["*"],
    "_pickle": ["*"],  # PyTorch uses _pickle internally
    "collections": ["OrderedDict", "defaultdict", "namedtuple"],
    "typing": ["*"],
    "numpy": ["*"],  # NumPy operations are safe
    "math": ["*"],  # Math operations are safe
    # YOLO/Ultralytics safe patterns
    "ultralytics": ["*"],
    "yolo": ["*"],
    # Standard ML libraries
    "sklearn": ["*"],
    "transformers": ["*"],
    "tokenizers": ["*"],
    "joblib": [
        "dump",
        "load",
        "Parallel",
        "delayed",
        "Memory",
        "hash",
        "_pickle_dump",
        "_pickle_load",
    ],
    "dill": ["dump", "dumps", "load", "loads", "copy"],
    "tensorflow": ["*"],
    "keras": ["*"],
}

# Dangerous actual code execution patterns in strings
ACTUAL_DANGEROUS_STRING_PATTERNS = [
    r"os\.system\s*\(",
    r"subprocess\.",
    r"exec\s*\(",
    r"eval\s*\(",
    r"__import__\s*\(",
    r"compile\s*\(",
    r"open\s*\(['\"].*['\"],\s*['\"]w",  # File write operations
    r"\.popen\s*\(",
    r"\.spawn\s*\(",
]


def _detect_ml_context(opcodes: list[tuple]) -> dict[str, Any]:
    """
    Detect ML framework context from opcodes with confidence scoring.
    Uses improved scoring that focuses on presence and diversity of ML patterns
    rather than their proportion of total opcodes.
    """
    context: dict[str, Any] = {
        "frameworks": {},
        "overall_confidence": 0.0,
        "is_ml_content": False,
        "detected_patterns": [],
    }

    total_opcodes = len(opcodes)
    if total_opcodes == 0:
        return context

    # Analyze GLOBAL opcodes for ML patterns
    global_refs: dict[str, int] = {}
    total_global_opcodes = 0

    for opcode, arg, pos in opcodes:
        if opcode.name == "GLOBAL" and isinstance(arg, str):
            total_global_opcodes += 1
            # Extract module name from global reference
            if "." in arg:
                module = arg.split(".")[0]
            elif " " in arg:
                module = arg.split(" ")[0]
            else:
                module = arg

            global_refs[module] = global_refs.get(module, 0) + 1

    # Check each framework with improved scoring
    for framework, patterns in ML_FRAMEWORK_PATTERNS.items():
        framework_score = 0.0
        matches: list[str] = []

        # Check module matches with improved scoring
        modules = patterns["modules"]
        if isinstance(modules, list):
            for module in modules:
                if module in global_refs:
                    # Score based on presence and frequency,
                    # not proportion of total opcodes
                    ref_count = global_refs[module]

                    # Base score for presence
                    module_score = 10.0  # Base score for any ML module presence

                    # Bonus for frequency (up to 20 more points)
                    if ref_count >= 5:
                        module_score += 20.0
                    elif ref_count >= 2:
                        module_score += 10.0
                    elif ref_count >= 1:
                        module_score += 5.0

                    framework_score += module_score
                    matches.append(f"module:{module}({ref_count})")

        # Store framework detection with much lower threshold
        if framework_score > 5.0:  # Much lower threshold - any ML module presence
            # Normalize confidence to 0-1 range
            confidence_boost = patterns["confidence_boost"]
            if isinstance(confidence_boost, (int, float)):
                confidence = min(framework_score / 100.0 * confidence_boost, 1.0)
                context["frameworks"][framework] = {
                    "confidence": confidence,
                    "matches": matches,
                    "raw_score": framework_score,
                }
                context["detected_patterns"].extend(matches)

    # Calculate overall ML confidence - highest framework confidence
    if context["frameworks"]:
        context["overall_confidence"] = max(
            fw["confidence"] for fw in context["frameworks"].values()
        )
        # Much more lenient threshold - any significant ML pattern detection
        context["is_ml_content"] = context["overall_confidence"] > 0.15  # Was 0.3

    return context


def _is_actually_dangerous_global(mod: str, func: str, ml_context: dict) -> bool:
    """
    Smart global reference analysis - distinguishes between legitimate ML operations
    and actual dangerous operations.
    """
    # If we have high ML confidence, be more lenient with "suspicious" globals
    if (
        ml_context.get("is_ml_content")
        and ml_context.get("overall_confidence", 0) > 0.5
    ):
        # Check if this is a known safe ML global
        if mod in ML_SAFE_GLOBALS:
            safe_funcs = ML_SAFE_GLOBALS[mod]
            if safe_funcs == ["*"] or func in safe_funcs:
                return False

    # Use original suspicious global check for genuinely suspicious patterns
    return is_suspicious_global(mod, func)


def _is_actually_dangerous_string(s: str, ml_context: dict) -> Optional[str]:
    """
    Smart string analysis - looks for actual executable code rather than ML patterns.
    """
    import re

    if not isinstance(s, str):
        return None

    # Check for ACTUAL dangerous patterns (not just ML magic methods)
    for pattern in ACTUAL_DANGEROUS_STRING_PATTERNS:
        if re.search(pattern, s, re.IGNORECASE):
            return pattern

    # If we have strong ML context, ignore common ML patterns
    if (
        ml_context.get("is_ml_content")
        and ml_context.get("overall_confidence", 0) > 0.6
    ):
        # Skip common ML magic method patterns
        if re.match(r"^__\w+__$", s):  # Simple magic methods like __call__, __init__
            return None

        # Skip tensor/layer names
        if any(
            term in s.lower()
            for term in ["layer", "conv", "batch", "norm", "relu", "pool", "linear"]
        ):
            return None

    # Check for base64-like strings (still suspicious)
    if len(s) > 100 and re.match(r"^[A-Za-z0-9+/=]+$", s):
        return "potential_base64"

    return None


def _should_ignore_opcode_sequence(opcodes: list[tuple], ml_context: dict) -> bool:
    """
    Determine if an opcode sequence should be ignored based on ML context.
    """
    if not ml_context.get("is_ml_content"):
        return False

    # High confidence ML content - be very permissive with opcode sequences
    if ml_context.get("overall_confidence", 0) > 0.7:
        return True

    # Medium confidence - check for specific ML patterns
    if ml_context.get("overall_confidence", 0) > 0.4:
        # Look for legitimate ML construction patterns
        reduce_count = sum(1 for opcode, _, _ in opcodes if opcode.name == "REDUCE")
        global_count = sum(1 for opcode, _, _ in opcodes if opcode.name == "GLOBAL")

        # High REDUCE/GLOBAL ratio suggests ML object construction
        if global_count > 0 and reduce_count / global_count > 0.5:
            return True

    return False


def _get_context_aware_severity(
    base_severity: IssueSeverity, ml_context: dict
) -> IssueSeverity:
    """
    Adjust severity based on ML context confidence.
    """
    if not ml_context.get("is_ml_content"):
        return base_severity

    confidence = ml_context.get("overall_confidence", 0)

    # High confidence ML content - downgrade severity
    if confidence > 0.8:
        if base_severity == IssueSeverity.CRITICAL:
            return IssueSeverity.WARNING
        elif base_severity == IssueSeverity.WARNING:
            return IssueSeverity.INFO
    elif confidence > 0.5:
        if base_severity == IssueSeverity.CRITICAL:
            return IssueSeverity.WARNING

    return base_severity


# ============================================================================
# END SMART DETECTION SYSTEM
# ============================================================================


def _is_legitimate_serialization_file(path: str) -> bool:
    """
    Validate that a file is a legitimate joblib or dill serialization file.
    This helps prevent security bypass by simply renaming malicious files.
    """
    try:
        with open(path, "rb") as f:
            # Read first few bytes to check for pickle magic
            header = f.read(10)
            if not header:
                return False

            # Check for standard pickle protocols (0-5)
            # Protocol 0: starts with '(' or other opcodes
            # Protocol 1: starts with ']' or other opcodes
            # Protocol 2+: starts with '\x80' followed by protocol number
            first_byte = header[0:1]
            if first_byte == b"\x80":
                # Protocols 2-5 start with \x80 followed by protocol number
                if len(header) < 2 or header[1] not in (2, 3, 4, 5):
                    return False
            elif first_byte not in (b"(", b"]", b"}", b"c", b"l", b"d", b"t", b"p"):
                # Common pickle opcode starts for protocols 0-1
                return False

            # For joblib files, look for joblib-specific patterns
            if path.lower().endswith(".joblib"):
                f.seek(0)
                # Try to find joblib-specific markers in first 2KB
                sample = f.read(2048)
                # Look for joblib-specific indicators
                joblib_indicators = [
                    b"joblib",
                    b"sklearn",
                    b"numpy",
                    b"_joblib",
                    b"__main__",
                    b"_pickle",
                    b"NumpyArrayWrapper",
                ]
                return any(marker in sample for marker in joblib_indicators)

            # For dill files, they're usually just enhanced pickle
            elif path.lower().endswith(".dill"):
                # Dill files should contain standard pickle format
                # Additional validation could check for dill-specific patterns
                return True

        return False
    except (OSError, IOError):
        # File doesn't exist or can't be read
        return False
    except Exception:
        # Other errors (e.g., permissions) - be conservative
        return False


def is_suspicious_global(mod: str, func: str) -> bool:
    """Check if a module.function reference is suspicious"""
    if mod in SUSPICIOUS_GLOBALS:
        val = SUSPICIOUS_GLOBALS[mod]
        if val == "*":
            return True
        if isinstance(val, list) and func in val:
            return True
    return False


def is_suspicious_string(s: str) -> Optional[str]:
    """Check if a string contains suspicious patterns"""
    import re

    if not isinstance(s, str):
        return None

    for pattern in SUSPICIOUS_STRING_PATTERNS:
        match = re.search(pattern, s)
        if match:
            return pattern

    # Check for base64-like strings (long strings with base64 charset)
    if len(s) > 40 and re.match(r"^[A-Za-z0-9+/=]+$", s):
        return "potential_base64"

    return None


def is_dangerous_reduce_pattern(opcodes: list[tuple]) -> Optional[dict[str, Any]]:
    """
    Check for patterns that indicate a dangerous __reduce__ method
    Returns details about the dangerous pattern if found, None otherwise
    """
    # Look for common patterns in __reduce__ exploits
    for i, (opcode, arg, pos) in enumerate(opcodes):
        # Check for GLOBAL followed by REDUCE - common in exploits
        if (
            opcode.name == "GLOBAL"
            and i + 1 < len(opcodes)
            and opcodes[i + 1][0].name == "REDUCE"
        ):
            if isinstance(arg, str):
                parts = (
                    arg.split(" ", 1)
                    if " " in arg
                    else arg.rsplit(".", 1)
                    if "." in arg
                    else [arg, ""]
                )
                if len(parts) == 2:
                    mod, func = parts
                    return {
                        "pattern": "GLOBAL+REDUCE",
                        "module": mod,
                        "function": func,
                        "position": pos,
                        "opcode": opcode.name,
                    }

        # Check for INST or OBJ opcodes which can also be used for code execution
        if opcode.name in ["INST", "OBJ", "NEWOBJ"] and isinstance(arg, str):
            return {
                "pattern": f"{opcode.name}_EXECUTION",
                "argument": arg,
                "position": pos,
                "opcode": opcode.name,
            }

        # Check for suspicious attribute access patterns (GETATTR followed by CALL)
        if (
            opcode.name == "GETATTR"
            and i + 1 < len(opcodes)
            and opcodes[i + 1][0].name == "CALL"
        ):
            return {
                "pattern": "GETATTR+CALL",
                "attribute": arg,
                "position": pos,
                "opcode": opcode.name,
            }

        # Check for suspicious strings in STRING or BINSTRING opcodes
        if opcode.name in [
            "STRING",
            "BINSTRING",
            "SHORT_BINSTRING",
            "UNICODE",
        ] and isinstance(arg, str):
            suspicious_pattern = is_suspicious_string(arg)
            if suspicious_pattern:
                return {
                    "pattern": "SUSPICIOUS_STRING",
                    "string_pattern": suspicious_pattern,
                    "string_preview": arg[:50] + ("..." if len(arg) > 50 else ""),
                    "position": pos,
                    "opcode": opcode.name,
                }

    return None


def check_opcode_sequence(
    opcodes: list[tuple], ml_context: dict
) -> list[dict[str, Any]]:
    """
    Analyze the full sequence of opcodes for suspicious patterns
    with ML context awareness.
    Returns a list of suspicious patterns found.
    """
    suspicious_patterns: list[dict[str, Any]] = []

    # SMART DETECTION: Check if we should ignore this sequence based on ML context
    if _should_ignore_opcode_sequence(opcodes, ml_context):
        return suspicious_patterns  # Return empty list for legitimate ML content

    # Count dangerous opcodes with ML context awareness
    dangerous_opcode_count = 0
    consecutive_dangerous = 0
    max_consecutive = 0

    for i, (opcode, arg, pos) in enumerate(opcodes):
        # Track dangerous opcodes
        if opcode.name in DANGEROUS_OPCODES:
            dangerous_opcode_count += 1
            consecutive_dangerous += 1
            max_consecutive = max(max_consecutive, consecutive_dangerous)
        else:
            consecutive_dangerous = 0

        # SMART DETECTION: Much higher threshold for ML content
        ml_confidence = ml_context.get("overall_confidence", 0)
        if ml_confidence > 0.7:
            threshold = 100  # Very high threshold for high-confidence ML
        elif ml_confidence > 0.4:
            threshold = 50  # Higher threshold for medium-confidence ML
        else:
            threshold = 20  # Still higher than original 5 for unknown content

        # If we see too many dangerous opcodes AND it's not clearly ML content
        if dangerous_opcode_count > threshold:
            suspicious_patterns.append(
                {
                    "pattern": "MANY_DANGEROUS_OPCODES",
                    "count": dangerous_opcode_count,
                    "max_consecutive": max_consecutive,
                    "ml_confidence": ml_confidence,
                    "position": pos,
                    "opcode": opcode.name,
                },
            )
            # Reset counter to avoid multiple alerts
            dangerous_opcode_count = 0
            max_consecutive = 0

    return suspicious_patterns


class PickleScanner(BaseScanner):
    """Scanner for Python Pickle files"""

    name = "pickle"
    description = "Scans Python pickle files for suspicious code references"
    supported_extensions = [
        ".pkl",
        ".pickle",
        ".dill",
        ".joblib",
        ".bin",
        ".pt",
        ".pth",
        ".ckpt",
    ]

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        # Additional pickle-specific configuration
        self.max_opcodes = self.config.get("max_opcodes", 1000000)

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if the file is a pickle based on extension and content"""
        file_ext = os.path.splitext(path)[1].lower()

        # For known pickle extensions, always handle
        if file_ext in [".pkl", ".pickle", ".dill", ".joblib"]:
            return True

        # For ambiguous extensions, check the actual file format
        if file_ext in [".bin", ".pt", ".pth", ".ckpt"]:
            try:
                # Import here to avoid circular dependency
                from modelaudit.utils.filetype import (
                    detect_file_format,
                    validate_file_type,
                )

                file_format = detect_file_format(path)

                # For security-sensitive pickle files, also validate file type
                # This helps detect potential file spoofing attacks
                if file_format == "pickle" and not validate_file_type(path):
                    # File type validation failed - this could be suspicious
                    # Log but still allow scanning for now (let scanner handle the validation)
                    import logging

                    pickle_logger = logging.getLogger("modelaudit.scanners.pickle")
                    pickle_logger.warning(
                        f"File type validation failed for potential pickle file: {path}"
                    )

                return file_format == "pickle"
            except Exception:
                # If detection fails, fall back to extension check
                return file_ext in cls.supported_extensions

        return False

    def scan(self, path: str) -> ScanResult:
        """Scan a pickle file for suspicious content"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        # Check if this is a .bin file that might be a PyTorch file
        is_bin_file = os.path.splitext(path)[1].lower() == ".bin"

        try:
            with open(path, "rb") as f:
                # Store the file path for use in issue locations
                self.current_file_path = path
                scan_result = self._scan_pickle_bytes(f, file_size)
                result.merge(scan_result)

                # For .bin files, also scan the remaining binary content
                # PyTorch files have pickle header followed by tensor data
                if is_bin_file and scan_result.success:
                    pickle_end_pos = f.tell()
                    remaining_bytes = file_size - pickle_end_pos

                    if remaining_bytes > 0:
                        # Check if this is likely a PyTorch model based on ML context
                        ml_context = scan_result.metadata.get("ml_context", {})
                        is_pytorch = "pytorch" in ml_context.get("frameworks", {})
                        ml_confidence = ml_context.get("overall_confidence", 0)

                        # Skip binary scanning for high-confidence ML model files
                        # as they contain tensor data that can trigger false positives
                        if is_pytorch and ml_confidence > 0.7:
                            result.metadata["binary_scan_skipped"] = True
                            result.metadata["skip_reason"] = (
                                "High-confidence PyTorch model detected"
                            )
                            result.bytes_scanned = file_size
                            result.metadata["pickle_bytes"] = pickle_end_pos
                            result.metadata["binary_bytes"] = remaining_bytes
                        else:
                            # Scan the binary content after pickle
                            binary_result = self._scan_binary_content(
                                f, pickle_end_pos, file_size
                            )

                            # Add binary scanning results
                            for issue in binary_result.issues:
                                result.add_issue(
                                    message=issue.message,
                                    severity=issue.severity,
                                    location=issue.location,
                                    details=issue.details,
                                    why=issue.why,
                                )

                            # Update total bytes scanned
                            result.bytes_scanned = file_size
                            result.metadata["pickle_bytes"] = pickle_end_pos
                            result.metadata["binary_bytes"] = remaining_bytes

        except Exception as e:
            result.add_issue(
                f"Error opening pickle file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _scan_pickle_bytes(self, file_obj: BinaryIO, file_size: int) -> ScanResult:
        """Scan pickle file content for suspicious opcodes"""
        result = self._create_result()
        opcode_count = 0
        suspicious_count = 0

        try:
            # Process the pickle
            start_pos = file_obj.tell()

            # Store opcodes for pattern analysis
            opcodes = []
            # Track strings on the stack for STACK_GLOBAL opcode analysis
            string_stack = []

            for opcode, arg, pos in pickletools.genops(file_obj):
                opcodes.append((opcode, arg, pos))
                opcode_count += 1

                # Track strings for STACK_GLOBAL analysis
                if opcode.name in [
                    "STRING",
                    "BINSTRING",
                    "SHORT_BINSTRING",
                    "SHORT_BINUNICODE",
                    "UNICODE",
                ] and isinstance(arg, str):
                    string_stack.append(arg)
                    # Keep only the last 10 strings to avoid memory issues
                    if len(string_stack) > 10:
                        string_stack.pop(0)

                # Check for too many opcodes
                if opcode_count > self.max_opcodes:
                    result.add_issue(
                        f"Too many opcodes in pickle (> {self.max_opcodes})",
                        severity=IssueSeverity.INFO,
                        location=self.current_file_path,
                        details={
                            "opcode_count": opcode_count,
                            "max_opcodes": self.max_opcodes,
                        },
                        why=get_pattern_explanation("pickle_size_limit"),
                    )
                    break

                # Check for timeout
                if time.time() - result.start_time > self.timeout:
                    result.add_issue(
                        f"Scanning timed out after {self.timeout} seconds",
                        severity=IssueSeverity.INFO,
                        location=self.current_file_path,
                        details={"opcode_count": opcode_count, "timeout": self.timeout},
                        why="The scan exceeded the configured time limit. Large or complex pickle files may take longer to analyze due to the number of opcodes that need to be processed.",
                    )
                    break

            # SMART DETECTION: Analyze ML context once for the entire pickle
            ml_context = _detect_ml_context(opcodes)

            # Add ML context to metadata for debugging
            result.metadata.update(
                {
                    "ml_context": ml_context,
                    "opcode_count": opcode_count,
                    "suspicious_count": suspicious_count,
                }
            )

            # Now analyze the collected opcodes with ML context awareness
            for opcode, arg, pos in opcodes:
                # Check for GLOBAL opcodes that might reference suspicious modules
                if opcode.name == "GLOBAL":
                    if isinstance(arg, str):
                        # Handle both "module function" and "module.function" formats
                        parts = (
                            arg.split(" ", 1)
                            if " " in arg
                            else arg.rsplit(".", 1)
                            if "." in arg
                            else [arg, ""]
                        )

                        if len(parts) == 2:
                            mod, func = parts
                            if _is_actually_dangerous_global(mod, func, ml_context):
                                suspicious_count += 1
                                severity = _get_context_aware_severity(
                                    IssueSeverity.CRITICAL, ml_context
                                )
                                result.add_issue(
                                    f"Suspicious reference {mod}.{func}",
                                    severity=severity,
                                    location=f"{self.current_file_path} (pos {pos})",
                                    details={
                                        "module": mod,
                                        "function": func,
                                        "position": pos,
                                        "opcode": opcode.name,
                                        "ml_context_confidence": ml_context.get(
                                            "overall_confidence", 0
                                        ),
                                    },
                                    why=get_import_explanation(mod),
                                )

                # SMART DETECTION: Only flag REDUCE opcodes if not clearly ML content
                if opcode.name == "REDUCE" and not ml_context.get(
                    "is_ml_content", False
                ):
                    severity = _get_context_aware_severity(
                        IssueSeverity.WARNING, ml_context
                    )
                    result.add_issue(
                        "Found REDUCE opcode - potential __reduce__ method execution",
                        severity=severity,
                        location=f"{self.current_file_path} (pos {pos})",
                        details={
                            "position": pos,
                            "opcode": opcode.name,
                            "ml_context_confidence": ml_context.get(
                                "overall_confidence", 0
                            ),
                        },
                        why=get_opcode_explanation("REDUCE"),
                    )

                # SMART DETECTION: Only flag other dangerous opcodes
                # if not clearly ML content
                if opcode.name in ["INST", "OBJ", "NEWOBJ"] and not ml_context.get(
                    "is_ml_content", False
                ):
                    severity = _get_context_aware_severity(
                        IssueSeverity.WARNING, ml_context
                    )
                    result.add_issue(
                        f"Found {opcode.name} opcode - potential code execution",
                        severity=severity,
                        location=f"{self.current_file_path} (pos {pos})",
                        details={
                            "position": pos,
                            "opcode": opcode.name,
                            "argument": str(arg),
                            "ml_context_confidence": ml_context.get(
                                "overall_confidence", 0
                            ),
                        },
                        why=get_opcode_explanation(opcode.name),
                    )

                # Check for suspicious strings
                if opcode.name in [
                    "STRING",
                    "BINSTRING",
                    "SHORT_BINSTRING",
                    "UNICODE",
                ] and isinstance(arg, str):
                    suspicious_pattern = _is_actually_dangerous_string(arg, ml_context)
                    if suspicious_pattern:
                        severity = _get_context_aware_severity(
                            IssueSeverity.WARNING, ml_context
                        )
                        result.add_issue(
                            f"Suspicious string pattern: {suspicious_pattern}",
                            severity=severity,
                            location=f"{self.current_file_path} (pos {pos})",
                            details={
                                "position": pos,
                                "opcode": opcode.name,
                                "pattern": suspicious_pattern,
                                "string_preview": arg[:50]
                                + ("..." if len(arg) > 50 else ""),
                                "ml_context_confidence": ml_context.get(
                                    "overall_confidence", 0
                                ),
                            },
                            why=get_pattern_explanation("encoded_strings")
                            if suspicious_pattern == "potential_base64"
                            else "This string contains patterns that match known security risks such as shell commands, code execution functions, or encoded data.",
                        )

            # Check for STACK_GLOBAL patterns
            # (rebuild from opcodes to get proper context)
            for i, (opcode, arg, pos) in enumerate(opcodes):
                if opcode.name == "STACK_GLOBAL":
                    # Find the two immediately preceding STRING-like opcodes
                    # STACK_GLOBAL expects exactly two strings on the stack:
                    # module and function
                    recent_strings: list[str] = []
                    for j in range(
                        i - 1, max(0, i - 10), -1
                    ):  # Look back at most 10 opcodes
                        prev_opcode, prev_arg, prev_pos = opcodes[j]
                        if prev_opcode.name in [
                            "STRING",
                            "BINSTRING",
                            "SHORT_BINSTRING",
                            "SHORT_BINUNICODE",
                            "UNICODE",
                        ] and isinstance(prev_arg, str):
                            recent_strings.insert(
                                0, prev_arg
                            )  # Insert at beginning to maintain order
                            if len(recent_strings) >= 2:
                                break

                    if len(recent_strings) >= 2:
                        # The two strings are module and function in that order
                        mod = recent_strings[0]  # First string pushed (module)
                        func = recent_strings[1]  # Second string pushed (function)
                        if _is_actually_dangerous_global(mod, func, ml_context):
                            suspicious_count += 1
                            severity = _get_context_aware_severity(
                                IssueSeverity.CRITICAL, ml_context
                            )
                            result.add_issue(
                                f"Suspicious module reference found: {mod}.{func}",
                                severity=severity,
                                location=f"{self.current_file_path} (pos {pos})",
                                details={
                                    "module": mod,
                                    "function": func,
                                    "position": pos,
                                    "opcode": opcode.name,
                                    "ml_context_confidence": ml_context.get(
                                        "overall_confidence", 0
                                    ),
                                },
                                why=get_import_explanation(mod),
                            )
                    else:
                        # Only warn about insufficient context if not ML content
                        if not ml_context.get("is_ml_content", False):
                            result.add_issue(
                                "STACK_GLOBAL opcode found without "
                                "sufficient string context",
                                severity=IssueSeverity.INFO,
                                location=f"{self.current_file_path} (pos {pos})",
                                details={
                                    "position": pos,
                                    "opcode": opcode.name,
                                    "stack_size": len(recent_strings),
                                    "ml_context_confidence": ml_context.get(
                                        "overall_confidence", 0
                                    ),
                                },
                                why="STACK_GLOBAL requires two strings on the stack (module and function name) to import and access module attributes. Insufficient context prevents determining which module is being accessed.",
                            )

            # Check for dangerous patterns in the opcodes
            dangerous_pattern = is_dangerous_reduce_pattern(opcodes)
            if dangerous_pattern and not ml_context.get("is_ml_content", False):
                suspicious_count += 1
                severity = _get_context_aware_severity(
                    IssueSeverity.CRITICAL, ml_context
                )
                module_name = dangerous_pattern.get("module", "")
                result.add_issue(
                    f"Detected dangerous __reduce__ pattern with "
                    f"{dangerous_pattern.get('module', '')}."
                    f"{dangerous_pattern.get('function', '')}",
                    severity=severity,
                    location=f"{self.current_file_path} "
                    f"(pos {dangerous_pattern.get('position', 0)})",
                    details={
                        **dangerous_pattern,
                        "ml_context_confidence": ml_context.get(
                            "overall_confidence", 0
                        ),
                    },
                    why=get_import_explanation(module_name)
                    if module_name
                    else "A dangerous pattern was detected that could execute arbitrary code during unpickling.",
                )

            # Check for suspicious opcode sequences with ML context
            suspicious_sequences = check_opcode_sequence(opcodes, ml_context)
            for sequence in suspicious_sequences:
                suspicious_count += 1
                severity = _get_context_aware_severity(
                    IssueSeverity.WARNING, ml_context
                )
                result.add_issue(
                    f"Suspicious opcode sequence: {sequence.get('pattern', 'unknown')}",
                    severity=severity,
                    location=f"{self.current_file_path} "
                    f"(pos {sequence.get('position', 0)})",
                    details={
                        **sequence,
                        "ml_context_confidence": ml_context.get(
                            "overall_confidence", 0
                        ),
                    },
                    why="This pickle contains an unusually high concentration of opcodes that can execute code (REDUCE, INST, OBJ, NEWOBJ). Such patterns are uncommon in legitimate model files.",
                )

            # Update metadata
            end_pos = file_obj.tell()
            result.bytes_scanned = end_pos - start_pos
            result.metadata.update(
                {"opcode_count": opcode_count, "suspicious_count": suspicious_count},
            )

        except Exception as e:
            # Handle known issues with legitimate serialization files
            file_ext = os.path.splitext(self.current_file_path)[1].lower()

            # Pre-validate file legitimacy to avoid nested exceptions
            is_legitimate_file = False
            if file_ext in {".joblib", ".dill"}:
                try:
                    is_legitimate_file = _is_legitimate_serialization_file(
                        self.current_file_path
                    )
                except Exception:
                    # If validation itself fails, treat as non-legitimate
                    is_legitimate_file = False

            # Check if this is a known benign error in legitimate serialization files
            is_benign_error = (
                isinstance(e, (ValueError, struct.error))
                and any(
                    msg in str(e).lower()
                    for msg in [
                        "unknown opcode",
                        "unpack requires",
                        "truncated",
                        "bad marshal data",
                    ]
                )
                and file_ext in {".joblib", ".dill"}
                and is_legitimate_file
            )

            if is_benign_error:
                # Log for security auditing but treat as non-fatal
                logger.warning(
                    f"Truncated pickle scan of {self.current_file_path}: {e}. "
                    f"This may be due to non-pickle data after STOP opcode."
                )
                result.metadata.update(
                    {
                        "truncated": True,
                        "truncation_reason": "post_stop_data_or_format_issue",
                        "exception_type": type(e).__name__,
                        "exception_message": str(e)[:100],  # Limit message length
                        "validated_format": True,
                    }
                )
                # Still add as info-level issue for transparency
                result.add_issue(
                    f"Scan truncated due to format complexity: {type(e).__name__}",
                    severity=IssueSeverity.INFO,
                    location=self.current_file_path,
                    details={
                        "reason": "post_stop_data_or_format_issue",
                        "opcodes_analyzed": opcode_count,
                        "file_format": file_ext,
                    },
                    why="This file contains data after the pickle STOP opcode or uses format features that cannot be fully analyzed. The analyzable portion was scanned for security issues.",
                )
            else:
                # Treat as critical error for unknown/suspicious cases
                result.add_issue(
                    f"Error analyzing pickle ops: {e}",
                    severity=IssueSeverity.CRITICAL,
                    details={
                        "exception": str(e),
                        "exception_type": type(e).__name__,
                        "file_extension": file_ext,
                        "opcodes_analyzed": opcode_count,
                    },
                )

        return result

    def _scan_binary_content(
        self, file_obj: BinaryIO, start_pos: int, file_size: int
    ) -> ScanResult:
        """Scan the binary content after pickle data for suspicious patterns"""
        result = self._create_result()

        try:
            # Common patterns that might indicate embedded Python code
            code_patterns = [
                b"import os",
                b"import sys",
                b"import subprocess",
                b"eval(",
                b"exec(",
                b"__import__",
                b"compile(",
                b"os.system",
                b"subprocess.call",
                b"subprocess.Popen",
                b"socket.socket",
            ]

            # Executable signatures with additional validation
            # For PE files, we need to check for the full DOS header structure
            # to avoid false positives from random "MZ" bytes in model weights
            executable_sigs = {
                b"\x7fELF": "Linux executable (ELF)",
                b"\xfe\xed\xfa\xce": "macOS executable (Mach-O 32-bit)",
                b"\xfe\xed\xfa\xcf": "macOS executable (Mach-O 64-bit)",
                b"\xcf\xfa\xed\xfe": "macOS executable (Mach-O)",
                b"#!/bin/": "Shell script shebang",
                b"#!/usr/bin/": "Shell script shebang",
            }

            # Read in chunks
            chunk_size = 1024 * 1024  # 1MB chunks
            bytes_scanned = 0

            while True:
                chunk = file_obj.read(chunk_size)
                if not chunk:
                    break

                current_offset = start_pos + bytes_scanned
                bytes_scanned += len(chunk)

                # Check for code patterns
                for pattern in code_patterns:
                    if pattern in chunk:
                        pos = chunk.find(pattern)
                        result.add_issue(
                            f"Suspicious code pattern in binary data: {pattern.decode('ascii', errors='ignore')}",
                            severity=IssueSeverity.INFO,
                            location=f"{self.current_file_path} (offset: {current_offset + pos})",
                            details={
                                "pattern": pattern.decode("ascii", errors="ignore"),
                                "offset": current_offset + pos,
                                "section": "binary_data",
                            },
                            why="Python code patterns found in binary sections of the file. Model weights are typically numeric data and should not contain readable code strings.",
                        )

                # Check for executable signatures
                for sig, description in executable_sigs.items():
                    if sig in chunk:
                        pos = chunk.find(sig)
                        result.add_issue(
                            f"Executable signature found in binary data: {description}",
                            severity=IssueSeverity.CRITICAL,
                            location=f"{self.current_file_path} (offset: {current_offset + pos})",
                            details={
                                "signature": sig.hex(),
                                "description": description,
                                "offset": current_offset + pos,
                                "section": "binary_data",
                            },
                            why="Executable files embedded in model data can run arbitrary code on the system. Model files should contain only serialized weights and configuration data.",
                        )

                # Special check for Windows PE files with more validation
                # to reduce false positives from random "MZ" bytes
                pe_sig = b"MZ"
                if pe_sig in chunk:
                    pos = chunk.find(pe_sig)
                    # For PE files, check if we have enough data to validate DOS header
                    if pos + 64 <= len(chunk):  # DOS header is 64 bytes
                        # Check for "This program cannot be run in DOS mode" string
                        # which appears in all PE files
                        dos_stub_msg = b"This program cannot be run in DOS mode"
                        # Look for this message within reasonable distance from MZ
                        search_end = min(pos + 512, len(chunk))
                        if dos_stub_msg in chunk[pos:search_end]:
                            result.add_issue(
                                "Executable signature found in binary data: Windows executable (PE)",
                                severity=IssueSeverity.CRITICAL,
                                location=f"{self.current_file_path} (offset: {current_offset + pos})",
                                details={
                                    "signature": pe_sig.hex(),
                                    "description": "Windows executable (PE) with valid DOS stub",
                                    "offset": current_offset + pos,
                                    "section": "binary_data",
                                },
                                why="Windows executable files embedded in model data can run arbitrary code on the system. The presence of a valid DOS stub confirms this is an actual PE executable.",
                            )

                # Check for timeout
                if time.time() - result.start_time > self.timeout:
                    result.add_issue(
                        f"Binary scanning timed out after {self.timeout} seconds",
                        severity=IssueSeverity.INFO,
                        location=self.current_file_path,
                        details={
                            "bytes_scanned": start_pos + bytes_scanned,
                            "timeout": self.timeout,
                        },
                        why="The binary content scan exceeded the configured time limit. Large model files may require more time to fully analyze.",
                    )
                    break

            result.bytes_scanned = bytes_scanned

        except Exception as e:
            result.add_issue(
                f"Error scanning binary content: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=self.current_file_path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )

        return result

```


### modelaudit/scanners/pmml_scanner.py

```python
import os
import re

from .base import BaseScanner, IssueSeverity, ScanResult

try:
    from defusedxml import ElementTree as DefusedET  # type: ignore

    HAS_DEFUSEDXML = True
except ImportError:  # pragma: no cover - defusedxml may not be installed
    HAS_DEFUSEDXML = False
    DefusedET = None

# Only import unsafe XML as fallback
if not HAS_DEFUSEDXML:
    import xml.etree.ElementTree as UnsafeET  # type: ignore


SUSPICIOUS_PATTERNS = [
    r"<script",
    r"exec\(",
    r"eval\(",
    r"import\s+os",
    r"subprocess",
    r"__import__",
    r"system\(",
]
URL_PATTERNS = ["http://", "https://", "file://", "ftp://"]
DANGEROUS_ENTITIES = ["<!DOCTYPE", "<!ENTITY", "<!ELEMENT", "<!ATTLIST"]


class PmmlScanner(BaseScanner):
    """Scanner for PMML model files.

    This scanner performs security checks on PMML (Predictive Model Markup Language) files
    to detect potential XML External Entity (XXE) attacks, malicious scripts, and suspicious
    external references.

    Security features:
    - Uses defusedxml for safe XML parsing when available
    - Detects DOCTYPE and ENTITY declarations that could enable XXE attacks
    - Scans for suspicious patterns in Extension elements
    - Identifies external resource references
    - Validates PMML structure and version
    """

    name = "pmml"
    description = "Scans PMML files for XML security issues and suspicious content"
    supported_extensions = [".pmml"]

    @classmethod
    def can_handle(cls, path: str) -> bool:
        if not os.path.isfile(path):
            return False
        ext = os.path.splitext(path)[1].lower()
        if ext in cls.supported_extensions:
            return True
        try:
            with open(path, "rb") as f:
                head = f.read(512)  # Increased from 256 for better detection
            return b"<PMML" in head or b"<pmml" in head
        except Exception:
            return False

    def scan(self, path: str) -> ScanResult:
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size
        result.metadata["has_defusedxml"] = HAS_DEFUSEDXML

        try:
            with open(path, "rb") as f:
                data = f.read()
            result.bytes_scanned = len(data)
        except Exception as e:  # pragma: no cover - unexpected read errors
            result.add_issue(
                f"Error reading file: {e}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        try:
            text = data.decode("utf-8")
        except UnicodeDecodeError:
            try:
                text = data.decode("utf-8", errors="replace")
                result.add_issue(
                    "Non UTF-8 characters in PMML file",
                    severity=IssueSeverity.WARNING,
                    location=path,
                    why="PMML files should be valid UTF-8 encoded XML. Non-UTF-8 characters may indicate corruption or malicious content.",
                )
            except Exception as e:
                result.add_issue(
                    f"Failed to decode file as text: {e}",
                    severity=IssueSeverity.CRITICAL,
                    location=path,
                    details={"exception": str(e), "exception_type": type(e).__name__},
                )
                result.finish(success=False)
                return result

        # Check for dangerous XML constructs before parsing
        self._check_dangerous_xml_constructs(text, result, path)

        # Parse XML using safe parser when available
        try:
            if HAS_DEFUSEDXML:
                root = DefusedET.fromstring(text)
            else:
                # Warn about using unsafe parser
                result.add_issue(
                    "Using unsafe XML parser - defusedxml not available",
                    severity=IssueSeverity.WARNING,
                    location=path,
                    why="defusedxml is not installed. The standard XML parser may be vulnerable to XXE attacks. Install defusedxml for better security.",
                )
                root = UnsafeET.fromstring(text)
        except Exception as e:
            result.add_issue(
                f"Malformed XML: {e}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
                why="The file contains malformed XML that cannot be parsed. This may indicate corruption or malicious content.",
            )
            result.finish(success=False)
            return result

        # Validate PMML structure
        self._validate_pmml_structure(root, result, path)

        # Check for suspicious content in the parsed XML
        self._check_suspicious_content(root, result, path)

        result.finish(success=True)
        return result

    def _check_dangerous_xml_constructs(
        self, text: str, result: ScanResult, path: str
    ) -> None:
        """Check for dangerous XML constructs that could enable XXE attacks."""
        text_upper = text.upper()

        for construct in DANGEROUS_ENTITIES:
            if construct in text_upper:
                result.add_issue(
                    f"PMML file contains {construct} declaration",
                    severity=IssueSeverity.CRITICAL,
                    location=path,
                    details={"construct": construct},
                    why=f"{construct} declarations can enable XML External Entity (XXE) attacks, "
                    "allowing attackers to read local files, perform SSRF attacks, or cause denial of service.",
                )

    def _validate_pmml_structure(self, root, result: ScanResult, path: str) -> None:
        """Validate basic PMML structure and extract metadata."""
        if root.tag.lower() != "pmml":
            result.add_issue(
                "Root element is not <PMML>",
                severity=IssueSeverity.WARNING,
                location=path,
                why="Valid PMML files should have <PMML> as the root element.",
            )
        else:
            version = root.attrib.get("version", "")
            result.metadata["pmml_version"] = version
            if not version:
                result.add_issue(
                    "PMML missing version attribute",
                    severity=IssueSeverity.INFO,
                    location=path,
                    why="PMML files should specify a version for compatibility.",
                )

    def _check_suspicious_content(self, root, result: ScanResult, path: str) -> None:
        """Check for suspicious patterns and external references in PMML content."""
        for elem in root.iter():
            # Combine element text content with attributes for comprehensive scanning
            elem_text = elem.text or ""
            attr_text = " ".join(f"{k}={v}" for k, v in elem.attrib.items())

            # For Extension elements, also include all child element text content and names
            if elem.tag.lower() == "extension":
                # Get all text content recursively
                all_text = self._get_all_text_content(elem)
                combined = f"{elem_text} {attr_text} {all_text}".lower()
            else:
                combined = f"{elem_text} {attr_text}".lower()

            # Check for external resource references
            for url_pattern in URL_PATTERNS:
                if url_pattern in combined:
                    result.add_issue(
                        f"PMML references external resource: {url_pattern}",
                        severity=IssueSeverity.WARNING,
                        location=path,
                        details={"tag": elem.tag, "url_pattern": url_pattern},
                        why="External references in PMML files may be used to exfiltrate data or perform network requests.",
                    )
                    break

            # Check for suspicious element names (like <script>)
            if elem.tag.lower() in ["script", "javascript", "python", "exec", "eval"]:
                result.add_issue(
                    f"Suspicious XML element found: <{elem.tag}>",
                    severity=IssueSeverity.WARNING,
                    location=path,
                    details={"tag": elem.tag},
                    why="Suspicious XML elements may contain executable code or scripts.",
                )

            # Special attention to Extension elements which can contain arbitrary content
            if elem.tag.lower() == "extension":
                for pattern in SUSPICIOUS_PATTERNS:
                    if re.search(pattern, combined, re.IGNORECASE):
                        result.add_issue(
                            "Suspicious content in <Extension> element",
                            severity=IssueSeverity.WARNING,
                            location=path,
                            details={"tag": elem.tag, "pattern": pattern},
                            why="Extension elements can contain arbitrary content and may be used to embed malicious code or scripts.",
                        )
                        break

    def _get_all_text_content(self, element) -> str:
        """Recursively get all text content from an element and its children."""
        text_parts = []

        # Add element name as it might be suspicious (e.g., <script>)
        text_parts.append(element.tag)

        # Add element text
        if element.text:
            text_parts.append(element.text.strip())

        # Add tail text (text after the element)
        if element.tail:
            text_parts.append(element.tail.strip())

        # Recursively process children
        for child in element:
            text_parts.append(self._get_all_text_content(child))

        return " ".join(filter(None, text_parts))

```


### modelaudit/scanners/pytorch_binary_scanner.py

```python
import logging
import os
import struct
from typing import Any, Optional

from .base import BaseScanner, IssueSeverity, ScanResult


class PyTorchBinaryScanner(BaseScanner):
    """Scanner for raw PyTorch binary tensor files (.bin)"""

    name = "pytorch_binary"
    description = "Scans PyTorch binary tensor files for suspicious patterns"
    supported_extensions = [".bin"]

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        # Get blacklist patterns from config
        self.blacklist_patterns = self.config.get("blacklist_patterns", [])

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not os.path.isfile(path):
            return False

        # Check file extension
        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False

        # Check if it's actually a pytorch binary file
        try:
            from modelaudit.utils.filetype import detect_file_format, validate_file_type

            file_format = detect_file_format(path)

            # Validate file type for security, but be permissive for .bin files
            # since they can contain various formats of legitimate binary data
            if not validate_file_type(path):
                # File type validation failed - log but don't reject immediately
                # for .bin files since they can contain arbitrary binary data
                logger = logging.getLogger("modelaudit.scanners.pytorch_binary")
                logger.warning(f"File type validation failed for .bin file: {path}")
                # Continue to check if it's still a valid pytorch_binary format

            return file_format == "pytorch_binary"
        except Exception:
            return False

    def scan(self, path: str) -> ScanResult:
        """Scan a PyTorch binary file for suspicious patterns"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            self.current_file_path = path

            # Check for suspiciously small files
            if file_size < 100:
                result.add_issue(
                    f"Suspiciously small binary file: {file_size} bytes",
                    severity=IssueSeverity.INFO,
                    location=path,
                    details={"file_size": file_size},
                )

            # Read file in chunks to look for suspicious patterns
            bytes_scanned = 0
            chunk_size = 1024 * 1024  # 1MB chunks

            with open(path, "rb") as f:
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break

                    bytes_scanned += len(chunk)

                    # Check for embedded Python code patterns
                    self._check_for_code_patterns(
                        chunk, result, bytes_scanned - len(chunk)
                    )

                    # Check for blacklisted patterns
                    if self.blacklist_patterns:
                        self._check_for_blacklist_patterns(
                            chunk, result, bytes_scanned - len(chunk)
                        )

                    # Check for executable file signatures
                    self._check_for_executable_signatures(
                        chunk, result, bytes_scanned - len(chunk)
                    )

            result.bytes_scanned = bytes_scanned

            # Check if file appears to be a valid tensor file
            self._validate_tensor_structure(path, result)

        except Exception as e:
            result.add_issue(
                f"Error scanning binary file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _check_for_code_patterns(
        self, chunk: bytes, result: ScanResult, offset: int
    ) -> None:
        """Check for patterns that might indicate embedded code"""
        # Common patterns that might indicate embedded Python code
        code_patterns = [
            b"import os",
            b"import sys",
            b"import subprocess",
            b"eval(",
            b"exec(",
            b"__import__",
            b"compile(",
            b"globals()",
            b"locals()",
            b"open(",
            b"file(",
            b"input(",
            b"raw_input(",
            b"execfile(",
            b"os.system",
            b"subprocess.call",
            b"subprocess.Popen",
            b"socket.socket",
        ]

        for pattern in code_patterns:
            if pattern in chunk:
                # Find the position within the chunk
                pos = chunk.find(pattern)
                result.add_issue(
                    f"Suspicious code pattern found: {pattern.decode('ascii', errors='ignore')}",
                    severity=IssueSeverity.INFO,
                    location=f"{self.current_file_path} (offset: {offset + pos})",
                    details={
                        "pattern": pattern.decode("ascii", errors="ignore"),
                        "offset": offset + pos,
                    },
                )

    def _check_for_blacklist_patterns(
        self, chunk: bytes, result: ScanResult, offset: int
    ) -> None:
        """Check for blacklisted patterns in the binary data"""
        for pattern in self.blacklist_patterns:
            pattern_bytes = pattern.encode("utf-8")
            if pattern_bytes in chunk:
                pos = chunk.find(pattern_bytes)
                result.add_issue(
                    f"Blacklisted pattern found: {pattern}",
                    severity=IssueSeverity.CRITICAL,
                    location=f"{self.current_file_path} (offset: {offset + pos})",
                    details={
                        "pattern": pattern,
                        "offset": offset + pos,
                    },
                )

    def _check_for_executable_signatures(
        self, chunk: bytes, result: ScanResult, offset: int
    ) -> None:
        """Check for executable file signatures"""
        # Common executable signatures
        executable_sigs = {
            b"MZ": "Windows executable (PE)",
            b"\x7fELF": "Linux executable (ELF)",
            b"\xfe\xed\xfa\xce": "macOS executable (Mach-O 32-bit)",
            b"\xfe\xed\xfa\xcf": "macOS executable (Mach-O 64-bit)",
            b"\xcf\xfa\xed\xfe": "macOS executable (Mach-O)",
            b"#!/": "Shell script shebang",
        }

        for sig, description in executable_sigs.items():
            if sig in chunk:
                pos = chunk.find(sig)
                result.add_issue(
                    f"Executable signature found: {description}",
                    severity=IssueSeverity.CRITICAL,
                    location=f"{self.current_file_path} (offset: {offset + pos})",
                    details={
                        "signature": sig.hex(),
                        "description": description,
                        "offset": offset + pos,
                    },
                )

    def _validate_tensor_structure(self, path: str, result: ScanResult) -> None:
        """Validate that the file appears to have valid tensor structure"""
        try:
            with open(path, "rb") as f:
                # Read first few bytes to check for common tensor patterns
                header = f.read(32)

                # PyTorch tensors often start with specific patterns
                # This is a basic check - real validation would require parsing the format
                if len(header) < 8:
                    result.add_issue(
                        "File too small to be a valid tensor file",
                        severity=IssueSeverity.INFO,
                        location=self.current_file_path,
                        details={"header_size": len(header)},
                    )
                    return

                # Check if it looks like it might contain float32/float64 data
                # by looking for patterns of IEEE 754 floats
                # This is a heuristic - not definitive

                # Try to interpret first 8 bytes as double
                try:
                    value = struct.unpack("d", header[:8])[0]
                    # Check if it's a reasonable float value (not NaN, not huge)
                    if not (-1e100 < value < 1e100) or value != value:  # NaN check
                        result.metadata["tensor_validation"] = "unusual_float_values"
                except struct.error as e:
                    result.add_issue(
                        "Error interpreting tensor header",
                        severity=IssueSeverity.DEBUG,
                        location=self.current_file_path,
                        details={
                            "exception": str(e),
                            "exception_type": type(e).__name__,
                        },
                    )

        except Exception as e:
            result.add_issue(
                f"Error validating tensor structure: {str(e)}",
                severity=IssueSeverity.DEBUG,
                location=self.current_file_path,
                details={"exception": str(e)},
            )

```


### modelaudit/scanners/pytorch_zip_scanner.py

```python
import io
import os
import zipfile
from typing import Any, Optional

from ..utils import sanitize_archive_path
from .base import BaseScanner, IssueSeverity, ScanResult
from .pickle_scanner import PickleScanner


class PyTorchZipScanner(BaseScanner):
    """Scanner for PyTorch Zip-based model files (.pt, .pth)"""

    name = "pytorch_zip"
    description = "Scans PyTorch model files for suspicious code in embedded pickles"
    supported_extensions = [".pt", ".pth"]

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        # Initialize a pickle scanner for embedded pickles
        self.pickle_scanner = PickleScanner(config)

    @staticmethod
    def _read_header(path: str, length: int = 4) -> bytes:
        """Return the first few bytes of a file."""
        try:
            with open(path, "rb") as f:
                return f.read(length)
        except Exception:
            return b""

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not os.path.isfile(path):
            return False

        # Check file extension
        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False

        return True

    def scan(self, path: str) -> ScanResult:
        """Scan a PyTorch model file for suspicious code"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        header = self._read_header(path)
        if not header.startswith(b"PK"):
            result.add_issue(
                f"Not a valid zip file: {path}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"path": path},
            )
            result.finish(success=False)
            return result

        try:
            # Store the file path for use in issue locations
            self.current_file_path = path

            with zipfile.ZipFile(path, "r") as z:
                safe_entries: list[str] = []
                for name in z.namelist():
                    _, is_safe = sanitize_archive_path(name, "/tmp/extract")
                    if not is_safe:
                        result.add_issue(
                            f"Archive entry {name} attempted path traversal outside the archive",
                            severity=IssueSeverity.CRITICAL,
                            location=f"{path}:{name}",
                            details={"entry": name},
                        )
                        continue
                    safe_entries.append(name)
                pickle_files = [n for n in safe_entries if n.endswith(".pkl")]
                result.metadata["pickle_files"] = pickle_files

                # Track number of bytes scanned
                bytes_scanned = 0

                # Scan each pickle file
                for name in pickle_files:
                    data = z.read(name)
                    bytes_scanned += len(data)

                    with io.BytesIO(data) as file_like:
                        sub_result = self.pickle_scanner._scan_pickle_bytes(
                            file_like,
                            len(data),
                        )

                    # Include the pickle filename in each issue
                    for issue in sub_result.issues:
                        if issue.details:
                            issue.details["pickle_filename"] = name
                        else:
                            issue.details = {"pickle_filename": name}

                        # Update location to include the main file path
                        if not issue.location:
                            issue.location = f"{path}:{name}"
                        elif "pos" in issue.location:
                            # If it's a position from the pickle scanner,
                            # prepend the file path
                            issue.location = f"{path}:{name} {issue.location}"

                    # Merge results
                    result.merge(sub_result)

                # Check for other suspicious files
                for name in safe_entries:
                    # Check for Python code files
                    if name.endswith(".py"):
                        result.add_issue(
                            f"Python code file found in PyTorch model: {name}",
                            severity=IssueSeverity.INFO,
                            location=f"{path}:{name}",
                            details={"file": name},
                        )
                    # Check for shell scripts or other executable files
                    elif name.endswith((".sh", ".bash", ".cmd", ".exe")):
                        result.add_issue(
                            f"Executable file found in PyTorch model: {name}",
                            severity=IssueSeverity.CRITICAL,
                            location=f"{path}:{name}",
                            details={"file": name},
                        )

                # Check for missing data.pkl (common in PyTorch models)
                if not pickle_files or "data.pkl" not in [
                    os.path.basename(f) for f in pickle_files
                ]:
                    result.add_issue(
                        "PyTorch model is missing 'data.pkl', which is "
                        "unusual for standard PyTorch models.",
                        severity=IssueSeverity.INFO,
                        location=self.current_file_path,
                        details={"missing_file": "data.pkl"},
                    )

                # Check for blacklist patterns in all files
                if (
                    hasattr(self, "config")
                    and self.config
                    and "blacklist_patterns" in self.config
                ):
                    blacklist_patterns = self.config["blacklist_patterns"]
                    for name in safe_entries:
                        try:
                            file_data = z.read(name)

                            # For pickled files, check for patterns in the binary data
                            if name.endswith(".pkl"):
                                for pattern in blacklist_patterns:
                                    # Convert pattern to bytes for binary search
                                    pattern_bytes = pattern.encode("utf-8")
                                    if pattern_bytes in file_data:
                                        result.add_issue(
                                            f"Blacklisted pattern '{pattern}' "
                                            f"found in pickled file {name}",
                                            severity=IssueSeverity.CRITICAL,
                                            location=f"{self.current_file_path} "
                                            f"({name})",
                                            details={
                                                "pattern": pattern,
                                                "file": name,
                                                "file_type": "pickle",
                                            },
                                        )
                            else:
                                # For text files, decode and search as text
                                try:
                                    content = file_data.decode("utf-8")
                                    for pattern in blacklist_patterns:
                                        if pattern in content:
                                            result.add_issue(
                                                f"Blacklisted pattern '{pattern}' "
                                                f"found in file {name}",
                                                severity=IssueSeverity.CRITICAL,
                                                location=f"{self.current_file_path} "
                                                f"({name})",
                                                details={
                                                    "pattern": pattern,
                                                    "file": name,
                                                    "file_type": "text",
                                                },
                                            )
                                except UnicodeDecodeError:
                                    # Skip blacklist checking for binary files
                                    # that can't be decoded as text
                                    pass
                        except Exception as e:
                            result.add_issue(
                                f"Error reading file {name}: {str(e)}",
                                severity=IssueSeverity.DEBUG,
                                location=f"{self.current_file_path} ({name})",
                                details={
                                    "zip_entry": name,
                                    "exception": str(e),
                                    "exception_type": type(e).__name__,
                                },
                            )

                result.bytes_scanned = bytes_scanned

        except zipfile.BadZipFile:
            result.add_issue(
                f"Not a valid zip file: {path}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"path": path},
            )
            result.finish(success=False)
            return result
        except Exception as e:
            result.add_issue(
                f"Error scanning PyTorch zip file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

```


### modelaudit/scanners/safetensors_scanner.py

```python
"""SafeTensors model scanner."""

from __future__ import annotations

import json
import os
import struct
from typing import Any, Optional

from .base import BaseScanner, IssueSeverity, ScanResult

# Map SafeTensors dtypes to byte sizes for integrity checking
_DTYPE_SIZES = {
    "F16": 2,
    "F32": 4,
    "F64": 8,
    "I8": 1,
    "I16": 2,
    "I32": 4,
    "I64": 8,
    "U8": 1,
    "U16": 2,
    "U32": 4,
    "U64": 8,
}


class SafeTensorsScanner(BaseScanner):
    """Scanner for SafeTensors model files."""

    name = "safetensors"
    description = "Scans SafeTensors model files for integrity issues"
    supported_extensions = [".safetensors"]

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path."""
        if not os.path.isfile(path):
            return False

        ext = os.path.splitext(path)[1].lower()
        if ext in cls.supported_extensions:
            return True

        try:
            from modelaudit.utils.filetype import detect_file_format

            return detect_file_format(path) == "safetensors"
        except Exception:
            return False

    def scan(self, path: str) -> ScanResult:
        """Scan a SafeTensors file."""
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            self.current_file_path = path
            with open(path, "rb") as f:
                header_len_bytes = f.read(8)
                if len(header_len_bytes) != 8:
                    result.add_issue(
                        "File too small to contain SafeTensors header length",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result

                header_len = struct.unpack("<Q", header_len_bytes)[0]
                if header_len <= 0 or header_len > file_size - 8:
                    result.add_issue(
                        "Invalid SafeTensors header length",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                        details={"header_len": header_len},
                    )
                    result.finish(success=False)
                    return result

                header_bytes = f.read(header_len)
                if len(header_bytes) != header_len:
                    result.add_issue(
                        "Failed to read SafeTensors header",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result

                if not header_bytes.strip().startswith(b"{"):
                    result.add_issue(
                        "SafeTensors header does not start with '{'",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result

                try:
                    header = json.loads(header_bytes.decode("utf-8"))
                except json.JSONDecodeError as e:
                    result.add_issue(
                        f"Invalid JSON header: {str(e)}",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )
                    result.finish(success=False)
                    return result

                result.metadata["tensor_count"] = len(
                    [k for k in header.keys() if k != "__metadata__"]
                )

                # Validate tensor offsets and sizes
                tensor_entries: list[tuple[str, Any]] = [
                    (k, v) for k, v in header.items() if k != "__metadata__"
                ]

                data_size = file_size - (8 + header_len)
                offsets = []
                for name, info in tensor_entries:
                    if not isinstance(info, dict):
                        result.add_issue(
                            f"Invalid tensor entry for {name}",
                            severity=IssueSeverity.CRITICAL,
                            location=path,
                        )
                        continue

                    begin, end = info.get("data_offsets", [0, 0])
                    dtype = info.get("dtype")
                    shape = info.get("shape", [])

                    if not isinstance(begin, int) or not isinstance(end, int):
                        result.add_issue(
                            f"Invalid data_offsets for {name}",
                            severity=IssueSeverity.CRITICAL,
                            location=path,
                        )
                        continue

                    if begin < 0 or end <= begin or end > data_size:
                        result.add_issue(
                            f"Tensor {name} offsets out of bounds",
                            severity=IssueSeverity.CRITICAL,
                            location=path,
                            details={"begin": begin, "end": end},
                        )
                        continue

                    offsets.append((begin, end))

                    # Validate dtype/shape size
                    expected_size = self._expected_size(dtype, shape)
                    if expected_size is not None and expected_size != end - begin:
                        result.add_issue(
                            f"Size mismatch for tensor {name}",
                            severity=IssueSeverity.CRITICAL,
                            location=path,
                            details={
                                "expected_size": expected_size,
                                "actual_size": end - begin,
                            },
                        )

                # Check offset continuity
                offsets.sort(key=lambda x: x[0])
                last_end = 0
                for begin, end in offsets:
                    if begin != last_end:
                        result.add_issue(
                            "Tensor data offsets have gaps or overlap",
                            severity=IssueSeverity.CRITICAL,
                            location=path,
                        )
                        break
                    last_end = end

                data_size = file_size - (8 + header_len)
                if last_end != data_size:
                    result.add_issue(
                        "Tensor data does not cover entire file",
                        severity=IssueSeverity.CRITICAL,
                        location=path,
                    )

                # Check metadata
                metadata = header.get("__metadata__", {})
                if isinstance(metadata, dict):
                    for key, value in metadata.items():
                        if isinstance(value, str) and len(value) > 1000:
                            result.add_issue(
                                f"Metadata value for {key} is very long",
                                severity=IssueSeverity.INFO,
                                location=path,
                                why="Metadata fields over 1000 characters are unusual in model files. Long strings in metadata could contain encoded payloads, scripts, or data exfiltration attempts.",
                            )
                        if isinstance(value, str) and any(
                            s in value.lower() for s in ["import ", "#!/", "\\"]
                        ):
                            result.add_issue(
                                f"Suspicious metadata value for {key}",
                                severity=IssueSeverity.INFO,
                                location=path,
                                why="Metadata containing code-like patterns (import statements, shebangs, escape sequences) is atypical for model files and may indicate embedded scripts or injection attempts.",
                            )

                # Bytes scanned = file size
                result.bytes_scanned = file_size

        except Exception as e:
            result.add_issue(
                f"Error scanning SafeTensors file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=not result.has_errors)
        return result

    @staticmethod
    def _expected_size(dtype: Optional[str], shape: list[int]) -> Optional[int]:
        """Return expected tensor byte size from dtype and shape."""
        if dtype not in _DTYPE_SIZES:
            return None
        size = _DTYPE_SIZES[dtype]
        total = 1
        for dim in shape:
            if not isinstance(dim, int) or dim < 0:
                return None
            total *= dim
        return total * size

```


### modelaudit/scanners/tf_savedmodel_scanner.py

```python
import os
from pathlib import Path
from typing import Any, Optional

from modelaudit.suspicious_symbols import SUSPICIOUS_OPS

from .base import BaseScanner, IssueSeverity, ScanResult

# Try to import TensorFlow, but handle the case where it's not installed
try:
    import tensorflow as tf  # noqa: F401
    from tensorflow.core.protobuf.saved_model_pb2 import SavedModel

    HAS_TENSORFLOW = True
    SavedModelType: type = SavedModel
except ImportError:
    HAS_TENSORFLOW = False

    # Create a placeholder for type hints when TensorFlow is not available
    class SavedModel:  # type: ignore[no-redef]
        """Placeholder for SavedModel when TensorFlow is not installed"""

        meta_graphs: list = []

    SavedModelType = SavedModel


class TensorFlowSavedModelScanner(BaseScanner):
    """Scanner for TensorFlow SavedModel format"""

    name = "tf_savedmodel"
    description = "Scans TensorFlow SavedModel for suspicious operations"
    supported_extensions = [".pb", ""]  # Empty string for directories

    def __init__(self, config: Optional[dict[str, Any]] = None):
        super().__init__(config)
        # Additional scanner-specific configuration
        self.suspicious_ops = set(self.config.get("suspicious_ops", SUSPICIOUS_OPS))

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not HAS_TENSORFLOW:
            return False

        if os.path.isfile(path):
            # Handle any .pb file (protobuf format)
            ext = os.path.splitext(path)[1].lower()
            return ext == ".pb"
        if os.path.isdir(path):
            # For directory, check if saved_model.pb exists
            return os.path.exists(os.path.join(path, "saved_model.pb"))
        return False

    def scan(self, path: str) -> ScanResult:
        """Scan a TensorFlow SavedModel file or directory"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        # Store the file path for use in issue locations
        self.current_file_path = path

        # Check if TensorFlow is installed
        if not HAS_TENSORFLOW:
            result = self._create_result()
            result.add_issue(
                "TensorFlow not installed, cannot scan SavedModel. Install with "
                "'pip install modelaudit[tensorflow]'.",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"path": path},
            )
            result.finish(success=False)
            return result

        # Determine if path is file or directory
        if os.path.isfile(path):
            return self._scan_saved_model_file(path)
        if os.path.isdir(path):
            return self._scan_saved_model_directory(path)
        result = self._create_result()
        result.add_issue(
            f"Path is neither a file nor a directory: {path}",
            severity=IssueSeverity.CRITICAL,
            location=path,
            details={"path": path},
        )
        result.finish(success=False)
        return result

    def _scan_saved_model_file(self, path: str) -> ScanResult:
        """Scan a single SavedModel protobuf file"""
        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            with open(path, "rb") as f:
                content = f.read()
                result.bytes_scanned = len(content)

                saved_model = SavedModelType()
                saved_model.ParseFromString(content)

                self._analyze_saved_model(saved_model, result)

        except Exception as e:
            result.add_issue(
                f"Error scanning TF SavedModel file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _scan_saved_model_directory(self, dir_path: str) -> ScanResult:
        """Scan a SavedModel directory"""
        result = self._create_result()

        # Look for saved_model.pb in the directory
        saved_model_path = Path(dir_path) / "saved_model.pb"
        if not saved_model_path.exists():
            result.add_issue(
                "No saved_model.pb found in directory.",
                severity=IssueSeverity.CRITICAL,
                location=dir_path,
            )
            result.finish(success=False)
            return result

        # Scan the saved_model.pb file
        file_scan_result = self._scan_saved_model_file(str(saved_model_path))
        result.merge(file_scan_result)

        # Check for other suspicious files in the directory
        for root, _dirs, files in os.walk(dir_path):
            for file in files:
                file_path = Path(root) / file
                # Look for potentially suspicious Python files
                if file.endswith(".py"):
                    result.add_issue(
                        f"Python file found in SavedModel: {file}",
                        severity=IssueSeverity.INFO,
                        location=str(file_path),
                        details={"file": file, "directory": root},
                    )

                # Check for blacklist patterns in text files
                if (
                    hasattr(self, "config")
                    and self.config
                    and "blacklist_patterns" in self.config
                ):
                    blacklist_patterns = self.config["blacklist_patterns"]
                    try:
                        # Only check text files
                        if file.endswith(
                            (
                                ".txt",
                                ".md",
                                ".json",
                                ".yaml",
                                ".yml",
                                ".py",
                                ".cfg",
                                ".conf",
                            ),
                        ):
                            with Path(file_path).open(
                                encoding="utf-8",
                                errors="ignore",
                            ) as f:
                                content = f.read()
                                for pattern in blacklist_patterns:
                                    if pattern in content:
                                        result.add_issue(
                                            f"Blacklisted pattern '{pattern}' "
                                            f"found in file {file}",
                                            severity=IssueSeverity.CRITICAL,
                                            location=str(file_path),
                                            details={"pattern": pattern, "file": file},
                                        )
                    except Exception as e:
                        result.add_issue(
                            f"Error reading file {file}: {str(e)}",
                            severity=IssueSeverity.DEBUG,
                            location=str(file_path),
                            details={
                                "file": file,
                                "exception": str(e),
                                "exception_type": type(e).__name__,
                            },
                        )

        result.finish(success=True)
        return result

    def _analyze_saved_model(self, saved_model: Any, result: ScanResult) -> None:
        """Analyze the saved model for suspicious operations"""
        suspicious_op_found = False
        op_counts: dict[str, int] = {}

        for meta_graph in saved_model.meta_graphs:
            graph_def = meta_graph.graph_def

            # Scan all nodes in the graph for suspicious operations
            for node in graph_def.node:
                # Count all operation types
                if node.op in op_counts:
                    op_counts[node.op] += 1
                else:
                    op_counts[node.op] = 1

                # Check if the operation is suspicious
                if node.op in self.suspicious_ops:
                    suspicious_op_found = True
                    result.add_issue(
                        f"Suspicious TensorFlow operation: {node.op}",
                        severity=IssueSeverity.CRITICAL,
                        location=f"{self.current_file_path} (node: {node.name})",
                        details={
                            "op_type": node.op,
                            "node_name": node.name,
                            "meta_graph": (
                                meta_graph.meta_info_def.tags[0]
                                if meta_graph.meta_info_def.tags
                                else "unknown"
                            ),
                        },
                    )

        # Add operation counts to metadata
        result.metadata["op_counts"] = op_counts
        result.metadata["suspicious_op_found"] = suspicious_op_found

```


### modelaudit/scanners/tflite_scanner.py

```python
import os

from .base import BaseScanner, IssueSeverity, ScanResult

try:
    import tflite

    HAS_TFLITE = True
except Exception:  # pragma: no cover - optional dependency
    HAS_TFLITE = False

# Thresholds to detect potential overflow or malicious sizes
_MAX_COUNT = 1_000_000
_MAX_DIM = 10_000_000


class TFLiteScanner(BaseScanner):
    """Scanner for TensorFlow Lite model files."""

    name = "tflite"
    description = "Scans TensorFlow Lite models for integrity and safety issues"
    supported_extensions = [".tflite"]

    @classmethod
    def can_handle(cls, path: str) -> bool:
        if not HAS_TFLITE:
            return False
        return (
            os.path.isfile(path)
            and os.path.splitext(path)[1].lower() in cls.supported_extensions
        )

    def scan(self, path: str) -> ScanResult:
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        result.metadata["file_size"] = self.get_file_size(path)

        if not HAS_TFLITE:
            result.add_issue(
                "tflite package not installed. Install with 'pip install modelaudit[tflite]'",
                severity=IssueSeverity.CRITICAL,
                location=path,
            )
            result.finish(success=False)
            return result

        try:
            with open(path, "rb") as f:
                data = f.read()
                result.bytes_scanned = len(data)
            model = tflite.Model.Model.GetRootAsModel(data, 0)
        except Exception as e:  # pragma: no cover - parse errors
            result.add_issue(
                f"Invalid TFLite file or parse error: {e}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        subgraph_count = model.SubgraphsLength()
        result.metadata["subgraph_count"] = subgraph_count
        if subgraph_count > _MAX_COUNT:
            result.add_issue(
                f"Model declares {subgraph_count} subgraphs which exceeds the safe limit",
                severity=IssueSeverity.CRITICAL,
                location=path,
            )

        for sg_index in range(subgraph_count):
            subgraph = model.Subgraphs(sg_index)
            tensors_len = subgraph.TensorsLength()
            operators_len = subgraph.OperatorsLength()
            result.metadata.setdefault("tensor_counts", []).append(tensors_len)
            result.metadata.setdefault("operator_counts", []).append(operators_len)

            if tensors_len > _MAX_COUNT or operators_len > _MAX_COUNT:
                result.add_issue(
                    "TFLite model has extremely large tensor or operator count",
                    severity=IssueSeverity.CRITICAL,
                    location=path,
                    details={"tensors": tensors_len, "operators": operators_len},
                )
                continue

            for t_index in range(tensors_len):
                tensor = subgraph.Tensors(t_index)
                shape = [tensor.Shape(i) for i in range(tensor.ShapeLength())]
                if any(dim > _MAX_DIM for dim in shape):
                    result.add_issue(
                        "Tensor dimension extremely large (possible overflow)",
                        severity=IssueSeverity.CRITICAL,
                        location=f"{path} (tensor {t_index})",
                        details={"shape": shape},
                    )

            for o_index in range(operators_len):
                op = subgraph.Operators(o_index)
                opcode = model.OperatorCodes(op.OpcodeIndex())
                builtin = opcode.BuiltinCode()
                if builtin == tflite.BuiltinOperator.CUSTOM:
                    custom = opcode.CustomCode()
                    name = custom.decode("utf-8", "ignore") if custom else "unknown"
                    result.add_issue(
                        f"Model uses custom operator '{name}'",
                        severity=IssueSeverity.WARNING,
                        location=f"{path} (operator {o_index})",
                        details={"custom_op": name},
                    )

        result.finish(success=not result.has_errors)
        return result

```


### modelaudit/scanners/weight_distribution_scanner.py

```python
import os
import zipfile
from typing import Any, Dict, List, Optional

import numpy as np
from scipy import stats

from .base import BaseScanner, IssueSeverity, ScanResult, logger

# Try to import format-specific libraries
try:
    import h5py

    HAS_H5PY = True
except ImportError:
    HAS_H5PY = False

try:
    import torch

    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False

try:
    import tensorflow as tf  # noqa: F401

    HAS_TENSORFLOW = True
except ImportError:
    HAS_TENSORFLOW = False


class WeightDistributionScanner(BaseScanner):
    """Scanner that detects anomalous weight distributions potentially indicating trojaned models"""

    name = "weight_distribution"
    description = (
        "Analyzes weight distributions to detect potential backdoors or trojans"
    )
    supported_extensions = [
        ".pt",
        ".pth",
        ".h5",
        ".keras",
        ".hdf5",
        ".pb",
        ".onnx",
        ".safetensors",
    ]

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        # Configuration parameters
        self.z_score_threshold = self.config.get("z_score_threshold", 3.0)
        self.cosine_similarity_threshold = self.config.get(
            "cosine_similarity_threshold", 0.7
        )
        self.weight_magnitude_threshold = self.config.get(
            "weight_magnitude_threshold", 3.0
        )
        self.llm_vocab_threshold = self.config.get("llm_vocab_threshold", 10000)
        self.enable_llm_checks = self.config.get("enable_llm_checks", False)

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not os.path.isfile(path):
            return False

        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False

        # Check if we have the necessary libraries for the format
        if ext in [".pt", ".pth"] and not HAS_TORCH:
            return False
        if ext in [".h5", ".keras", ".hdf5"] and not HAS_H5PY:
            return False
        if ext == ".pb" and not HAS_TENSORFLOW:
            return False

        return True

    def scan(self, path: str) -> ScanResult:
        """Scan a model file for weight distribution anomalies"""
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            # Extract weights based on file format
            ext = os.path.splitext(path)[1].lower()

            if ext in [".pt", ".pth"]:
                weights_info = self._extract_pytorch_weights(path)
            elif ext in [".h5", ".keras", ".hdf5"]:
                weights_info = self._extract_keras_weights(path)
            elif ext == ".pb":
                weights_info = self._extract_tensorflow_weights(path)
            elif ext == ".onnx":
                weights_info = self._extract_onnx_weights(path)
            elif ext == ".safetensors":
                weights_info = self._extract_safetensors_weights(path)
            else:
                result.add_issue(
                    f"Unsupported model format for weight distribution scanner: {ext}",
                    severity=IssueSeverity.DEBUG,
                    location=path,
                )
                result.finish(success=False)
                return result

            if not weights_info:
                result.add_issue(
                    "Could not extract weights from model",
                    severity=IssueSeverity.DEBUG,
                    location=path,
                )
                result.finish(success=True)
                return result

            # Analyze the weights
            anomalies = self._analyze_weight_distributions(weights_info)

            # Add issues for any anomalies found
            for anomaly in anomalies:
                result.add_issue(
                    anomaly["description"],
                    severity=anomaly["severity"],
                    location=path,
                    details=anomaly["details"],
                    why=anomaly.get("why"),
                )

            # Add metadata
            result.metadata["layers_analyzed"] = len(weights_info)
            result.metadata["anomalies_found"] = len(anomalies)

            result.bytes_scanned = file_size

        except Exception as e:
            result.add_issue(
                f"Error analyzing weight distributions: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _extract_pytorch_weights(self, path: str) -> Dict[str, np.ndarray]:
        """Extract weights from PyTorch model files"""
        if not HAS_TORCH:
            return {}

        weights_info: Dict[str, np.ndarray] = {}

        try:
            # Load model with map_location to CPU to avoid GPU requirements
            model_data = torch.load(path, map_location=torch.device("cpu"))

            # Handle different PyTorch save formats
            if isinstance(model_data, dict):
                # State dict format
                state_dict = model_data.get("state_dict", model_data)

                # Find final layer weights (classification head)
                for key, value in state_dict.items():
                    if isinstance(value, torch.Tensor):
                        # Look for final layer patterns
                        if (
                            any(
                                pattern in key.lower()
                                for pattern in [
                                    "fc",
                                    "classifier",
                                    "head",
                                    "output",
                                    "final",
                                ]
                            )
                            and "weight" in key.lower()
                        ):
                            # PyTorch uses (out_features, in_features) but we expect (in_features, out_features)
                            weights_info[key] = value.detach().cpu().numpy().T
                        # Also include all weight tensors for comprehensive analysis
                        elif "weight" in key.lower() and len(value.shape) >= 2:
                            # PyTorch uses (out_features, in_features) but we expect (in_features, out_features)
                            weights_info[key] = value.detach().cpu().numpy().T

            elif hasattr(model_data, "state_dict"):
                # Full model format
                state_dict = model_data.state_dict()
                for key, value in state_dict.items():
                    if "weight" in key.lower() and isinstance(value, torch.Tensor):
                        # PyTorch uses (out_features, in_features) but we expect (in_features, out_features)
                        weights_info[key] = value.detach().cpu().numpy().T

        except Exception as e:
            logger.debug(f"Failed to extract weights from {path}: {e}")
            # Try loading as a zip file (newer PyTorch format)
            try:
                with zipfile.ZipFile(path, "r") as z:
                    # Look for data.pkl which contains the weights
                    if "data.pkl" in z.namelist():
                        # We can't easily extract weights from pickle without executing it
                        # This is a limitation we should document
                        pass
            except Exception as e:
                logger.debug(f"Failed to extract weights from {path}: {e}")

        return weights_info

    def _extract_keras_weights(self, path: str) -> Dict[str, np.ndarray]:
        """Extract weights from Keras/TensorFlow H5 model files"""
        if not HAS_H5PY:
            return {}

        weights_info: Dict[str, np.ndarray] = {}

        try:
            with h5py.File(path, "r") as f:
                # Navigate through the HDF5 structure to find weights
                def extract_weights(name, obj):
                    if isinstance(obj, h5py.Dataset):
                        # Check if this is a weight array
                        if "kernel" in name or "weight" in name:
                            weights_info[name] = np.array(obj)

                f.visititems(extract_weights)

        except Exception as e:
            logger.debug(f"Failed to extract weights from {path}: {e}")

        return weights_info

    def _extract_tensorflow_weights(self, path: str) -> Dict[str, np.ndarray]:
        """Extract weights from TensorFlow SavedModel files"""
        if not HAS_TENSORFLOW:
            return {}

        weights_info: Dict[str, np.ndarray] = {}

        # TensorFlow SavedModel weight extraction is complex and would require
        # loading the full graph. For now, we'll return empty.
        # This is a limitation that should be documented.

        return weights_info

    def _extract_onnx_weights(self, path: str) -> Dict[str, np.ndarray]:
        """Extract weights from ONNX model files"""
        try:
            import onnx

            HAS_ONNX = True
        except ImportError:
            HAS_ONNX = False

        if not HAS_ONNX:
            return {}

        weights_info: Dict[str, np.ndarray] = {}

        try:
            model = onnx.load(path)

            # Extract initializers (weights)
            for initializer in model.graph.initializer:
                if "weight" in initializer.name.lower():
                    weights_info[initializer.name] = onnx.numpy_helper.to_array(
                        initializer
                    )

        except Exception as e:
            logger.debug(f"Failed to extract weights from {path}: {e}")

        return weights_info

    def _extract_safetensors_weights(self, path: str) -> Dict[str, np.ndarray]:
        """Extract weights from SafeTensors files"""
        try:
            from safetensors import safe_open

            HAS_SAFETENSORS = True
        except ImportError:
            HAS_SAFETENSORS = False

        if not HAS_SAFETENSORS:
            return {}

        weights_info: Dict[str, np.ndarray] = {}

        try:
            with safe_open(path, framework="numpy") as f:
                for key in f.keys():
                    if "weight" in key.lower():
                        weights_info[key] = f.get_tensor(key)

        except Exception as e:
            logger.debug(f"Failed to extract weights from {path}: {e}")

        return weights_info

    def _analyze_weight_distributions(
        self, weights_info: Dict[str, np.ndarray]
    ) -> List[Dict[str, Any]]:
        """Analyze weight distributions for anomalies"""
        anomalies = []

        # Focus on final layer weights (classification heads)
        final_layer_candidates = {}
        for name, weights in weights_info.items():
            if (
                any(
                    pattern in name.lower()
                    for pattern in [
                        "fc",
                        "classifier",
                        "head",
                        "output",
                        "final",
                        "dense",
                    ]
                )
                and "weight" in name.lower()
            ):
                if len(weights.shape) == 2:  # Ensure it's a 2D weight matrix
                    final_layer_candidates[name] = weights

        # If no clear final layer found, analyze all 2D weight matrices
        if not final_layer_candidates:
            final_layer_candidates = {
                name: weights
                for name, weights in weights_info.items()
                if len(weights.shape) == 2
            }

        # Analyze each candidate layer
        for layer_name, weights in final_layer_candidates.items():
            layer_anomalies = self._analyze_layer_weights(layer_name, weights)
            anomalies.extend(layer_anomalies)

        return anomalies

    def _analyze_layer_weights(
        self, layer_name: str, weights: np.ndarray
    ) -> List[Dict[str, Any]]:
        """Analyze a single layer's weights for anomalies"""
        anomalies: List[Dict[str, Any]] = []

        # Weights shape is typically (input_features, output_features) for dense layers
        if len(weights.shape) != 2:
            return anomalies

        n_inputs, n_outputs = weights.shape

        # Detect if this is likely an LLM vocabulary layer or large transformer model
        is_likely_llm = (
            n_outputs > self.llm_vocab_threshold  # Large vocab layer
            or n_inputs
            > 768  # Large hidden dimensions typical of LLMs (768+ for BERT/GPT)
            or "transformer" in layer_name.lower()  # Transformer architecture
            or "attention" in layer_name.lower()  # Attention layers
            or "gpt" in layer_name.lower()  # GPT models
            or "bert" in layer_name.lower()  # BERT models
            or "llama" in layer_name.lower()  # LLaMA models
            or "t5" in layer_name.lower()  # T5 models
            or
            # GPT-style layer patterns
            layer_name.startswith("h.")  # GPT-2 style layers (h.0, h.1, etc.)
            or "mlp" in layer_name.lower()  # Multi-layer perceptron in transformers
            or "c_fc" in layer_name.lower()  # GPT-2 feed-forward layers
            or "c_attn" in layer_name.lower()  # GPT-2 attention layers
            or "c_proj" in layer_name.lower()  # GPT-2 projection layers
        )

        # Skip checks for LLMs if disabled (default behavior)
        if is_likely_llm and not self.enable_llm_checks:
            return []

        # For LLMs, we need much stricter thresholds to avoid false positives
        if is_likely_llm:
            # For LLMs, only check for extreme outliers with much higher thresholds
            z_score_threshold = max(
                8.0, self.z_score_threshold * 2.5
            )  # Much higher threshold
            outlier_percentage_threshold = 0.0001  # 0.01% for LLMs - very restrictive
        else:
            z_score_threshold = self.z_score_threshold
            outlier_percentage_threshold = 0.01  # 1% for classification models

        # 1. Check for outlier output neurons using Z-score
        output_norms = np.linalg.norm(weights, axis=0)  # L2 norm of each output neuron
        if len(output_norms) > 1:
            z_scores = np.abs(stats.zscore(output_norms))
            outlier_indices = np.where(z_scores > z_score_threshold)[0]

            # Only flag if the number of outliers is reasonable
            outlier_percentage = len(outlier_indices) / n_outputs
            if (
                len(outlier_indices) > 0
                and outlier_percentage < outlier_percentage_threshold
            ):
                anomalies.append(
                    {
                        "description": f"Layer '{layer_name}' has {len(outlier_indices)} output neurons with abnormal weight magnitudes",
                        "severity": IssueSeverity.INFO,
                        "details": {
                            "layer": layer_name,
                            "outlier_neurons": outlier_indices.tolist()[
                                :10
                            ],  # Limit to first 10
                            "total_outliers": len(outlier_indices),
                            "outlier_percentage": float(outlier_percentage * 100),
                            "z_scores": z_scores[outlier_indices].tolist()[:10],
                            "weight_norms": output_norms[outlier_indices].tolist()[:10],
                            "mean_norm": float(np.mean(output_norms)),
                            "std_norm": float(np.std(output_norms)),
                        },
                        "why": "Neurons with weight magnitudes significantly different from others in the same layer may indicate tampering, backdoors, or training anomalies. These outliers are flagged when their statistical z-score exceeds the threshold.",
                    }
                )

        # 2. Check for dissimilar weight vectors using cosine similarity
        # Only perform this check for classification models (small number of outputs)
        if 2 < n_outputs <= 1000:  # Skip for large vocabulary models
            # Compute pairwise cosine similarities
            normalized_weights = weights / (np.linalg.norm(weights, axis=0) + 1e-8)
            similarities = np.dot(normalized_weights.T, normalized_weights)

            dissimilar_neurons = []
            # Find neurons that are dissimilar to all others
            for i in range(n_outputs):
                # Get similarities to other neurons
                other_similarities = np.concatenate(
                    [similarities[i, :i], similarities[i, i + 1 :]]
                )
                max_similarity = (
                    np.max(np.abs(other_similarities))
                    if len(other_similarities) > 0
                    else 0
                )

                if max_similarity < self.cosine_similarity_threshold:
                    dissimilar_neurons.append((i, max_similarity))

            # Only flag if we have a small number of dissimilar neurons (< 5% or max 3)
            if 0 < len(dissimilar_neurons) <= max(3, int(0.05 * n_outputs)):
                for neuron_idx, max_sim in dissimilar_neurons:
                    anomalies.append(
                        {
                            "description": f"Layer '{layer_name}' output neuron {neuron_idx} has unusually dissimilar weights",
                            "severity": IssueSeverity.INFO,
                            "details": {
                                "layer": layer_name,
                                "neuron_index": neuron_idx,
                                "max_similarity_to_others": float(max_sim),
                                "weight_norm": float(output_norms[neuron_idx]),
                                "total_outputs": n_outputs,
                            },
                            "why": "Neurons with weight patterns completely unlike others in the same layer are uncommon in standard training. This dissimilarity (measured by cosine similarity below threshold) may indicate injected functionality or training irregularities.",
                        }
                    )

        # 3. Check for extreme weight values
        weight_magnitudes = np.abs(weights)
        mean_magnitude = np.mean(weight_magnitudes)
        std_magnitude = np.std(weight_magnitudes)
        threshold = mean_magnitude + self.weight_magnitude_threshold * std_magnitude

        extreme_weights = np.where(weight_magnitudes > threshold)
        if len(extreme_weights[0]) > 0:
            # Group by output neuron
            neurons_with_extreme_weights = np.unique(extreme_weights[1])
            # Only flag if very few neurons affected (< 0.1% or max 5)
            if len(neurons_with_extreme_weights) <= max(5, int(0.001 * n_outputs)):
                anomalies.append(
                    {
                        "description": f"Layer '{layer_name}' has neurons with extremely large weight values",
                        "severity": IssueSeverity.INFO,
                        "details": {
                            "layer": layer_name,
                            "affected_neurons": neurons_with_extreme_weights.tolist()[
                                :10
                            ],  # Limit list
                            "total_affected": len(neurons_with_extreme_weights),
                            "num_extreme_weights": len(extreme_weights[0]),
                            "threshold": float(threshold),
                            "max_weight": float(np.max(weight_magnitudes)),
                            "total_outputs": n_outputs,
                        },
                        "why": "Weight values that are orders of magnitude larger than typical can cause numerical instability, overflow attacks, or may encode hidden data. The threshold is set at 100 times the mean magnitude.",
                    }
                )

        return anomalies

```


### modelaudit/scanners/zip_scanner.py

```python
import os
import zipfile
from typing import Any, Dict, Optional

from ..utils import sanitize_archive_path
from .base import BaseScanner, IssueSeverity, ScanResult


class ZipScanner(BaseScanner):
    """Scanner for generic ZIP archive files"""

    name = "zip"
    description = "Scans ZIP archive files and their contents recursively"
    supported_extensions = [".zip", ".npz"]

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(config)
        self.max_depth = self.config.get("max_zip_depth", 5)  # Prevent zip bomb attacks
        self.max_entries = self.config.get(
            "max_zip_entries", 10000
        )  # Limit number of entries

    @classmethod
    def can_handle(cls, path: str) -> bool:
        """Check if this scanner can handle the given path"""
        if not os.path.isfile(path):
            return False

        # Check file extension
        ext = os.path.splitext(path)[1].lower()
        if ext not in cls.supported_extensions:
            return False

        # Verify it's actually a zip file
        try:
            with zipfile.ZipFile(path, "r") as _:
                pass
            return True
        except zipfile.BadZipFile:
            return False
        except Exception:
            return False

    def scan(self, path: str) -> ScanResult:
        """Scan a ZIP file and its contents"""
        # Check if path is valid
        path_check_result = self._check_path(path)
        if path_check_result:
            return path_check_result

        result = self._create_result()
        file_size = self.get_file_size(path)
        result.metadata["file_size"] = file_size

        try:
            # Store the file path for use in issue locations
            self.current_file_path = path

            # Scan the zip file recursively
            scan_result = self._scan_zip_file(path, depth=0)
            result.merge(scan_result)

        except zipfile.BadZipFile:
            result.add_issue(
                f"Not a valid zip file: {path}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"path": path},
            )
            result.finish(success=False)
            return result
        except Exception as e:
            result.add_issue(
                f"Error scanning zip file: {str(e)}",
                severity=IssueSeverity.CRITICAL,
                location=path,
                details={"exception": str(e), "exception_type": type(e).__name__},
            )
            result.finish(success=False)
            return result

        result.finish(success=True)
        return result

    def _scan_zip_file(self, path: str, depth: int = 0) -> ScanResult:
        """Recursively scan a ZIP file and its contents"""
        result = ScanResult(scanner_name=self.name)

        # Check depth to prevent zip bomb attacks
        if depth >= self.max_depth:
            result.add_issue(
                f"Maximum ZIP nesting depth ({self.max_depth}) exceeded",
                severity=IssueSeverity.WARNING,
                location=path,
                details={"depth": depth, "max_depth": self.max_depth},
            )
            return result

        with zipfile.ZipFile(path, "r") as z:
            # Check number of entries
            if len(z.namelist()) > self.max_entries:
                result.add_issue(
                    f"ZIP file contains too many entries "
                    f"({len(z.namelist())} > {self.max_entries})",
                    severity=IssueSeverity.WARNING,
                    location=path,
                    details={
                        "entries": len(z.namelist()),
                        "max_entries": self.max_entries,
                    },
                )
                return result

            # Scan each file in the archive
            for name in z.namelist():
                info = z.getinfo(name)

                _, is_safe = sanitize_archive_path(name, "/tmp/extract")
                if not is_safe:
                    result.add_issue(
                        f"Archive entry {name} attempted path traversal outside the archive",
                        severity=IssueSeverity.CRITICAL,
                        location=f"{path}:{name}",
                        details={"entry": name},
                    )
                    continue

                # Skip directories
                if name.endswith("/"):
                    continue

                # Check compression ratio for zip bomb detection
                if info.compress_size > 0:
                    compression_ratio = info.file_size / info.compress_size
                    if compression_ratio > 100:
                        result.add_issue(
                            f"Suspicious compression ratio ({compression_ratio:.1f}x) "
                            f"in entry: {name}",
                            severity=IssueSeverity.WARNING,
                            location=f"{path}:{name}",
                            details={
                                "entry": name,
                                "compressed_size": info.compress_size,
                                "uncompressed_size": info.file_size,
                                "ratio": compression_ratio,
                            },
                        )

                # Extract and scan the file
                try:
                    max_entry_size = self.config.get(
                        "max_entry_size", 10485760
                    )  # 10 MB default
                    data = b""
                    with z.open(name) as entry:
                        while True:
                            chunk = entry.read(4096)  # Read in 4 KB chunks
                            if not chunk:
                                break
                            data += chunk
                            if len(data) > max_entry_size:
                                raise ValueError(
                                    f"ZIP entry {name} exceeds maximum size of "
                                    f"{max_entry_size} bytes"
                                )

                    # Check if it's another zip file
                    if name.lower().endswith(".zip"):
                        # Write to temporary file and scan recursively
                        import tempfile

                        with tempfile.NamedTemporaryFile(
                            suffix=".zip", delete=False
                        ) as tmp:
                            tmp.write(data)
                            tmp_path = tmp.name

                        try:
                            nested_result = self._scan_zip_file(tmp_path, depth + 1)
                            # Update locations in nested results
                            for issue in nested_result.issues:
                                if issue.location and issue.location.startswith(
                                    tmp_path
                                ):
                                    issue.location = issue.location.replace(
                                        tmp_path, f"{path}:{name}", 1
                                    )
                            result.merge(nested_result)
                        finally:
                            os.unlink(tmp_path)
                    else:
                        # Try to scan the file with appropriate scanner
                        # Write to temporary file with proper extension
                        import tempfile

                        _, ext = os.path.splitext(name)
                        with tempfile.NamedTemporaryFile(
                            suffix=ext, delete=False
                        ) as tmp:
                            tmp.write(data)
                            tmp_path = tmp.name

                        try:
                            # Import core here to avoid circular import
                            from .. import core

                            # Use core.scan_file to scan with appropriate scanner
                            file_result = core.scan_file(tmp_path, self.config)

                            # Update locations in file results
                            for issue in file_result.issues:
                                if issue.location:
                                    if issue.location.startswith(tmp_path):
                                        issue.location = issue.location.replace(
                                            tmp_path, f"{path}:{name}", 1
                                        )
                                    else:
                                        issue.location = (
                                            f"{path}:{name} {issue.location}"
                                        )
                                else:
                                    issue.location = f"{path}:{name}"

                                # Add zip entry name to details
                                if issue.details:
                                    issue.details["zip_entry"] = name
                                else:
                                    issue.details = {"zip_entry": name}

                            result.merge(file_result)

                            # If no scanner handled the file, count the bytes ourselves
                            if file_result.scanner_name == "unknown":
                                result.bytes_scanned += len(data)
                        finally:
                            os.unlink(tmp_path)

                except Exception as e:
                    result.add_issue(
                        f"Error scanning ZIP entry {name}: {str(e)}",
                        severity=IssueSeverity.WARNING,
                        location=f"{path}:{name}",
                        details={"entry": name, "exception": str(e)},
                    )

        return result

```


### modelaudit/suspicious_symbols.py

```python
"""
Consolidated suspicious symbols used by ModelAudit security scanners.

This module centralizes all security pattern definitions used across ModelAudit scanners
to ensure consistency, maintainability, and comprehensive threat detection.

Architecture Overview:
    The suspicious symbols system provides a centralized repository of security patterns
    that are imported by individual scanners (PickleScanner, TensorFlowScanner, etc.).
    This approach ensures:

    1. **Consistency**: All scanners use the same threat definitions
    2. **Maintainability**: Security patterns are updated in one location
    3. **Extensibility**: New patterns can be added without modifying multiple files
    4. **Performance**: Compiled regex patterns are shared across scanners

Usage Examples:
    >>> from modelaudit.suspicious_symbols import SUSPICIOUS_GLOBALS, SUSPICIOUS_OPS
    >>>
    >>> # Check if a global reference is suspicious
    >>> if "os" in SUSPICIOUS_GLOBALS:
    >>>     print("os module flagged as suspicious")
    >>>
    >>> # Check TensorFlow operations
    >>> if "PyFunc" in SUSPICIOUS_OPS:
    >>>     print("PyFunc operation flagged as suspicious")

Security Pattern Categories:
    - SUSPICIOUS_GLOBALS: Dangerous Python modules/functions (pickle files)
    - SUSPICIOUS_STRING_PATTERNS: Regex patterns for malicious code strings
    - SUSPICIOUS_OPS: Dangerous TensorFlow operations
    - SUSPICIOUS_LAYER_TYPES: Risky Keras layer types
    - SUSPICIOUS_CONFIG_PROPERTIES: Dangerous configuration keys
    - SUSPICIOUS_CONFIG_PATTERNS: Manifest file security patterns

Maintenance Guidelines:
    When adding new patterns:
    1. Document the security rationale in comments
    2. Add corresponding test cases
    3. Consider false positive impact on legitimate ML models
    4. Test against real-world model samples
    5. Update this module's docstring with new pattern categories

Performance Considerations:
    - String patterns use compiled regex for efficiency
    - Dictionary lookups are O(1) for module checks
    - Patterns are loaded once at import time
    - Consider pattern complexity for large model files

Version History:
    - v1.0: Initial consolidation from individual scanner files
    - v1.1: Added documentation and architecture overview
"""

from typing import Any

from .explanations import DANGEROUS_OPCODES as _EXPLAIN_OPCODES

# =============================================================================
# PICKLE SECURITY PATTERNS
# =============================================================================

# Suspicious globals used by PickleScanner
# These represent Python modules/functions that can execute arbitrary code
# when encountered in pickle files during deserialization
SUSPICIOUS_GLOBALS = {
    # System interaction modules - HIGH RISK
    "os": "*",  # File system operations, command execution
    "posix": "*",  # Unix system calls (os.system equivalent)
    "sys": "*",  # Python runtime manipulation
    "subprocess": "*",  # Process spawning and control
    "runpy": "*",  # Dynamic module execution
    # Code execution functions - CRITICAL RISK
    "builtins": [
        "eval",
        "exec",
        "compile",
        "open",
        "input",
        "__import__",
    ],  # Dynamic code evaluation and file access
    "operator": ["attrgetter"],  # Attribute access bypass
    "importlib": ["import_module"],  # Dynamic module loading
    # Serialization/deserialization - MEDIUM RISK
    "pickle": ["loads", "load"],  # Recursive pickle loading
    "base64": ["b64decode", "b64encode", "decode"],  # Encoding/obfuscation
    "codecs": ["decode", "encode"],  # Text encoding manipulation
    # File system operations - HIGH RISK
    "shutil": ["rmtree", "copy", "move"],  # File system modifications
    "tempfile": ["mktemp"],  # Temporary file creation
    # Process control - CRITICAL RISK
    "pty": ["spawn"],  # Pseudo-terminal spawning
    "platform": ["system", "popen"],  # System information/execution
    # Low-level system access - CRITICAL RISK
    "ctypes": ["*"],  # C library access
    "socket": ["*"],  # Network communication
    # Serialization libraries that can execute arbitrary code - HIGH RISK
    "dill": [
        "load",
        "loads",
        "load_module",
        "load_module_asdict",
        "load_session",
    ],  # dill's load helpers can execute arbitrary code when unpickling
    # References to the private dill._dill module are also suspicious
    "dill._dill": "*",
}

# Builtin functions that enable dynamic code execution or module loading
DANGEROUS_BUILTINS = [
    "eval",
    "exec",
    "compile",
    "open",
    "input",
    "__import__",
]

# Suspicious string patterns used by PickleScanner
# Regex patterns that match potentially malicious code in string literals
SUSPICIOUS_STRING_PATTERNS = [
    # Python magic methods - can hide malicious code
    r"__[\w]+__",  # Magic methods like __reduce__, __setstate__
    # Encoding/decoding operations - often used for obfuscation
    r"base64\.b64decode",  # Base64 decoding
    # Dynamic code execution - CRITICAL
    r"eval\(",  # Dynamic expression evaluation
    r"exec\(",  # Dynamic code execution
    # System command execution - CRITICAL
    r"os\.system",  # Direct system command execution
    r"subprocess\.(?:Popen|call|check_output)",  # Process spawning
    # Dynamic imports - HIGH RISK
    r"import ",  # Import statements in strings
    r"importlib",  # Dynamic import library
    r"__import__",  # Built-in import function
    # Code construction - MEDIUM RISK
    r"lambda",  # Anonymous function creation
    # Hex encoding - possible obfuscation
    r"\\x[0-9a-fA-F]{2}",  # Hex-encoded characters
]

# Dangerous pickle opcodes that can lead to code execution
DANGEROUS_OPCODES = set(_EXPLAIN_OPCODES.keys())

# =============================================================================
# TENSORFLOW/KERAS SECURITY PATTERNS
# =============================================================================

# Suspicious TensorFlow operations
# These operations can perform file I/O, code execution, or system interaction
SUSPICIOUS_OPS = {
    # File system operations - HIGH RISK
    "ReadFile",  # Read arbitrary files
    "WriteFile",  # Write arbitrary files
    "MergeV2Checkpoints",  # Checkpoint manipulation
    "Save",  # Save operations (potential overwrite)
    "SaveV2",  # SaveV2 operations
    # Code execution - CRITICAL RISK
    "PyFunc",  # Execute Python functions
    "PyCall",  # Call Python code
    # System operations - CRITICAL RISK
    "ShellExecute",  # Execute shell commands
    "ExecuteOp",  # Execute arbitrary operations
    "SystemConfig",  # System configuration access
    # Data decoding - MEDIUM RISK (can process untrusted data)
    "DecodeRaw",  # Raw data decoding
    "DecodeJpeg",  # JPEG decoding (image processing)
    "DecodePng",  # PNG decoding (image processing)
}

# Suspicious Keras layer types
# Layer types that can contain arbitrary code or complex functionality
SUSPICIOUS_LAYER_TYPES = {
    "Lambda": "Can contain arbitrary Python code",
    "TFOpLambda": "Can call TensorFlow operations",
    "Functional": "Complex layer that might hide malicious components",
    "PyFunc": "Can execute Python code",
    "CallbackLambda": "Can execute callbacks at runtime",
}

# Suspicious configuration properties for Keras models
# Configuration keys that might contain executable code
SUSPICIOUS_CONFIG_PROPERTIES = [
    "function",  # Function references
    "module",  # Module specifications
    "code",  # Code strings
    "eval",  # Evaluation expressions
    "exec",  # Execution commands
    "import",  # Import statements
    "subprocess",  # Process control
    "os.",  # Operating system calls
    "system",  # System function calls
    "popen",  # Process opening
    "shell",  # Shell access
]

# =============================================================================
# MANIFEST/CONFIGURATION SECURITY PATTERNS
# =============================================================================

# Suspicious configuration patterns for manifest files
# Grouped by threat category for easier maintenance and understanding
SUSPICIOUS_CONFIG_PATTERNS = {
    # Network access patterns - MEDIUM RISK
    # These patterns indicate potential for unauthorized network communication
    "network_access": [
        "url",  # URLs for data exfiltration
        "endpoint",  # API endpoints
        "server",  # Server specifications
        "host",  # Host configurations
        "callback",  # Callback URLs
        "webhook",  # Webhook endpoints
        "http",  # HTTP protocol usage
        "https",  # HTTPS protocol usage
        "ftp",  # FTP protocol usage
        "socket",  # Socket connections
    ],
    # File access patterns - HIGH RISK
    # These patterns indicate potential for unauthorized file system access
    "file_access": [
        "file",  # File references
        "path",  # Path specifications
        "directory",  # Directory access
        "folder",  # Folder references
        "output",  # Output file specifications
        "save",  # Save operations
        "load",  # Load operations
        "write",  # Write operations
        "read",  # Read operations
    ],
    # Code execution patterns - CRITICAL RISK
    # These patterns indicate potential for arbitrary code execution
    "execution": [
        "exec",  # Execution commands
        "eval",  # Evaluation expressions
        "execute",  # Execute operations
        "run",  # Run commands
        "command",  # Command specifications
        "script",  # Script references
        "shell",  # Shell access
        "subprocess",  # Process spawning
        "system",  # System calls
        "code",  # Code strings
    ],
    # Credential patterns - HIGH RISK (data exposure)
    # These patterns indicate potential credential exposure
    "credentials": [
        "password",  # Password fields
        "secret",  # Secret values
        "credential",  # Credential specifications
        "auth",  # Authentication data
        "authentication",  # Authentication configuration
        "api_key",  # API key storage
    ],
}

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================


def get_all_suspicious_patterns() -> dict[str, Any]:
    """
    Get all suspicious patterns for testing or analysis.

    Returns:
        Dictionary containing all pattern categories with metadata
    """
    return {
        "pickle_globals": {
            "patterns": SUSPICIOUS_GLOBALS,
            "description": "Dangerous Python modules/functions in pickle files",
            "risk_level": "HIGH",
        },
        "pickle_strings": {
            "patterns": SUSPICIOUS_STRING_PATTERNS,
            "description": "Regex patterns for malicious code strings",
            "risk_level": "MEDIUM-HIGH",
        },
        "dangerous_builtins": {
            "patterns": DANGEROUS_BUILTINS,
            "description": "Builtin functions enabling dynamic code execution",
            "risk_level": "HIGH",
        },
        "dangerous_opcodes": {
            "patterns": sorted(DANGEROUS_OPCODES),
            "description": "Pickle opcodes that can trigger code execution",
            "risk_level": "HIGH",
        },
        "tensorflow_ops": {
            "patterns": SUSPICIOUS_OPS,
            "description": "Dangerous TensorFlow operations",
            "risk_level": "HIGH",
        },
        "keras_layers": {
            "patterns": SUSPICIOUS_LAYER_TYPES,
            "description": "Risky Keras layer types",
            "risk_level": "MEDIUM",
        },
        "config_properties": {
            "patterns": SUSPICIOUS_CONFIG_PROPERTIES,
            "description": "Dangerous configuration keys",
            "risk_level": "MEDIUM",
        },
        "manifest_patterns": {
            "patterns": SUSPICIOUS_CONFIG_PATTERNS,
            "description": "Manifest file security patterns",
            "risk_level": "MEDIUM",
        },
    }


def validate_patterns() -> list[str]:
    """
    Validate all suspicious patterns for correctness.

    Returns:
        List of validation warnings/errors (empty list if all valid)
    """
    import re

    warnings = []

    # Validate regex patterns
    for pattern in SUSPICIOUS_STRING_PATTERNS:
        try:
            re.compile(pattern)
        except re.error as e:
            warnings.append(f"Invalid regex pattern '{pattern}': {e}")

    # Validate global patterns structure
    for module, funcs in SUSPICIOUS_GLOBALS.items():
        if not isinstance(module, str):
            warnings.append(f"Module name must be string: {module}")
        if not (funcs == "*" or isinstance(funcs, list)):
            warnings.append(f"Functions must be '*' or list for module {module}")

    # Validate dangerous builtins
    for builtin in DANGEROUS_BUILTINS:
        if not isinstance(builtin, str):
            warnings.append(f"Builtin name must be string: {builtin}")

    # Validate dangerous opcodes
    for opcode in DANGEROUS_OPCODES:
        if not isinstance(opcode, str):
            warnings.append(f"Opcode name must be string: {opcode}")

    return warnings

```


## Summary

This context file contains:
- Complete README documentation
- All 30 Python source files from the modelaudit package
- Estimated total tokens: 88,701

The ModelAudit tool is a security scanner for AI/ML models that detects:
- Malicious code execution in pickled models
- Suspicious TensorFlow operations
- Dangerous pickle opcodes
- License compliance issues
- And many other security risks across different model formats

For development, testing, and deployment information, refer to the README section above.
